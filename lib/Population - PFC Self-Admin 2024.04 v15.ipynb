{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:51:54.535674Z",
     "start_time": "2025-08-20T17:51:54.209774Z"
    }
   },
   "source": [
    "###IMPORT MODULES/LIBRARIES ###\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import correlate\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import roc_auc_score as auROC\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "np.set_printoptions(suppress=True)\n",
    "from tqdm import tqdm #progress bar\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:51:55.677656Z",
     "start_time": "2025-08-20T17:51:55.674354Z"
    }
   },
   "source": [
    "alphalevel = 0.05\n",
    "bh_correction = 'no'\n",
    "\n",
    "extractedsignalsonly = 'no'\n",
    "mineventstoanalyze = 3\n",
    "\n",
    "save_aligned_data = 'no'\n",
    "save_auc_array = 'no'\n",
    "\n",
    "delete_overlaps = 'yes'\n",
    "separation_requirement = 1000\n",
    "\n",
    "frameaveraging = 4\n",
    "framerate = 30\n",
    "timebetweenframes = 33.333333\n",
    "averagedframerate = framerate/frameaveraging \n",
    "\n",
    "z_score_data = 'yes'\n",
    "\n",
    "pre_window_size = int(10*averagedframerate)\n",
    "window_size =  int((pre_window_size*2)+(1.6*averagedframerate))\n",
    "post_window_size = window_size - pre_window_size\n",
    "\n",
    "baselinefirstframe = 0\n",
    "baselinelastframe = int(3*averagedframerate)\n",
    "infusionframe = int(pre_window_size+(1.6*averagedframerate))\n",
    "aucfirstframe = int(pre_window_size-(5*averagedframerate))\n",
    "auclastframe = int(pre_window_size+(5*averagedframerate))\n",
    "\n",
    "eventofinterest = 'activeleverall'\n",
    "\n",
    "basedir = '../data'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:51:56.570150Z",
     "start_time": "2025-08-20T17:51:56.555688Z"
    }
   },
   "source": [
    "def analyze_single_session(indir, window_size, pre_window_size):\n",
    "    tempfiles = next(os.walk(indir))[2]\n",
    "    npyfiles = [f for f in tempfiles if os.path.splitext(f)[1]=='.npy' and 'extractedsignals_raw' in f]\n",
    "    matfiles = [f for f in tempfiles if os.path.splitext(f)[1]=='.mat']\n",
    "    if len(npyfiles) > 1:\n",
    "        npyfile = [f for f in tempfiles if os.path.splitext(f)[1]=='.npy' and 'extractedsignals_raw' in f and not 'part2' in f and not 'part3' in f and not 'part4' in f]\n",
    "        npyfile = npyfile [0]\n",
    "        matfile = [f for f in tempfiles if os.path.splitext(f)[1]=='.mat' and not 'results' in f and not 'part2' in f and not 'part3' in f and not 'part4' in f]\n",
    "        matfile = matfile [0]\n",
    "    else:\n",
    "        npyfile = npyfiles[0]\n",
    "        matfile = matfiles[0]\n",
    "\n",
    "    signals = np.squeeze(np.load(os.path.join(indir, npyfile)))\n",
    "    numrois = signals.shape[0] \n",
    "\n",
    "    behaviordata = sio.loadmat(os.path.join(indir, matfile))\n",
    "    eventlog = np.squeeze(behaviordata['eventlog'])\n",
    "    lastframe_timestamp_part1 = np.max(eventlog)\n",
    "\n",
    "    if len(npyfiles) > 1:\n",
    "        npyfiles2 = [f for f in tempfiles if os.path.splitext(f)[1]=='.npy' and 'extractedsignals_raw'and 'part2' in f]\n",
    "        matfiles2 = [f for f in tempfiles if os.path.splitext(f)[1]=='.mat' and 'part2' in f and not 'results' in f]\n",
    "        npyfile2 = npyfiles2[0]\n",
    "        matfile2 = matfiles2[0]\n",
    "        signals2 = np.squeeze(np.load(os.path.join(indir, npyfile2))) \n",
    "        signals = np.hstack((signals, signals2))\n",
    "        behaviordata2 = sio.loadmat(os.path.join(indir, matfile2))\n",
    "        eventlog2 = np.squeeze(behaviordata2['eventlog'])\n",
    "        eventlog2[:,1] = eventlog2[:,1]+lastframe_timestamp_part1\n",
    "        eventlog = np.concatenate((eventlog,eventlog2))\n",
    "        lastframe_timestamp_part2 = np.max(eventlog)\n",
    "    if len(npyfiles) > 2:\n",
    "        npyfiles3 = [f for f in tempfiles if os.path.splitext(f)[1]=='.npy' and 'extractedsignals_raw'and 'part3' in f]\n",
    "        matfiles3 = [f for f in tempfiles if os.path.splitext(f)[1]=='.mat' and 'part3' in f and not 'results' in f]\n",
    "        npyfile3 = npyfiles3[0]\n",
    "        matfile3 = matfiles3[0]\n",
    "        signals3 = np.squeeze(np.load(os.path.join(indir, npyfile3))) \n",
    "        signals = np.hstack((signals, signals3))\n",
    "        behaviordata3 = sio.loadmat(os.path.join(indir, matfile3))\n",
    "        eventlog3 = np.squeeze(behaviordata3['eventlog'])\n",
    "        eventlog3[:,1] = eventlog3[:,1]+lastframe_timestamp_part2\n",
    "        eventlog = np.concatenate((eventlog,eventlog3))\n",
    "        lastframe_timestamp_part3 = np.max(eventlog)\n",
    "    if len(npyfiles) > 3:\n",
    "        npyfiles4 = [f for f in tempfiles if os.path.splitext(f)[1]=='.npy' and 'extractedsignals_raw' and 'part4' in f]\n",
    "        matfiles4 = [f for f in tempfiles if os.path.splitext(f)[1]=='.mat' and 'part4' in f and not 'results' in f]\n",
    "        npyfile4 = npyfiles4[0]\n",
    "        matfile4 = matfiles4[0]\n",
    "        signals4 = np.squeeze(np.load(os.path.join(indir, npyfile4))) \n",
    "        signals = np.hstack((signals, signals4))\n",
    "        behaviordata4 = sio.loadmat(os.path.join(indir, matfile4))\n",
    "        eventlog4 = np.squeeze(behaviordata4['eventlog'])\n",
    "        eventlog4[:,1] = eventlog4[:,1]+lastframe_timestamp_part3\n",
    "        eventlog = np.concatenate((eventlog,eventlog4))\n",
    "\n",
    "    activelever = eventlog[eventlog[:,0]==22,1]\n",
    "    activelevertimeout = eventlog[eventlog[:,0]==222,1]\n",
    "    \n",
    "    if isinstance(window_size, str): \n",
    "        return activelever, activelevertimeout\n",
    "\n",
    "    if 'active' in eventofinterest:\n",
    "        newarray = np.array([])\n",
    "        temp = np.sort(np.hstack((activelever, activelevertimeout)))\n",
    "        temp = np.delete(temp, np.argwhere(np.ediff1d(temp)<separation_requirement)+1)\n",
    "        if eventofinterest == 'activelever':\n",
    "            for i in range(len(temp)):\n",
    "                if temp[i] in activelever:\n",
    "                    newarray = np.append(newarray, temp[i])\n",
    "            activelever = newarray\n",
    "        if eventofinterest == 'activelevertimeout':\n",
    "            for i in range(len(temp)):\n",
    "                if temp[i] in activelevertimeout:\n",
    "                    newarray = np.append(newarray, temp[i])\n",
    "            activelevertimeout = newarray\n",
    "        if eventofinterest == 'activeleverall':\n",
    "            activeleverall = temp\n",
    "\n",
    "    if eventofinterest == 'activelever':\n",
    "        events = activelever\n",
    "    elif eventofinterest == 'activelevertimeout':\n",
    "        events = activelevertimeout\n",
    "    elif eventofinterest == 'activeleverall':\n",
    "        events = activeleverall\n",
    "\n",
    "    if len(events) < 2:\n",
    "        return np.nan*np.ones((2, window_size)), np.nan*np.ones((2, window_size)), np.nan*np.ones((2, window_size)), np.nan*np.ones((2, window_size))\n",
    "\n",
    "    if animal == 'CTL1' or animal == 'ER-L1' or animal == 'ER-L2' or animal == 'IG-19' or animal == 'IG-28' or animal == 'PGa-T1' or animal == 'XYZ':\n",
    "        frame_timestamps = np.array(assumed_frame_timestamps)\n",
    "    else:\n",
    "        frame_timestamps = fix_any_dropped_frames(eventlog[eventlog[:,0]==9,1])\n",
    "    frame_timestamps = frame_timestamps[::frameaveraging]\n",
    "\n",
    "    if signals.shape[1] > frame_timestamps.shape[0]:\n",
    "        signals = signals[:,:frame_timestamps.shape[0]-1]\n",
    "\n",
    "    signals /= np.nanmean(signals, axis=1)[:, None]\n",
    "    \n",
    "    if z_score_data == 'yes':\n",
    "        for neuron in range(signals.shape[0]):\n",
    "            mean = np.nanmean(signals[neuron])\n",
    "            std = np.nanstd(signals[neuron])\n",
    "            signals[neuron] = (signals[neuron] - mean)/std\n",
    "    \n",
    "    signalsT = signals.T\n",
    "\n",
    "    if len(events) < mineventstoanalyze:\n",
    "        return np.nan*np.ones((2, window_size)), np.nan*np.ones((2, window_size)), np.nan*np.ones((2, window_size)), np.nan*np.ones((2, window_size))\n",
    "\n",
    "    def calculate_aligneddata_forevent(signalsT2, events2):\n",
    "        framenumberfor_eventofinterest = np.squeeze(framenumberforevent(events2, frame_timestamps))\n",
    "\n",
    "        numtrials = framenumberfor_eventofinterest.shape[0]\n",
    "        alignedevents = np.nan*np.zeros([numtrials,window_size,numrois])\n",
    "\n",
    "        for i in np.flip(range(numtrials)):\n",
    "            eventindex = framenumberfor_eventofinterest[i]\n",
    "\n",
    "            if np.isfinite(eventindex) and eventindex > pre_window_size and eventindex < np.shape(signalsT2)[0]-post_window_size:\n",
    "                eventindex = int(eventindex)\n",
    "                alignedevents[i, :, :] = signalsT2[eventindex-pre_window_size:eventindex+post_window_size, :]\n",
    "            else:\n",
    "                alignedevents = np.delete(alignedevents, i, axis = 0)\n",
    "\n",
    "        return alignedevents\n",
    "\n",
    "    alignedevents = calculate_aligneddata_forevent(signalsT, events) \n",
    "\n",
    "    for i in range(signals.shape[0]):\n",
    "        if np.isnan(np.mean(signals[i,:])):\n",
    "            print(animal, fov, 'IMAGE J ROI.ZIP CELL NUMBER %s HAS NaNs AND SHOULD BE CHANGED'%(i+1))\n",
    "    \n",
    "    alignedevents = np.swapaxes(alignedevents, 0,2)\n",
    "        \n",
    "    popevents = np.nanmean(alignedevents, axis=2)\n",
    "\n",
    "    if save_aligned_data == 'yes':\n",
    "        np.save(os.path.join(indir, 'popevents_%s_%s.npy'%(eventofinterest, separation_requirement)), popevents)\n",
    "        np.save(os.path.join(indir, 'alignedevents_%s_%s.npy'%(eventofinterest, separation_requirement)), alignedevents)\n",
    "    return popevents, alignedevents, events, events\n",
    "\n",
    "\n",
    "def framenumberforevent(event, frame_timestamps):\n",
    "    framenumber = np.nan*np.zeros(event.shape)\n",
    "    for ie, e in enumerate(event):\n",
    "        if np.isnan(e):\n",
    "            framenumber[ie] = np.nan\n",
    "        else:\n",
    "            temp = np.nonzero(frame_timestamps<=e)[0]\n",
    "            if temp.shape[0]>0:\n",
    "                framenumber[ie] = np.nonzero(frame_timestamps<=e)[0][-1]\n",
    "            else:\n",
    "                framenumber[ie] = 0\n",
    "    return framenumber\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:53:53.637281Z",
     "start_time": "2025-08-20T17:51:58.014479Z"
    }
   },
   "source": [
    "def fix_any_dropped_frames(frame_timestamps):\n",
    "    first_frame = np.array([0])\n",
    "    last_frame = np.array([int(np.max(frame_timestamps)+(500*timebetweenframes))])\n",
    "    frame_index_temp = np.concatenate((first_frame,frame_timestamps, last_frame))\n",
    "    frames_missed = []\n",
    "    for i in range(len(frame_index_temp)-1):\n",
    "            numframes_missed = int(np.round((frame_index_temp[i+1]-frame_index_temp[i])\\\n",
    "                /timebetweenframes)-1)\n",
    "            if numframes_missed > 0:\n",
    "                for j in range(numframes_missed):\n",
    "                    frame_missed = np.array([frame_index_temp[i] + (int(timebetweenframes * (j+1)))])\n",
    "                    frames_missed = np.concatenate((frames_missed, frame_missed))\n",
    "    corrected_frame_index = np.array(sorted(np.concatenate((frame_index_temp, frames_missed))))\n",
    "    return corrected_frame_index\n",
    "\n",
    "def fix_assumed_frames(frames):\n",
    "    dropped_frames = []\n",
    "    diff_frames = np.diff(frames)\n",
    "    inter_frame_interval = 33\n",
    "    frame_drop_idx = np.where(diff_frames>1.5*inter_frame_interval)[0]\n",
    "    for idx in frame_drop_idx:\n",
    "        numframesdropped = int(np.round((frames[idx+1]-frames[idx])/(inter_frame_interval+0.0))-1)\n",
    "        temp = [frames[idx]+a*inter_frame_interval for a in range(1,numframesdropped+1)]\n",
    "        dropped_frames.extend(temp)\n",
    "    corrected_frames = np.sort(np.concatenate((frames, np.array(dropped_frames))))\n",
    "    return corrected_frames\n",
    "\n",
    "try:\n",
    "    behaviordata_noframes = sio.loadmat('../data/empty.mat')\n",
    "    eventlog_noframes = np.squeeze(behaviordata_noframes['eventlog'])\n",
    "\n",
    "    max_of_eventlog_noframes = max(eventlog_noframes[:,1])\n",
    "    length_of_eventlog_noframes = len(eventlog_noframes[:,1])\n",
    "    x = np.vstack((eventlog_noframes, eventlog_noframes, eventlog_noframes))\n",
    "    x[length_of_eventlog_noframes:,1]= x[length_of_eventlog_noframes:,1]+max_of_eventlog_noframes\n",
    "    x[length_of_eventlog_noframes*2:,1]= x[length_of_eventlog_noframes*2:,1]+(2*max_of_eventlog_noframes)\n",
    "\n",
    "    eventlog_noframes = x\n",
    "\n",
    "    assumed_frames = fix_any_dropped_frames(eventlog_noframes[eventlog_noframes[:,0]==9,1])\n",
    "    assumed_frame_timestamps = fix_assumed_frames(eventlog_noframes[eventlog_noframes[:,0]==9,1])\n",
    "except FileNotFoundError:\n",
    "    print(\"empty.mat not found. Generating uniform timestamps based on signal length.\")\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:56:25.541765Z",
     "start_time": "2025-08-20T17:56:25.537692Z"
    }
   },
   "source": [
    "###OTHER FUNCTIONS\n",
    "def calculate_auROC(x,y,offset_to_zero=True):\n",
    "    U, p = stats.mannwhitneyu(x,y)\n",
    "    labels = np.concatenate((np.ones(x.shape), np.zeros(y.shape)))\n",
    "    data = np.concatenate((x,y))\n",
    "    A = auROC(labels, data)\n",
    "    if offset_to_zero:\n",
    "        return (2*(A-0.5), p)\n",
    "    else:\n",
    "        return (A, p)\n",
    "    \n",
    "def Benjamini_Hochberg_correction(vector_of_pvals, alpha = 0.05):\n",
    "    sortedpvals = np.sort(vector_of_pvals)\n",
    "    orderofpvals = np.argsort(vector_of_pvals)\n",
    "    m = sortedpvals[np.isfinite(sortedpvals)].shape[0] #Total number of hypotheses\n",
    "    for i in range(m):\n",
    "        if sortedpvals[i] > (i+1)*alpha/m:\n",
    "            k = i\n",
    "            break\n",
    "        elif i == m-1:\n",
    "            k = m-1\n",
    "        \n",
    "    correctedpvals = np.copy(vector_of_pvals)\n",
    "    correctedpvals[orderofpvals[k:]] = 1\n",
    "    correctedpvals[np.isnan(vector_of_pvals)] = np.nan\n",
    "    return correctedpvals\n",
    "\n",
    "def iterate_dirs(basedir, days):\n",
    "    for day in sorted(days):\n",
    "        animals = next(os.walk(os.path.join(basedir, day)))[1]\n",
    "        for animal in sorted(animals):\n",
    "            FOVs = next(os.walk(os.path.join(basedir, day, animal)))[1]\n",
    "            for fov in sorted(FOVs):\n",
    "                yield basedir, day, animal, fov\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-08-20T17:56:26.680601Z",
     "start_time": "2025-08-20T17:56:26.662111Z"
    }
   },
   "source": [
    "tempdays = next(os.walk(os.path.join(basedir)))[1]\n",
    "days = []\n",
    "for t in tempdays:\n",
    "    if t!= 'Cascade' and t!= 'CellTracking' and t!= 'Codes' and t != '.ipynb_checkpoints' and t != 'Other' and t!= 'Results':\n",
    "        days = np.append(days, t)\n",
    "numdays = len(days)\n",
    "\n",
    "popevents_day = {}\n",
    "popevents_fov = {}\n",
    "\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "    if day not in popevents_fov:\n",
    "        popevents_day[day] = np.nan*np.ones((1, window_size))\n",
    "        popevents_fov[day] = {}\n",
    "        print(day)\n",
    "    tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "for day in sorted(days):\n",
    "    popevents_day[day] = popevents_day[day][1:,:]\n",
    "    baseline = np.mean(popevents_day[day][:, baselinefirstframe:baselinelastframe], axis=1)\n",
    "    popevents_day[day] = popevents_day[day] - baseline[:, None]\n",
    "popevents_all = np.vstack([popevents_day[day] for day in sorted(days)])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 EarlyAcq\n",
      "1 MidAcq\n",
      "2 LateAcq\n",
      "3 EarlyExt\n",
      "4 LastExt\n",
      "5 CueRein\n",
      "6 DrugRein\n",
      "7 TMTRein\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:56:28.062713Z",
     "start_time": "2025-08-20T17:56:28.051820Z"
    }
   },
   "source": [
    "aucpvals_day = {}\n",
    "aucpvals_fov = {}\n",
    "\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "    if day not in aucpvals_fov:\n",
    "        aucpvals_day[day] = np.nan*np.ones((1, 2))\n",
    "        aucpvals_fov[day] = {}\n",
    "    tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "    if 'alignedevents_%s_%s.npy'%(eventofinterest, separation_requirement) in tempfiles: \n",
    "        if animal not in aucpvals_fov[day]:\n",
    "            aucpvals_fov[day][animal] = {}\n",
    "        aucpvals_fov[day][animal][fov] = np.load(os.path.join(basedir, day, animal, fov, 'aucpvals_%s_%s.npy'%(eventofinterest, separation_requirement)))\n",
    "        aucpvals_day[day] = np.vstack((aucpvals_day[day], aucpvals_fov[day][animal][fov]))\n",
    "\n",
    "for day in sorted(days):\n",
    "    aucpvals_day[day] = aucpvals_day[day][1:, :]  # Remove the first row of NaNs\n",
    "\n",
    "aucpvals_all = np.vstack([aucpvals_day[day] for day in sorted(days)])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:56:30.051300Z",
     "start_time": "2025-08-20T17:56:29.779655Z"
    }
   },
   "source": [
    "if numdays > 1:\n",
    "    fig, axs = plt.subplots(2, numdays, figsize=(15, 8))\n",
    "else: \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(5, 8))\n",
    "    \n",
    "sns.set_style('white')\n",
    "\n",
    "cmax = 4\n",
    "cmin = -cmax\n",
    "ymax = 4\n",
    "ymin = -ymax\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    temp = np.array(popevents_day[day])\n",
    "    temp_std = []\n",
    "    for neuron in range(temp.shape[0]):\n",
    "        neuron_std = np.nanstd(temp[neuron, baselinefirstframe:baselinelastframe])\n",
    "        temp_std = np.hstack((temp_std, neuron_std))\n",
    "        temp[neuron, :] = temp[neuron, :]/neuron_std\n",
    "    if d == 0:\n",
    "        print('data z-scored')\n",
    "            \n",
    "    tempresponse = np.nanmean(temp, axis=1)\n",
    "    sortresponse = np.argsort(tempresponse)[::-1]\n",
    "    numneurons_temp = len(sortresponse)\n",
    "\n",
    "    ax = axs[0, d]\n",
    "\n",
    "    im = ax.imshow(temp[sortresponse], cmap=plt.get_cmap('PRGn_r'), vmin=cmin, vmax=cmax, aspect='auto')\n",
    "    \n",
    "    ax.grid(False)\n",
    "    ax.set_ylabel('%s neurons'%numneurons_temp)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.plot([pre_window_size, pre_window_size], \n",
    "            [0, numneurons_temp], '--k', linewidth=1.5)\n",
    "    ax.plot([infusionframe, infusionframe],\n",
    "            [0, numneurons_temp], '--k', linewidth=1.5) \n",
    "\n",
    "    ax = axs[1, d]\n",
    "    ax.set_xticks([])\n",
    "    ax.plot(np.mean(temp, axis = 0))\n",
    "    ax.plot([pre_window_size, pre_window_size],\n",
    "             [-0.5,2.5],'--k', linewidth=1.5)\n",
    "    ax.plot([0, window_size],\n",
    "            [0,0], '--k', linewidth=0.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data z-scored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boqui\\AppData\\Local\\Temp\\ipykernel_55720\\3454249289.py:33: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "  im = ax.imshow(temp[sortresponse], cmap=plt.get_cmap('PRGn_r'), vmin=cmin, vmax=cmax, aspect='auto')\n",
      "C:\\Users\\boqui\\OneDrive\\Desktop\\nsync\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\boqui\\OneDrive\\Desktop\\nsync\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 16 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:56:57.048767Z",
     "start_time": "2025-08-20T17:56:56.632363Z"
    }
   },
   "source": [
    "fig, axs = plt.subplots(numdays, figsize=(4, 15), sharex=True, sharey=False)\n",
    "\n",
    "sns.set_style('white')\n",
    "colors = ['purple', 'gray', 'green']\n",
    "\n",
    "numneurons_day = {}\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    numneurons_day[day] = np.shape(aucpvals_day[day] )[0]\n",
    "    if numneurons_day[day] > 0:\n",
    "        activatedneurons = np.count_nonzero((aucpvals_day[day] [:,1]<=alphalevel) & (aucpvals_day[day] [:,0]>=0))/numneurons_day[day]\n",
    "        inhibitedneurons = np.count_nonzero((aucpvals_day[day] [:,1]<=alphalevel) & (aucpvals_day[day] [:,0]<0))/numneurons_day[day]\n",
    "        nonresponders = np.count_nonzero(aucpvals_day[day] [:,1]>alphalevel)/numneurons_day[day] \n",
    "    \n",
    "        counts = np.array([activatedneurons, nonresponders, inhibitedneurons])\n",
    "        labels = (int(activatedneurons*1000)/10, '', int(inhibitedneurons*1000)/10) #goofy calculations to plot % in each group\n",
    "    else:\n",
    "        counts = np.array([0, 1, 0])\n",
    "        labels = ('no data','','')\n",
    "        \n",
    "    ax = axs[d]\n",
    "    ax.pie(counts, colors=colors, labels=labels)\n",
    "    ax.set_title(day)\n",
    "    fig.tight_layout()\n",
    "\n",
    "numneurons_all = sum(numneurons_day.values())\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 400x1500 with 8 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T17:57:21.750218Z",
     "start_time": "2025-08-20T17:57:21.736247Z"
    }
   },
   "source": [
    "cell_mapping = {}\n",
    "cellcounterall = 0\n",
    "\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, days):\n",
    "    tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "    if 'popevents_%s_%s.npy' % (eventofinterest, separation_requirement) in tempfiles: \n",
    "        temp1 = np.load(os.path.join(basedir, day, animal, fov, 'popevents_%s_%s.npy' % (eventofinterest, separation_requirement)))  \n",
    "        numneurons_temp = temp1.shape[0]\n",
    "        for neuron in range(numneurons_temp):\n",
    "            cell_mapping[(day, animal, fov, neuron)] = cellcounterall + neuron\n",
    "        cellcounterall += numneurons_temp\n",
    "\n",
    "keys_day = {}\n",
    "idxs_day = {}\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    keys_day[day] = [key for key in cell_mapping.keys() if key[0] == day]\n",
    "    idxs_day[day] = [value for key, value in cell_mapping.items() if key[0] == day]\n",
    "\n",
    "unique_values = len(set(cell_mapping.values()))\n",
    "print(f\"Number of neuron values in cell_mapping: {unique_values}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neuron values in cell_mapping: 0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING: ONLY USE THIS SECTION IF YOU WANT TO REDO CLUSTERING!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### This function enables you to apply defined operators (based on auROC and pvalue significance) to clustering analysis.\n",
    "def create_condition(aucpvals_day, auROC_op, pval_op):\n",
    "    if auROC_op == '>' and pval_op == 'significant':\n",
    "        return np.logical_and(aucpvals_day[:,1] <= alphalevel, aucpvals_day[:,0] >= 0)\n",
    "    elif auROC_op == '<' and pval_op == 'significant':\n",
    "        return np.logical_and(aucpvals_day[:,1] <= alphalevel, aucpvals_day[:,0] < 0)\n",
    "    elif auROC_op == 'all' and pval_op == 'significant':\n",
    "        return aucpvals_day[:,1] <= alphalevel\n",
    "    elif auROC_op == '>' and pval_op == 'notsignificant':\n",
    "        return np.logical_and(aucpvals_day[:,1] > alphalevel, aucpvals_day[:,0] >= 0)\n",
    "    elif auROC_op == '<' and pval_op == 'notsignificant':\n",
    "        return np.logical_and(aucpvals_day[:,1] > alphalevel, aucpvals_day[:,0] < 0)\n",
    "    elif auROC_op == 'all' and pval_op == 'notsignificant':\n",
    "        return aucpvals_day[:,1] > alphalevel\n",
    "    elif auROC_op == '>' and pval_op == 'all':\n",
    "        return aucpvals_day[:,0] >= 0\n",
    "    elif auROC_op == '<' and pval_op == 'all':\n",
    "        return aucpvals_day[:,0] < 0\n",
    "    elif auROC_op == 'all' and pval_op == 'all':\n",
    "        return np.ones_like(aucpvals_day[:,0], dtype=bool)\n",
    "\n",
    "### This array is set to all zeros, and then each clustering analysis will index cluster labels into the array. \n",
    "### Protected by 'globals' function to prevent overwriting when doing multiple clustering analyses (you don't have to run this cell more than once)\n",
    "### If you need to overwrite the array, just add random letters to 'clusterids_all' and rerun cell. \n",
    "if 'clusterids_all' not in globals():\n",
    "    clusterids_all = np.zeros((numneurons_all))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### WHAT DATA DO YOU WANT TO USE FOR CLUSTERING, BASED ON AUROC VALUE? + RESPONDERS, - RESPONDERS, OR ALL? ###\n",
    "### WHAT DATA DO YOU WANT TO USE FOR CLUSTERING, BASED ON AUROC P-VALUE? SIGNIFICANT, NOT SIGNIFICANT, OR ALL? ###\n",
    "auROC_operator = '>'  # 'all', \">\", or \"<\"\n",
    "pval_operator = 'significant' #'significant', 'notsignificant', or 'all'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Select data for clustering based on operators above ###\n",
    "condition = create_condition(aucpvals_all, auROC_operator, pval_operator)\n",
    "popevents_forclustering = popevents_all[np.where(condition)]\n",
    "numneurons_forclustering = np.shape(popevents_forclustering)[0]\n",
    "print(np.shape(popevents_forclustering), 'popevents_forclustering array size')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### NORMALIZE RESPONSE AMPLITUDES ACROSS NEURONS TO 1?\n",
    "# Calculate the maximum absolute value for each neuron\n",
    "max_abs_values = np.max(np.abs(popevents_forclustering), axis=1, keepdims=True)\n",
    "\n",
    "# Normalize each neuron's response\n",
    "popevents_forclustering = popevents_forclustering / max_abs_values\n",
    "print(popevents_forclustering.shape, 'popevents_forclustering array normalized')\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## CUT THE LENGTH OF IMAGING WINDOW (WINDOW SIZE) FOR CLUSTERING? ###\n",
    "preseconds = 10 ###seconds before event to cluster\n",
    "postseconds = 11.6 ###seconds after event to cluster\n",
    "popevents_forclustering = popevents_forclustering[:, pre_window_size - int(averagedframerate*preseconds): pre_window_size + int(averagedframerate *postseconds)]\n",
    "print (popevents_forclustering.shape, 'popevents_forclustering array size after cutting frames')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### AVERAGE FRAMES FOR CLUSTERING? ###\n",
    "cluster_frameaveraging = 1 ### MUST BE USABLE TO DIVIDE BY FRAME LENGTH DENOTED ABOVE FOR CLUSTERING. SET TO 1 AND RUN IF YOU DON'T WANT TO AVERAGE\n",
    "temp = popevents_forclustering.reshape(popevents_forclustering.shape[0], popevents_forclustering.shape[1] // cluster_frameaveraging, cluster_frameaveraging) \n",
    "popevents_forclustering = temp.mean(axis = 2)\n",
    "print(popevents_forclustering.shape, 'popevents_forclustering array size after frame averaging')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT ARRAY FOR CLUSTERING\n",
    "cmax = 0.5 # Maximum colormap value. \n",
    "trial_types = ['Active']\n",
    "\n",
    "fig, axs = plt.subplots(2,2,figsize=(3*2,3*2), sharex='all', sharey='row')\n",
    "\n",
    "for t in range(len(trial_types)):\n",
    "    axs[0,t].set_title(trial_types[t])\n",
    "    ax = axs[0,t]\n",
    "    tempresponse = np.nanmean(popevents_forclustering, axis=1)\n",
    "    sortresponse = np.argsort(tempresponse)[::-1]\n",
    "    sns.heatmap(popevents_forclustering[sortresponse],\n",
    "                ax=ax,\n",
    "                cmap=plt.get_cmap('PRGn_r'),\n",
    "                vmin=-cmax,\n",
    "                vmax=cmax)\n",
    "\n",
    "    ax.axvline(preseconds*averagedframerate/cluster_frameaveraging, linestyle='--', color='k', linewidth=0.5)    \n",
    "    ax.set_xlabel('Time from Press (frames)')\n",
    "    \n",
    "    ax = axs[1,t]\n",
    "    sns.lineplot(data=np.mean(popevents_forclustering[:, t*window_size:(t+1)*window_size], axis=0), dashes=False,\n",
    "               ax=ax)\n",
    "    ax.axvline(preseconds*averagedframerate/cluster_frameaveraging, linestyle='--', color='k', linewidth=0.5)    \n",
    "    ax.set_xlabel('Time from Press (frames)')\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "axs[0,0].set_ylabel('Neurons')\n",
    "axs[1,0].set_ylabel('Mean norm. fluor.')\n",
    "\n",
    "\n",
    "fig.subplots_adjust(right=0.82)\n",
    "\n",
    "fig.savefig(os.path.join(basedir, 'Results', '%s_%s_HeatmapForClustering, %s_%s.PNG'%(eventofinterest, separation_requirement, auROC_operator, pval_operator)), format='PNG')\n",
    "fig.savefig(os.path.join(basedir, 'Results', '%s_%s_HeatmapForClustering, %s_%s.PDF'%(eventofinterest, separation_requirement, auROC_operator, pval_operator)), format='PDF')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Perform PCA on the data plotted above to reduce demensionality of the dataset before clustering\n",
    "pca = PCA(n_components=popevents_forclustering.shape[1], whiten=True)\n",
    "pca.fit(popevents_forclustering) \n",
    "\n",
    "with open(os.path.join(basedir, 'Results','pcaresults_%s_%s.pickle'%(auROC_operator, pval_operator)), 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "    \n",
    "transformed_data = pca.transform(popevents_forclustering)\n",
    "\n",
    "np.save(os.path.join(basedir, 'Results', 'transformed_data._%s_%s.npy'%(auROC_operator, pval_operator)), transformed_data)\n",
    "\n",
    "pca_vectors = pca.components_\n",
    "print ('Number of PCs = %d'%(pca_vectors.shape[0]))\n",
    "\n",
    "x = 100*pca.explained_variance_ratio_\n",
    "print('Variance Explained by PC\\n', x)\n",
    "\n",
    "### Define number of PCs here using one of the two methods below. Comment out 1 method ###\n",
    "num_retained_pcs = np.count_nonzero(x>5) ### based on min variance explained. usually ~5 works well\n",
    "\n",
    "# xprime = x - (x[0] + (x[-1]-x[0])/(x.size-1)*np.arange(x.size)) # or defined as the number at which the scree plot bends.\n",
    "# num_retained_pcs = np.argmin(xprime) # or defined as the number at which the scree plot bends.\n",
    "\n",
    "print ('Number of PCs to keep = %d'%(num_retained_pcs))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "ax.plot(np.arange(pca.explained_variance_ratio_.shape[0]).astype(int)+1, x, 'k')\n",
    "ax.set_ylabel('Percentage of\\nvariance explained')\n",
    "ax.set_xlabel('PC number')\n",
    "ax.axvline(num_retained_pcs, linestyle='--', color='k', linewidth=0.5)\n",
    "ax.set_title('Scree plot')\n",
    "ax.set_xlim([0,50])\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "fig.subplots_adjust(left=0.3)\n",
    "fig.subplots_adjust(right=0.98)\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "numcols = 3.0\n",
    "fig, axs = plt.subplots(int(np.ceil(num_retained_pcs/numcols)), int(numcols), sharey='all',\n",
    "                        figsize=(2*numcols, 2*int(np.ceil(num_retained_pcs/numcols))))\n",
    "for pc in range(num_retained_pcs):\n",
    "    ax = axs.flat[pc]\n",
    "    for k, tempkey in enumerate(trial_types):\n",
    "        ax.plot(pca_vectors[pc, k*window_size:(k+1)*window_size],\n",
    "                label='PC %d: %s'%(pc+1, tempkey))\n",
    "    ax.axvline(preseconds*averagedframerate/cluster_frameaveraging, linestyle='--', color='k', linewidth=0.5)    \n",
    "    ax.axhline(0, linestyle='--', color='k', linewidth=0.5)\n",
    "\n",
    "    if pc%numcols:\n",
    "        ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "fig.text(0.5, 0.05, 'Time from Press (s)', horizontalalignment='center', rotation='horizontal')\n",
    "fig.text(0.02, 0.6, 'PCA weights', verticalalignment='center', rotation='vertical')\n",
    "fig.tight_layout()\n",
    "for ax in axs.flat[num_retained_pcs:]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.08, hspace=0.08)\n",
    "fig.subplots_adjust(bottom=0.13)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(os.path.join(basedir, 'Results', '%s_%s_PCs_%s_%s.PDF'%(eventofinterest, separation_requirement, auROC_operator, pval_operator)), format='PDF')\n",
    "fig.savefig(os.path.join(basedir, 'Results', '%s_%s_PCs_%s_%s.PNG'%(eventofinterest, separation_requirement, auROC_operator, pval_operator)), format='PNG')\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Perform clustering based on the maximum number of PCs expected\n",
    "max_n_clusters = num_retained_pcs + 1 # Maximum number of clusters expected. Almost always ends up being the name number of PCs + 1\n",
    "\n",
    "possible_n_clusters = np.arange(2, max_n_clusters+1) #This requires a minimum of 2 clusters.\n",
    "### When the data contain no clusters at all, it will be quite visible when inspecting the two obtained clusters, as the responses of the clusters will be quite similar. \n",
    "\n",
    "possible_n_nearest_neighbors = np.arange(int(numneurons_forclustering/5), int((numneurons_forclustering/10)*9), int(numneurons_forclustering/4))\n",
    "      \n",
    "silhouette_scores = np.nan*np.ones((possible_n_clusters.size,\n",
    "                                    possible_n_nearest_neighbors.size))\n",
    "\n",
    "for n_clustersidx, n_clusters in enumerate(possible_n_clusters):\n",
    "    for nnidx, nn in enumerate(possible_n_nearest_neighbors):\n",
    "        model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=nn)\n",
    "        model.fit(transformed_data[:,:num_retained_pcs])\n",
    "        silhouette_scores[n_clustersidx, nnidx] = silhouette_score(transformed_data[:,:num_retained_pcs],\n",
    "                                                                   model.labels_,\n",
    "                                                                   metric='cosine')\n",
    "        print ('Done with numclusters = %d, num nearest neighbors = %d: score = %.3f'%(n_clusters, nn, silhouette_scores[n_clustersidx, nnidx]))\n",
    "print ('Done with model fitting')\n",
    "\n",
    "temp = {}\n",
    "temp['possible_n_clusters'] = possible_n_clusters\n",
    "temp['possible_n_nearest_neighbors'] = possible_n_nearest_neighbors\n",
    "temp['silhouette_scores'] = silhouette_scores\n",
    "temp['shape'] = 'cluster_nn'\n",
    "\n",
    "with open(os.path.join(basedir, 'Results', 'silhouette_scores_%s_%s.pickle'%(auROC_operator, pval_operator)), 'wb') as f:\n",
    "        pickle.dump(temp, f)   \n",
    " \n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Use clustering results to plot silhouette scores. The higher the sillhouette the better the response \"fits\" with the cluster\n",
    "with open(os.path.join(basedir, 'Results', 'silhouette_scores_%s_%s.pickle'%(auROC_operator, pval_operator)), 'rb') as f:\n",
    "        silhouette_scores = pickle.load(f)\n",
    "        \n",
    "# Identify optimal parameters from the above parameter space\n",
    "temp = np.where(silhouette_scores['silhouette_scores']==np.nanmax(silhouette_scores['silhouette_scores']))\n",
    "n_clusters = silhouette_scores['possible_n_clusters'][temp[0][0]]\n",
    "n_nearest_neighbors = silhouette_scores['possible_n_nearest_neighbors'][temp[1][0]]\n",
    "\n",
    "print (n_clusters, n_nearest_neighbors)\n",
    "\n",
    "# Redo clustering with these optimal parameters\n",
    "model = SpectralClustering(n_clusters=n_clusters,\n",
    "                           affinity='nearest_neighbors',\n",
    "                           n_neighbors=n_nearest_neighbors)\n",
    "\n",
    "model.fit(transformed_data[:,:num_retained_pcs])\n",
    "\n",
    "clusterids_temp = model.labels_\n",
    "uniqueclusterids_temp = list(set(clusterids_temp))\n",
    "numclusters_temp = len(uniqueclusterids_temp)\n",
    "\n",
    "temp = silhouette_score(transformed_data[:,:num_retained_pcs], model.labels_, metric='cosine')\n",
    "\n",
    "print ('Number of clusters = %d, average silhouette = %.3f'%(numclusters_temp, temp))\n",
    "\n",
    "# Save this optimal clustering model.\n",
    "with open(os.path.join(basedir, 'Results', 'clusteringmodel_%s_%s.pickle'%(auROC_operator, pval_operator)), 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def make_silhouette_plot(X, cluster_labels):\n",
    "        \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(4, 4)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    ax.set_xlim([-0.4, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels, metric='cosine')\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels, metric='cosine')\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values, alpha=0.9)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax.set_xticks([-0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    fig.savefig(os.path.join(basedir, 'Results', 'Silhouettes_%s_%s.PDF'%(auROC_operator, pval_operator)), format='PDF')\n",
    "    fig.savefig(os.path.join(basedir, 'Results', 'Silhouettes_%s_%s.PNG'%(auROC_operator, pval_operator)), format='PNG')\n",
    "      \n",
    "make_silhouette_plot(transformed_data[:,:num_retained_pcs], clusterids_temp)    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### this section inserts cluster ids into clusterids_all array, and then plots current clusters ### \n",
    "condition = create_condition(aucpvals_all, auROC_operator, pval_operator)\n",
    "\n",
    "clusterids_all[np.where(condition)] = clusterids_temp + 1 + max(clusterids_all[np.where(~condition)])\n",
    "numclusters = int(max(clusterids_all))+1\n",
    "\n",
    "for c in range(numclusters):\n",
    "    temp = popevents_all[np.where(clusterids_all == c)]\n",
    "    cluster_ci = 1.96 * np.std(temp, axis=0) / np.sqrt(len(temp)) ### cluster confidence interval\n",
    "    cluster_mean = np.mean(temp, axis = 0)\n",
    "    plt.plot(cluster_mean,  label=f'Cluster {c}')\n",
    "    plt.fill_between(range(len(cluster_mean)), cluster_mean - cluster_ci, cluster_mean + cluster_ci, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add vertical dashed line at pre_window_size\n",
    "plt.axvline(x=pre_window_size, color='k', linestyle='--')\n",
    "\n",
    "# Add horizontal dashed line at y=0\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "# Set x-axis and y-axis labels \n",
    "plt.ylabel(r'$\\Delta F/F$', fontsize=12)  # Use LaTeX for delta symbol\n",
    "plt.xlabel(\"Seconds\")\n",
    "\n",
    "# Set x-axis ticks to represent seconds from -10 to +11.6\n",
    "xticks = [0, pre_window_size, window_size]\n",
    "xlabels = [int(-pre_window_size/averagedframerate), 0, (window_size-pre_window_size)/averagedframerate]\n",
    "plt.xticks(xticks, labels=xlabels)\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFTER YOU RUN ALL DESIRED CLUSTERING ANALYSES, USE THE BELOW LINES TO REORDER CLUSTERIDS BASED ON THEIR PEAK RESPONSES, AND THEN SAVE THE CLUSTERING IDS TO YOUR DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### REORDER CLUSTER IDS BASED ON PEAK RESPONSES, AND THEN SAVE THE DATA\n",
    "# Find the peak response for each cluster, excluding cluster 0 which represents neurons that were not clustered (e.g., not significant neurons)\n",
    "peak_responses = np.nan*np.ones((numclusters))\n",
    "for c in range(1, numclusters):\n",
    "    cluster_mean = np.mean(popevents_all[np.where(clusterids_all == c)], axis=0)\n",
    "    min_peak, max_peak = min(cluster_mean), max(cluster_mean)\n",
    "    if max_peak > np.abs(min_peak):\n",
    "        peak_responses[c] = max_peak\n",
    "    else:\n",
    "        peak_responses[c] = min_peak\n",
    "\n",
    "# Determine the new cluster IDs based on the peak of the cluster mean\n",
    "sorted_clusters = np.argsort(peak_responses)[::-1]\n",
    "cluster_mapping = {old_cluster: new_cluster for new_cluster, old_cluster in enumerate(sorted_clusters)}\n",
    "\n",
    "# Create clusterids_all_renumbered\n",
    "clusterids_all_renumbered = np.array([cluster_mapping[old_cluster] for old_cluster in clusterids_all])\n",
    "\n",
    "### save clusterids_all_renumbered\n",
    "# np.save(os.path.join(basedir, 'Results', 'clusterids_all.npy'), clusterids_all_renumbered)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#replot the data to ensure renumbering worked correctly\n",
    "\n",
    "for c in range(numclusters):\n",
    "    temp = popevents_all[np.where(clusterids_all_renumbered == c)]\n",
    "    cluster_ci = 1.96 * np.std(temp, axis=0) / np.sqrt(len(temp)) ### cluster confidence interval\n",
    "    cluster_mean = np.mean(temp, axis = 0)\n",
    "    label = f'Cluster {c}' if c > 0 else 'Unclustered'\n",
    "    plt.plot(cluster_mean,  label=label)\n",
    "    plt.fill_between(range(len(cluster_mean)), cluster_mean - cluster_ci, cluster_mean + cluster_ci, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add vertical dashed line at pre_window_size\n",
    "plt.axvline(x=pre_window_size, color='k', linestyle='--')\n",
    "\n",
    "# Add horizontal dashed line at y=0\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "# Set x-axis and y-axis labels \n",
    "plt.ylabel(r'$\\Delta F/F$', fontsize=12)  # Use LaTeX for delta symbol\n",
    "plt.xlabel(\"Seconds\")\n",
    "\n",
    "# Set x-axis ticks to represent seconds from -10 to +11.6\n",
    "xticks = [0, pre_window_size, window_size]\n",
    "xlabels = [int(-pre_window_size/averagedframerate), 0, (window_size-pre_window_size)/averagedframerate]\n",
    "plt.xticks(xticks, labels=xlabels)\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'ClusteringResults.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'ClusteringResults.PNG'), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO PLOT PREVIOUSLY CLUSTERED DATA, CONTINUE HERE (DON'T RUN CLUSTERING ABOVE)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### LOAD REQUIRED DATA\n",
    "popevents_all = np.load(os.path.join(basedir, 'Results', 'popevents_%s_%s.npy'%(eventofinterest, separation_requirement))) ### 2D array (neurons x window_size) of all neuronal responses across all days\n",
    "aucpvals_all = np.load(os.path.join(basedir, 'Results', 'aucpvals_%s_%s.npy'%(eventofinterest, separation_requirement))) ### 2D array of auROC and associated pval (neurons x 2), neurons in same order as popevents\n",
    "clusterids_all = np.load(os.path.join(basedir, 'Results', 'clusterids_all.npy')) ### 1D array of cluster ids, neurons in same order as popevents\n",
    "numclusters = int(max(clusterids_all))+1\n",
    "\n",
    "print('shape of popactive %s, aucpvals %s, and clusterids %s'%(popevents_all.shape, aucpvals_all.shape, clusterids_all.shape))\n",
    "\n",
    "# Find peak response of each cluster for subsequent graphing\n",
    "peak_responses = np.nan*np.ones((numclusters))\n",
    "for c in range(1, numclusters):\n",
    "    cluster_mean = np.mean(popevents_all[np.where(clusterids_all == c)], axis=0)\n",
    "    min_peak, max_peak = min(cluster_mean), max(cluster_mean)\n",
    "    if max_peak > np.abs(min_peak):\n",
    "        peak_responses[c] = max_peak\n",
    "    else:\n",
    "        peak_responses[c] = min_peak\n",
    "\n",
    "# Define response direction of each cluster for future separation\n",
    "cluster_response_patterns = {\n",
    "    0: ([0], 'unclustered'),\n",
    "    1: (np.where(peak_responses > 0)[0], 'excited'),\n",
    "    2: (np.where(peak_responses < 0)[0], 'inhibited')}\n",
    "\n",
    "# Define colors for each cluster\n",
    "cluster_colors = []\n",
    "for c in range(1, numclusters+1):\n",
    "    if c <= 4:\n",
    "        color = mcolors.to_hex(plt.cm.Purples(1-(c-1)/6))\n",
    "    else:\n",
    "        color = mcolors.to_hex(plt.cm.Greens(0.5+(c-5)/6))\n",
    "    cluster_colors.append(color)\n",
    "cluster_colors.insert(0, 'black')\n",
    "\n",
    "# Create a dictionary for each day that includes cluster ids\n",
    "clusterids_day = {}\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    clusterids_day[day] = []\n",
    "    for neuron_key in keys_day[day]:\n",
    "        cell_number = cell_mapping[(neuron_key)]\n",
    "        clusterids_day[day] = np.append(clusterids_day[day], clusterids_all[cell_number])\n",
    "\n",
    "# Create a dictionary for each fov that includes cluster ids\n",
    "clusterids_fov = {}\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "    tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "    if 'popevents_%s_%s.npy'%(eventofinterest, separation_requirement) in tempfiles: \n",
    "        if day not in clusterids_fov:\n",
    "            clusterids_fov[day] = {}\n",
    "        if animal not in clusterids_fov[day]:\n",
    "            clusterids_fov[day][animal] = {}\n",
    "        if fov not in clusterids_fov[day][animal]:\n",
    "            temp_numneurons = np.shape(popevents_fov[day][animal][fov])[0]\n",
    "            clusterids_fov[day][animal][fov] = []\n",
    "            for neuron in range(temp_numneurons):\n",
    "                clusterids_fov[day][animal][fov] = np.append(clusterids_fov[day][animal][fov],\\\n",
    "                                                             clusterids_all[cell_mapping[(day, animal, fov, neuron)]])\n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE LINEPLOT FOR EACH CLUSTER\n",
    "# Create a figure and axis with the desired size\n",
    "fig, ax = plt.subplots(figsize=(6, 4))  # Adjust the figsize as needed\n",
    "\n",
    "# Variables used for data storage and plotting\n",
    "heatmap_all = np.nan * np.ones((1, window_size))\n",
    "heatmap_gap = 1  # gap inserted between clusters in heatmap as \"empty neurons\"\n",
    "\n",
    "# Replot the data to ensure data was aligned, saved, and loaded correctly\n",
    "for c in range(numclusters):\n",
    "    temp = popevents_all[np.where(clusterids_all == c)]\n",
    "    if plot_zscored_popevents == 'yes':\n",
    "        temp_std = []\n",
    "        for neuron in range(temp.shape[0]):\n",
    "            neuron_std = np.nanstd(temp[neuron, baselinefirstframe:baselinelastframe])\n",
    "            temp_std = np.hstack((temp_std, neuron_std))\n",
    "            temp[neuron, :] = temp[neuron, :]/neuron_std\n",
    "        if c == 0:\n",
    "            print('data z-scored')\n",
    "    cluster_ci = 1.96 * np.std(temp, axis=0) / np.sqrt(len(temp))  ### cluster confidence interval\n",
    "    cluster_mean = np.mean(temp, axis=0)\n",
    "    label = f'Cluster {c}' if c > 0 else 'Unclustered'\n",
    "    ax.plot(cluster_mean, label=label, color=cluster_colors[c])\n",
    "    ax.fill_between(range(len(cluster_mean)), cluster_mean - cluster_ci, cluster_mean + cluster_ci, alpha=0.3,\n",
    "                    color=cluster_colors[c])\n",
    "\n",
    "    # Fill heatmap array for next cell\n",
    "    if c != 0:\n",
    "        tempresponse = np.amax(temp, axis=1)\n",
    "        if peak_responses[c] > 0:\n",
    "            sortresponse = np.argsort(tempresponse)[::-1]\n",
    "        else:\n",
    "            sortresponse = np.argsort(tempresponse)\n",
    "        heatmap_all = np.vstack((heatmap_all, temp[sortresponse]))\n",
    "        heatmap_all = np.vstack((heatmap_all, np.nan * np.ones((heatmap_gap, window_size))))\n",
    "heatmap_all = heatmap_all[1:-heatmap_gap, :]  ### delete extra rows from heatmap array\n",
    "\n",
    "# Add legend outside the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Add vertical dashed line at pre_window_size\n",
    "ax.axvline(x=pre_window_size, color='k', linestyle='--')\n",
    "\n",
    "# Add horizontal dashed line at y=0\n",
    "ax.axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "# Set x-axis and y-axis labels\n",
    "ax.set_ylabel(r'$\\Delta F/F$', fontsize=10)  # Use LaTeX for delta symbol\n",
    "ax.set_xlabel(\"Seconds\")\n",
    "\n",
    "# Set x-axis ticks to represent seconds from -10 to +11.6\n",
    "xticks = [0, pre_window_size, window_size]\n",
    "xlabels = [int(-pre_window_size / averagedframerate), 0, (window_size - pre_window_size) / averagedframerate]\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(xlabels)\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# if plot_zscored_popevents == 'no':\n",
    "    # plt.savefig(os.path.join(basedir, 'Results', 'Clustering_LinePlot.PDF'), format='PDF', bbox_inches='tight')\n",
    "    # plt.savefig(os.path.join(basedir, 'Results', 'Clustering_LinePlot.PNG'), format='PNG', bbox_inches='tight')\n",
    "if plot_zscored_popevents == 'yes':\n",
    "    plt.savefig(os.path.join(basedir, 'Results', 'Clustering_LinePlot_zscored.PDF'), format='PDF', bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(basedir, 'Results', 'Clustering_LinePlot_zscored.PNG'), format='PNG', bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE HEATPLOT FOR EACH CLUSTER\n",
    "# Create a figure with the desired size\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 8))\n",
    "\n",
    "# Plot the heatmap\n",
    "im = ax.imshow(heatmap_all, cmap=plt.get_cmap('PRGn_r'), vmin=-cmax, vmax=cmax, aspect='auto')\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(im, ax=ax, shrink=0.5)\n",
    "\n",
    "# Add cluster labels to the left of the heatmap\n",
    "spacetilnow = 0\n",
    "for c in range(1, numclusters):\n",
    "    numneuronstemp = popevents_all[np.where(clusterids_all == c)].shape[0]\n",
    "    \n",
    "    # Update the position for the next cluster\n",
    "    spacetilnow += numneuronstemp + heatmap_gap\n",
    "    \n",
    "    # Add cluster label\n",
    "    ax.text(-50, spacetilnow - (0.5 * numneuronstemp), f'Cluster = {c}', fontsize=10, ha='center', va='center')\n",
    "\n",
    "ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel(\"Seconds\")\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(xlabels)\n",
    "ax.set_yticks([])\n",
    "ax.set_title('All Days', fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Clustering_HeatMap.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Clustering_HeatMap.PNG'), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a figure and axis with the desired size\n",
    "fig, axs = plt.subplots(1, len(days), figsize=(10, 2), sharex=True, sharey=False)\n",
    "scalebar_size = 0.3\n",
    "\n",
    "# Replot the data to ensure data was aligned, saved, and loaded correctly\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    ax = axs[d]\n",
    "    for c in range(numclusters):\n",
    "        temp = popevents_day[day][np.where(clusterids_day[day] == c)]\n",
    "        cluster_ci = 1.96 * np.std(temp, axis=0) / np.sqrt(len(temp))  ### cluster confidence interval\n",
    "        cluster_mean = np.mean(temp, axis=0)\n",
    "        label = f'Cluster {c}' if c > 0 else 'Unclustered'\n",
    "        ax.plot(cluster_mean, label=label, color=cluster_colors[c], linewidth = 1.5)\n",
    "#         ax.fill_between(range(len(cluster_mean)), cluster_mean - cluster_ci, cluster_mean + cluster_ci, alpha=0.3,\n",
    "#                         color=cluster_colors[c])\n",
    "\n",
    "    # Add vertical dashed line at pre_window_size\n",
    "    ax.axvline(x=pre_window_size, color='k', linestyle='--')\n",
    "\n",
    "    # Add horizontal dashed line at y=0\n",
    "    ax.axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "    # Set y-axis labels only for the far-left graph\n",
    "    if d == 0:\n",
    "        ax.set_ylabel(r'%s $\\Delta F/F$'%scalebar_size, fontsize=10)  # Use LaTeX for delta symbol\n",
    "\n",
    "    # Set x-axis and y-axis labels\n",
    "    ax.set_xlabel(f\"{window_size / averagedframerate:.1f} Seconds\")\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    ax.plot([-6, -6], [0.3, 0.3+scalebar_size], 'k-', lw = 1) ### y-scale bar with a value that is the difference between the second two digits as df/f\n",
    "\n",
    "    # Remove the border surrounding each graph\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Clustering_LinePlot_Days.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Clustering_LinePlot_Days.PNG'), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE HEATPLOT FOR EACH CLUSTER, ON EACH DAY\n",
    "# Create a figure with the desired size\n",
    "fig, axs = plt.subplots(1, len(days), figsize=(12, 5), sharex=True)\n",
    "\n",
    "# Loop through each day\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    heatmap_day = np.nan*np.ones((1, window_size))\n",
    "    spacetilnow = 0\n",
    "    \n",
    "    for c in range(1, numclusters):\n",
    "        temp = popevents_day[day][np.where(clusterids_day[day] == c)]\n",
    "        tempresponse = np.amax(temp, axis=1)\n",
    "        if peak_responses[c] > 0:\n",
    "            sortresponse = np.argsort(tempresponse)[::-1]\n",
    "        else:\n",
    "            sortresponse = np.argsort(tempresponse)\n",
    "        heatmap_day = np.vstack((heatmap_day, temp[sortresponse]))\n",
    "        heatmap_day = np.vstack((heatmap_day, np.nan * np.ones((heatmap_gap, window_size))))\n",
    "        numneuronstemp = popevents_day[day][np.where(clusterids_day[day] == c)].shape[0]\n",
    "        spacetilnow += numneuronstemp + heatmap_gap\n",
    "    \n",
    "    heatmap_day = heatmap_day[1:-heatmap_gap, :]  # delete extra rows from heatmap array\n",
    "    ax = axs[d]\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(heatmap_day, cmap=plt.get_cmap('PRGn_r'), vmin=-cmax, vmax=cmax, aspect='auto')\n",
    "    \n",
    "    ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth=1)\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Add cluster labels to the left of the heatmap\n",
    "    spacetilnow = 0\n",
    "    for c in range(1, numclusters):\n",
    "        numneuronstemp = popevents_day[day][np.where(clusterids_day[day] == c)].shape[0]\n",
    "        spacetilnow += numneuronstemp + heatmap_gap\n",
    "        ax.text(-20, spacetilnow - (0.5 * numneuronstemp), f'{c}', fontsize=10, ha='center', va='center')\n",
    "    \n",
    "    ax.set_xlabel(\"Seconds\")\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_title(f'Day {day},\\nn = {np.count_nonzero(clusterids_day[day] != 0)} Neurons', fontsize=10)\n",
    "\n",
    "# Adjust layout and save/show the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Clustering_HeatMap_Days.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Clustering_HeatMap_Days.PNG'), format='PNG')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(4, numdays + 1, figsize=(numdays * 1.2, 3), sharex=True, sharey=False)\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "cluster_colors[0] = 'lightgray'\n",
    "\n",
    "numneurons_cluster_day = {}\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    numneurons_cluster_day[day] = []\n",
    "    for cluster in range(numclusters):\n",
    "        numneuronstemp = int(np.count_nonzero(clusterids_day[day] == cluster))\n",
    "        numneurons_cluster_day[day].append(numneuronstemp)\n",
    "\n",
    "    ax = axs[0, d]\n",
    "    ax.pie(numneurons_cluster_day[day], colors=cluster_colors, wedgeprops={\"edgecolor\": \"none\"}, startangle=90)\n",
    "    ax.set_title(day)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('All ')\n",
    "        \n",
    "    ax = axs[1, d]\n",
    "    ax.pie(numneurons_cluster_day[day][1:], colors=cluster_colors[1:], wedgeprops={\"edgecolor\": \"none\"}, startangle=90)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('Sig.')\n",
    "        \n",
    "    ax = axs[2, d]\n",
    "    ax.pie(numneurons_cluster_day[day][1:np.count_nonzero(peak_responses[1:] > 0) + 1], \\\n",
    "           colors=cluster_colors[1:np.count_nonzero(peak_responses[1:] > 0) + 1], wedgeprops={\"edgecolor\": \"none\"},\n",
    "           startangle=90)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('Excited')\n",
    "    \n",
    "    ax = axs[3, d]\n",
    "    ax.pie(numneurons_cluster_day[day][np.count_nonzero(peak_responses[1:] > 0) + 1:], \\\n",
    "           colors=cluster_colors[np.count_nonzero(peak_responses[1:] > 0) + 1:], wedgeprops={\"edgecolor\": \"none\"},\n",
    "           startangle=90)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('Inhibited')\n",
    "\n",
    "numneurons_cluster_all = []\n",
    "for cluster in range(numclusters):\n",
    "    numneuronstemp = int(np.count_nonzero(clusterids_all == cluster))\n",
    "    numneurons_cluster_all.append(numneuronstemp)\n",
    "\n",
    "ax = axs[0, -1]\n",
    "ax.pie(numneurons_cluster_all, colors=cluster_colors, wedgeprops={\"edgecolor\": \"none\"}, startangle=90)\n",
    "ax.set_title('All Days')\n",
    "\n",
    "ax = axs[1, -1]\n",
    "ax.pie(numneurons_cluster_all[1:], colors=cluster_colors[1:], wedgeprops={\"edgecolor\": \"none\"}, startangle=90)\n",
    "\n",
    "ax = axs[2, -1]\n",
    "ax.pie(numneurons_cluster_all[1:np.count_nonzero(peak_responses[1:] > 0) + 1], \\\n",
    "       colors=cluster_colors[1:np.count_nonzero(peak_responses[1:] > 0) + 1], wedgeprops={\"edgecolor\": \"none\"},\n",
    "       startangle=90)\n",
    "\n",
    "ax = axs[3, -1]\n",
    "ax.pie(numneurons_cluster_all[np.count_nonzero(peak_responses[1:] > 0) + 1:], \\\n",
    "       colors=cluster_colors[np.count_nonzero(peak_responses[1:] > 0) + 1:], wedgeprops={\"edgecolor\": \"none\"},\n",
    "       startangle=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "# fig.savefig(os.path.join(basedir, 'Results', 'Clustering_PieCharts.PDF'), format='PDF')\n",
    "# fig.savefig(os.path.join(basedir, 'Results', 'Clustering_PieCharts.PNG'), format='PNG') ### Smaller File. Great for testing code.\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(4, numdays + 1, figsize=(numdays * 1.2, 3), sharex=True, sharey=False)\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "cluster_colors[0] = 'lightgray'\n",
    "\n",
    "numneurons_cluster_day = {}\n",
    "numneurons_cluster_days = np.nan*np.ones((numclusters, numdays))\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    numneurons_cluster_day[day] = []\n",
    "    for cluster in range(numclusters):\n",
    "        numneuronstemp = int(np.count_nonzero(clusterids_day[day] == cluster))\n",
    "        numneurons_cluster_day[day].append(numneuronstemp)\n",
    "        numneurons_cluster_days[cluster, d] = numneuronstemp\n",
    "\n",
    "    ax = axs[0, d]\n",
    "    ax.pie(numneurons_cluster_day[day], colors=cluster_colors, wedgeprops={\"edgecolor\": \"none\"}, startangle=90)\n",
    "    ax.set_title(day)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('All ')\n",
    "        \n",
    "    ax = axs[1, d]\n",
    "    ax.pie(numneurons_cluster_day[day][1:], colors=cluster_colors[1:], wedgeprops={\"edgecolor\": \"none\"}, startangle=90)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('Sig.')\n",
    "        \n",
    "    ax = axs[2, d]\n",
    "    ax.pie(numneurons_cluster_day[day][1:np.count_nonzero(peak_responses[1:] > 0) + 1], \\\n",
    "           colors=cluster_colors[1:np.count_nonzero(peak_responses[1:] > 0) + 1], wedgeprops={\"edgecolor\": \"none\"},\n",
    "           startangle=90)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('Excited')\n",
    "    \n",
    "    ax = axs[3, d]\n",
    "    ax.pie(numneurons_cluster_day[day][np.count_nonzero(peak_responses[1:] > 0) + 1:], \\\n",
    "           colors=cluster_colors[np.count_nonzero(peak_responses[1:] > 0) + 1:], wedgeprops={\"edgecolor\": \"none\"},\n",
    "           startangle=90)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('Inhibited')\n",
    "\n",
    "numneurons_cluster_all = []\n",
    "for cluster in range(numclusters):\n",
    "    numneuronstemp = int(np.count_nonzero(clusterids_all == cluster))\n",
    "    numneurons_cluster_all.append(numneuronstemp)\n",
    "\n",
    "ax = axs[0, -1]\n",
    "ax.pie(numneurons_cluster_all, colors=cluster_colors, wedgeprops={\"edgecolor\": \"none\"}, startangle=90)\n",
    "ax.set_title('All Days')\n",
    "\n",
    "ax = axs[1, -1]\n",
    "ax.pie(numneurons_cluster_all[1:], colors=cluster_colors[1:], wedgeprops={\"edgecolor\": \"none\"}, startangle=90)\n",
    "\n",
    "ax = axs[2, -1]\n",
    "ax.pie(numneurons_cluster_all[1:np.count_nonzero(peak_responses[1:] > 0) + 1], \\\n",
    "       colors=cluster_colors[1:np.count_nonzero(peak_responses[1:] > 0) + 1], wedgeprops={\"edgecolor\": \"none\"},\n",
    "       startangle=90)\n",
    "\n",
    "ax = axs[3, -1]\n",
    "ax.pie(numneurons_cluster_all[np.count_nonzero(peak_responses[1:] > 0) + 1:], \\\n",
    "       colors=cluster_colors[np.count_nonzero(peak_responses[1:] > 0) + 1:], wedgeprops={\"edgecolor\": \"none\"},\n",
    "       startangle=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "# fig.savefig(os.path.join(basedir, 'Results', 'Clustering_PieCharts.PDF'), format='PDF')\n",
    "# fig.savefig(os.path.join(basedir, 'Results', 'Clustering_PieCharts.PNG'), format='PNG') ### Smaller File. Great for testing code.\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CHANGES IN THE PROPORTION OF CELLS PER CLUSTER ACROSS DAYS\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(5, 9), sharex=True)\n",
    "\n",
    "### CHI SQUARE TEST\n",
    "print(numneurons_cluster_days.shape)\n",
    "\n",
    "### ROW 1\n",
    "#Make a copy of numneurons_cluster_days to avoid overwriting findings from above cell\n",
    "numneurons_cluster_days_proportions = numneurons_cluster_days.copy()\n",
    "\n",
    "### Line graph showing proportion of all neurons that fit into each cluster per day\n",
    "ax = axs[0, 0]\n",
    "ax.set_title('proportion of all neurons')\n",
    "for c in range(1, numclusters):\n",
    "    for d, day in enumerate(sorted(days)):\n",
    "        numneurons_cluster_days_proportions[c, d] /= numneurons_day[day]\n",
    "    ax.plot(numneurons_cluster_days_proportions[c,:], color = cluster_colors[c]) \n",
    "\n",
    "### Heatmap showing proportion of all neurons that fit into each cluster per day\n",
    "ax = axs[0, 1]\n",
    "ax.set_title('proportion of all neurons')\n",
    "for c in range(5, numclusters):\n",
    "    numneurons_cluster_days_proportions[c,:] = -numneurons_cluster_days_proportions[c,:]\n",
    "# Plot the heatmap\n",
    "im = ax.imshow(numneurons_cluster_days_proportions[1:], cmap=plt.get_cmap('PRGn_r'), vmin=-.1, vmax=.1, aspect='auto')\n",
    "# Add colorbar\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "\n",
    "### ROW 2\n",
    "#Make another copy of numneurons_cluster_days to avoid overwriting findings from above cell\n",
    "numneurons_cluster_days_proportions = numneurons_cluster_days.copy()\n",
    "\n",
    "### Line graph showing proportion of sig. neurons that fit into each cluster per day\n",
    "ax = axs[1, 0]\n",
    "ax.set_title('proportion of sig neurons')\n",
    "for c in range(1, numclusters):\n",
    "    for d, day in enumerate(sorted(days)):\n",
    "        temp_significantneurons = np.sum(numneurons_cluster_days[1:, d])\n",
    "        numneurons_cluster_days_proportions[c, d] /= temp_significantneurons\n",
    "    ax.plot(numneurons_cluster_days_proportions[c,:], color = cluster_colors[c]) \n",
    "\n",
    "### Heatmap showing proportion of sig. neurons that fit into each cluster per day\n",
    "ax = axs[1, 1]\n",
    "ax.set_title('proportion of sig neurons')\n",
    "for c in range(5, numclusters):\n",
    "    numneurons_cluster_days_proportions[c,:] = -numneurons_cluster_days_proportions[c,:]\n",
    "# Plot the heatmap\n",
    "im = ax.imshow(numneurons_cluster_days_proportions[1:], cmap=plt.get_cmap('PRGn_r'), vmin=-.25, vmax=.25, aspect='auto')\n",
    "# Add colorbar\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "\n",
    "### ROW 3\n",
    "#Make another copy of numneurons_cluster_days to avoid overwriting findings from above cell\n",
    "numneurons_cluster_days_proportions = numneurons_cluster_days.copy()\n",
    "\n",
    "### Line graph showing change in proportion of sig. neurons that fit into each cluster per day vs. day 1\n",
    "ax = axs[2, 0]\n",
    "ax.set_title(' proportion of all')\n",
    "for c in range(1, numclusters):\n",
    "    for d, day in enumerate(sorted(days)):\n",
    "        numneurons_cluster_days_proportions[c, d] /= numneurons_day[day]\n",
    "    numneurons_cluster_days_proportions[c, :] /= numneurons_cluster_days_proportions[c, 0]\n",
    "    ax.plot(numneurons_cluster_days_proportions[c,:], color = cluster_colors[c]) \n",
    "\n",
    "### CHI SQUARE TEST\n",
    "for c in range(1, numclusters):\n",
    "    expected_proportion = numneurons_cluster_days[c, 0]/sum(numneurons_cluster_days[1:, 0])\n",
    "    unexpected_proportion = 1 - expected_proportion\n",
    "    for d, day in enumerate(sorted(days)):\n",
    "        f_exp = [expected_proportion*sum(numneurons_cluster_days[1:, d]), unexpected_proportion*sum(numneurons_cluster_days[1:, d])]\n",
    "        f_obs = [numneurons_cluster_days[c, d], sum(numneurons_cluster_days[1:, d]) -  numneurons_cluster_days[c, d]]\n",
    "        chi, chi_pval = stats.chisquare(f_obs, f_exp)\n",
    "        if chi > 100:\n",
    "            if f_obs[0] > f_exp[0]:\n",
    "                print(c, day, chi, chi_pval)\n",
    "\n",
    "### Heatmap showing change in proportion of sig. neurons that fit into each cluster per day vs. day 1\n",
    "ax = axs[2, 1]\n",
    "ax.set_title(' proportion of all')\n",
    "for c in range(5, numclusters):\n",
    "    numneurons_cluster_days_proportions[c,:] = -numneurons_cluster_days_proportions[c,:]\n",
    "# Plot the heatmap\n",
    "im = ax.imshow(numneurons_cluster_days_proportions[1:], cmap=plt.get_cmap('PRGn_r'), vmin=-6.5, vmax=6.5, aspect='auto')\n",
    "# Add colorbar\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "### ROW 4\n",
    "#Make another copy of numneurons_cluster_days to avoid overwriting findings from above cell\n",
    "numneurons_cluster_days_proportions = numneurons_cluster_days.copy()\n",
    "\n",
    "### Line graph showing change in proportion of sig. neurons that fit into each cluster per day vs. day 1\n",
    "ax = axs[3, 0]\n",
    "ax.set_title(' proportion of sig')\n",
    "for c in range(1, numclusters):\n",
    "    for d, day in enumerate(sorted(days)):\n",
    "        temp_significantneurons = np.sum(numneurons_cluster_days[1:, d])\n",
    "        numneurons_cluster_days_proportions[c, d] /= temp_significantneurons\n",
    "    numneurons_cluster_days_proportions[c, :] /= numneurons_cluster_days_proportions[c, 0]\n",
    "    ax.plot(numneurons_cluster_days_proportions[c,:], color = cluster_colors[c]) \n",
    "\n",
    "### Heatmap showing change in proportion of sig. neurons that fit into each cluster per day vs. day 1\n",
    "ax = axs[3, 1]\n",
    "ax.set_title(' proportion of sig')\n",
    "for c in range(5, numclusters):\n",
    "    numneurons_cluster_days_proportions[c,:] = -numneurons_cluster_days_proportions[c,:]\n",
    "# Plot the heatmap\n",
    "im = ax.imshow(numneurons_cluster_days_proportions[1:], cmap=plt.get_cmap('PRGn_r'), vmin=-2.5, vmax=2.5, aspect='auto')\n",
    "# Add colorbar\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Cluster Proportion Change Days.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Cluster Proportion Change Days.png'), format='PNG')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS CORRELATIONS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "overwrite_crosscorrelations = 'no' #'yes' or 'no'. Be careful, as this analysis (if yes) takes many hours if you have a lot of neurons (mine took 6 with 16,000 neurons)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Define a function to calculate cross-correlation and Pearson correlation for a pair of neurons\n",
    "def compute_correlations(n1, n2):\n",
    "    temp1 = popevents_all[n1, :]\n",
    "    temp2 = popevents_all[n2, :]\n",
    "\n",
    "    cross_corr = correlate(temp1, temp2, mode='same')\n",
    "    peak_corr_index = np.argmax(np.abs(cross_corr))\n",
    "    frame_lag = int(window_size / 2) - peak_corr_index\n",
    "\n",
    "    if frame_lag > 0:\n",
    "        temp1_shifted = temp1[:window_size - frame_lag]\n",
    "        temp2_shifted = temp2[frame_lag:]\n",
    "    else:\n",
    "        temp1_shifted = temp1[-frame_lag:]\n",
    "        temp2_shifted = temp2[:window_size + frame_lag]\n",
    "\n",
    "    time_lag = frame_lag / averagedframerate\n",
    "    rval, _ = stats.pearsonr(temp1_shifted, temp2_shifted)\n",
    "\n",
    "    return time_lag, rval\n",
    "\n",
    "if overwrite_crosscorrelations == 'yes':\n",
    "    # Create empty arrays\n",
    "    timelag_all = np.nan*np.ones((numneurons_all, numneurons_all))\n",
    "    crosscorr_all = np.nan*np.ones((numneurons_all, numneurons_all))\n",
    "    \n",
    "    # Set up parallel processing\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(compute_correlations)(n1, n2) for n1 in range(numneurons_all) for n2 in range(numneurons_all))\n",
    "\n",
    "    \n",
    "    # Unpack results into timelag_all and crosscorr_all\n",
    "    for idx, (time_lag, rval) in enumerate(results):\n",
    "        n1 = idx // numneurons_all\n",
    "        n2 = idx % numneurons_all\n",
    "        timelag_all[n1, n2] = time_lag\n",
    "        crosscorr_all[n1, n2] = rval\n",
    "        \n",
    "    \n",
    "    # Save Arrays\n",
    "    np.save(os.path.join(basedir, 'Results', 'timelag_%s_%s.npy'%(eventofinterest, separation_requirement)), timelag_all)\n",
    "    np.save(os.path.join(basedir, 'Results', 'crosscorr_%s_%s.npy'%(eventofinterest, separation_requirement)), crosscorr_all)\n",
    "\n",
    "# Load Arrays\n",
    "timelag_all = np.load(os.path.join(basedir, 'Results', 'timelag_%s_%s.npy'%(eventofinterest, separation_requirement)))\n",
    "crosscorr_all = np.load(os.path.join(basedir, 'Results', 'crosscorr_%s_%s.npy'%(eventofinterest, separation_requirement)))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CROSS CORRELATIONS FOR CLUSTERED AND UNCLUSTERED NEURONS\n",
    "\n",
    "# Sort the crosscorrelation data by cluster ID, excluding cluster ID 0\n",
    "sortingarray = np.argsort(clusterids_all)\n",
    "crosscorr_all_sorted = crosscorr_all[sortingarray][:, sortingarray]\n",
    "timelag_all_sorted = timelag_all[sortingarray][:, sortingarray]\n",
    "\n",
    "# Mask out data where cluster ID is 0\n",
    "numneurons_notsignificant = np.count_nonzero(clusterids_all == 0)\n",
    "numneurons_significant = np.count_nonzero(clusterids_all > 0)\n",
    "\n",
    "crosscorr_cluster = crosscorr_all_sorted[numneurons_notsignificant:, numneurons_notsignificant:]\n",
    "timelag_cluster = timelag_all_sorted[numneurons_notsignificant:, numneurons_notsignificant:]\n",
    "\n",
    "# Plot the sorted cross correlations and time lags\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot cross correlations using matplotlib or seaborn. Note that seaborn overemphasizes differences, so you need to make the plot very large to see actual results well\n",
    "axs[0].imshow(crosscorr_cluster, vmin=-1, vmax=1, cmap='PRGn_r', aspect='auto')\n",
    "\n",
    "# ax = axs[0]\n",
    "# sns.heatmap(crosscorr_cluster, ax=ax,cmap=plt.get_cmap('PRGn_r'),vmin=-1,vmax=1)\n",
    "\n",
    "axs[0].set_title('Cross Correlation Between Clusters', fontsize=10)\n",
    "\n",
    "# Plot time lags\n",
    "axs[1].imshow(timelag_cluster, vmin = -5, vmax = 5, cmap='PRGn_r', aspect='auto')\n",
    "axs[1].set_title('Time Lags Between Clusters', fontsize=10)\n",
    "\n",
    "# Create x and y tick labels for each cluster\n",
    "start_pos = 0\n",
    "xtick_positions = []\n",
    "ytick_positions = []\n",
    "for c in range(1, numclusters):\n",
    "    numneurons_temp = len(np.where(clusterids_all == c)[0])\n",
    "    end_pos = start_pos + numneurons_temp\n",
    "    mid_pos = (start_pos + end_pos) // 2\n",
    "    xtick_positions.append(mid_pos)\n",
    "    ytick_positions.append(mid_pos)\n",
    "    start_pos = end_pos\n",
    "    if c != numclusters -1:\n",
    "        for i in range(2):\n",
    "            axs[i].axhline(start_pos, color='white', linewidth=1, linestyle='--')\n",
    "            axs[i].axvline(start_pos, color='white', linewidth=1, linestyle='--')\n",
    "    for i in range(2):\n",
    "        axs[i].text(-150, mid_pos, f'{c}', fontsize=10, ha='center', va='center')\n",
    "        axs[i].text(mid_pos, numneurons_significant+150, f'{c}', fontsize=10, ha='center', va='center')\n",
    "\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "    \n",
    "# Adjust margins to prevent axis labels from being cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Cluster Correlation Heatmap.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Cluster Correlation Heatmap.png'), format='PNG')\n",
    "\n",
    "plt.close()\n",
    "print('Done')\n",
    " "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITHIN SESSION RESPONSE STABILITY"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### THIS SECTION ANALYZES WITHIN SESSION TRIAL STABILITY. ADJUST PARAMETERS BELOW TO YOUR LIKING\n",
    "\n",
    "# Calculate by 'mean' or 'auROC'\n",
    "measurement = 'auROC'\n",
    "event = 'activeleverall' ### 'activeleverall' or 'cueresponse'\n",
    "\n",
    "proportion_of_trials = 0.33 ### proportion of trials at beggining and end of session to compare. Low numbers may not work if there are very few trials\n",
    "\n",
    "if measurement == 'mean':\n",
    "    xlim=([-5,5])\n",
    "    ylim=([-5,5])\n",
    "else:\n",
    "    xlim=([-1.1,1.1])\n",
    "    ylim=([-1.1,1.1])\n",
    "    \n",
    "if event == 'cueresponse':\n",
    "    epoch_baseline = [pre_window_size-int(1.6*averagedframerate), pre_window_size] ### For Cue\n",
    "    epoch_event = [pre_window_size, pre_window_size+int(1.6*averagedframerate)] ### For Cue\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE ARRAYS FOR COMPARISON OF TRIAL STABILITY\n",
    "\n",
    "# Define colors for each cluster\n",
    "cluster_colors = []\n",
    "for c in range(1, numclusters+1):\n",
    "    if c <= 4:\n",
    "        color = mcolors.to_hex(plt.cm.Purples(1-(c-1)/6))\n",
    "    else:\n",
    "        color = mcolors.to_hex(plt.cm.Greens(0.5+(c-5)/6))\n",
    "    cluster_colors.append(color)\n",
    "cluster_colors.insert(0, 'black')\n",
    "\n",
    "cellcounter = 0\n",
    "within_stability_all = np.nan*np.ones((numneurons_all, 2))\n",
    "\n",
    "if event == 'activeleverall':\n",
    "    print(event)\n",
    "    for basedir, day, animal, fov in iterate_dirs(basedir, days):\n",
    "        tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "        if 'alignedevents_%s_%s.npy'%(eventofinterest, separation_requirement) in tempfiles:\n",
    "            alignedevents_fov = np.load(os.path.join(basedir, day, animal, fov, 'alignedevents_%s_%s.npy'%(eventofinterest, separation_requirement)))\n",
    "            numtrials = alignedevents_fov.shape[2]\n",
    "            numtrialsforstability = int(np.floor(numtrials*proportion_of_trials))\n",
    "            numneurons = (np.shape(alignedevents_fov)[0])\n",
    "            for neuron in range(numneurons):\n",
    "                if numtrialsforstability < 2:\n",
    "                    within_stability_all[cellcounter+neuron,0] = np.nan\n",
    "                    within_stability_all[cellcounter+neuron,1] = np.nan\n",
    "                elif measurement == 'mean':\n",
    "                    within_stability_all[cellcounter+neuron,0] = np.nanmean(alignedevents_fov[neuron,aucfirstframe:auclastframe,:numtrialsforstability])- \\\n",
    "                                                     np.nanmean(alignedevents_fov[neuron,baselinefirstframe:baselinelastframe,:numtrialsforstability])\n",
    "                    within_stability_all[cellcounter+neuron,1] = np.nanmean(alignedevents_fov[neuron,aucfirstframe:auclastframe,-numtrialsforstability:])- \\\n",
    "                                                     np.nanmean(alignedevents_fov[neuron,baselinefirstframe:baselinelastframe,-numtrialsforstability:])\n",
    "                elif measurement == 'auROC':\n",
    "                    within_stability_all[cellcounter+neuron,0] = calculate_auROC(np.nanmean(alignedevents_fov[neuron,aucfirstframe:auclastframe,-numtrialsforstability:], axis = 1), \\\n",
    "                                                     np.nanmean(alignedevents_fov[neuron,baselinefirstframe:baselinelastframe,-numtrialsforstability:], axis = 1))[0]\n",
    "                    within_stability_all[cellcounter+neuron,1] = calculate_auROC(np.nanmean(alignedevents_fov[neuron,aucfirstframe:auclastframe,:numtrialsforstability], axis = 1), \\\n",
    "                                                     np.nanmean(alignedevents_fov[neuron,baselinefirstframe:baselinelastframe,:numtrialsforstability], axis = 1))[0]\n",
    "            cellcounter+=numneurons\n",
    "\n",
    "if event == 'cueresponse':\n",
    "    for basedir, day, animal, fov in iterate_dirs(basedir, days):\n",
    "        tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "        if 'alignedevents_%s_%s.npy'%('activeleverall', separation_requirement) in tempfiles:\n",
    "            alignedevents_fov = np.load(os.path.join(basedir, day, animal, fov, 'alignedevents_%s_%s.npy'%('activeleverall', separation_requirement)))\n",
    "            numneurons = (np.shape(alignedevents_fov)[0])\n",
    "            if 'alignedevents_%s_%s.npy'%('activelevertimeout', separation_requirement) not in tempfiles:\n",
    "                for neuron in range(numneurons):\n",
    "                    within_stability_all[cellcounter+neuron,0] = np.nan\n",
    "                    within_stability_all[cellcounter+neuron,1] = np.nan\n",
    "            else:\n",
    "                alignedevents1_fov = np.load(os.path.join(basedir, day, animal, fov, 'alignedevents_%s_%s.npy'%('activelever', separation_requirement)))\n",
    "                alignedevents2_fov = np.load(os.path.join(basedir, day, animal, fov, 'alignedevents_%s_%s.npy'%('activelevertimeout', separation_requirement)))\n",
    "                numtrials1 = alignedevents1_fov.shape[2]\n",
    "                numtrialsforstability1 = int(np.floor(numtrials1*proportion_of_trials))\n",
    "                numtrials2 = alignedevents2_fov.shape[2]\n",
    "                numtrialsforstability2 = int(np.floor(numtrials2*proportion_of_trials))\n",
    "                for neuron in range(numneurons):\n",
    "                    alignedevents1_neuron = alignedevents1_fov[neuron, :, :]\n",
    "                    alignedevents2_neuron = alignedevents2_fov[neuron, :, :]\n",
    "                    alignedevents1_neuron_baseline = np.mean(alignedevents1_neuron[epoch_baseline[0]:epoch_baseline[1],:], axis = 0)\n",
    "                    alignedevents2_neuron_baseline = np.mean(alignedevents2_neuron[epoch_baseline[0]:epoch_baseline[1],:], axis = 0)\n",
    "                    alignedevents1_neuron = alignedevents1_neuron - alignedevents1_neuron_baseline[None, :]\n",
    "                    alignedevents2_neuron = alignedevents2_neuron = alignedevents2_neuron_baseline[None, :]\n",
    "                    cueresponse = alignedevents1_neuron - np.mean(alignedevents2_neuron, axis = 1)[:, None]                     \n",
    "                    if numtrialsforstability1 < 2 or numtrialsforstability2 < 2:\n",
    "                        within_stability_all[cellcounter+neuron,0] = np.nan\n",
    "                        within_stability_all[cellcounter+neuron,1] = np.nan\n",
    "                    elif measurement == 'mean':\n",
    "                        within_stability_all[cellcounter+neuron,0] = np.nanmean(cueresponse[epoch_event[0]:epoch_event[1],:numtrialsforstability1])- \\\n",
    "                                                         np.nanmean(cueresponse[epoch_baseline[0]:epoch_baseline[1],:numtrialsforstability1])\n",
    "                        within_stability_all[cellcounter+neuron,1] = np.nanmean(cueresponse[epoch_event[0]:epoch_event[1],-numtrialsforstability1:])- \\\n",
    "                                                         np.nanmean(cueresponse[epoch_baseline[0]:epoch_baseline[1],-numtrialsforstability1:])\n",
    "                    elif measurement == 'auROC':\n",
    "                        within_stability_all[cellcounter+neuron,0] = calculate_auROC(np.nanmean(cueresponse[epoch_event[0]:epoch_event[1],-numtrialsforstability1:], axis = 1), \\\n",
    "                                                         np.nanmean(cueresponse[epoch_baseline[0]:epoch_baseline[1],-numtrialsforstability1:], axis = 1))[0]\n",
    "                        within_stability_all[cellcounter+neuron,1] = calculate_auROC(np.nanmean(cueresponse[epoch_event[0]:epoch_event[1],:numtrialsforstability1], axis = 1), \\\n",
    "                                                         np.nanmean(cueresponse[epoch_baseline[0]:epoch_baseline[1],:numtrialsforstability1], axis = 1))[0]\n",
    "            cellcounter+=numneurons\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ALL DAYS CORRELATION\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "valid_indices = ~np.isnan(within_stability_all[:, 0])\n",
    "rval, rpval = stats.pearsonr(within_stability_all[:, 0][valid_indices], within_stability_all[:, 1][valid_indices])\n",
    "tval, tpval = stats.ttest_rel(within_stability_all[:, 0][valid_indices], within_stability_all[:, 1][valid_indices])\n",
    "\n",
    "# Create a DataFrame from within_stability_all\n",
    "df = pd.DataFrame(within_stability_all, columns=['EarlyTrials', 'LateTrials'])\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 2.5))\n",
    "\n",
    "# Subplot 1: Correlation plot\n",
    "axs[0].scatter(x='EarlyTrials', y='LateTrials', data=df, \n",
    "                color='black', alpha=.2, s=1)\n",
    "sns.regplot(x='EarlyTrials', y='LateTrials', data=df, ax=axs[0],\n",
    "            line_kws={'color': 'red', 'linewidth': 1}, scatter=False)\n",
    "\n",
    "axs[0].set_xlabel('Early Trials')  # Set the x-axis label\n",
    "axs[0].set_ylabel('Late Trials')  # Set the y-axis label\n",
    "axs[0].set_title('Trial Stability', fontsize=10)  # Set the title of the plot\n",
    "print(f'r = {rval:.5f}, p = {rpval: .5f}')  # Display correlation information\n",
    "print(f't = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "axs[0].set_xlim(xlim[0], xlim[1])  # Set equal aspect ratio\n",
    "axs[0].set_ylim(ylim[0], ylim[1])  # Set equal aspect ratio\n",
    "\n",
    "if measurement == 'mean':\n",
    "    axs[0].set_xscale('symlog')\n",
    "    axs[0].set_yscale('symlog')\n",
    "\n",
    "# Calculate means and SEM\n",
    "mean_early = np.nanmean(within_stability_all[:, 0])\n",
    "mean_late = np.nanmean(within_stability_all[:, 1])\n",
    "sem_early = stats.sem(within_stability_all[:, 0],  nan_policy='omit')\n",
    "sem_late = stats.sem(within_stability_all[:, 1], nan_policy='omit')\n",
    "\n",
    "# Subplot 2: Bar graph with error bars representing SEM\n",
    "sns.barplot(x=['EarlyTrials', 'LateTrials'], y=[mean_early, mean_late], ax=axs[1], color = 'gray')\n",
    "axs[1].errorbar(x=['EarlyTrials', 'LateTrials'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "\n",
    "axs[1].set_ylabel('%s %s'%(event, measurement))\n",
    "axs[1].set_ylim(0, .1)\n",
    "\n",
    "axs[1].set_title('Average Response by Group')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability, All %s %s.PDF' % (event, measurement)), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability, All %s %s.png' % (event, measurement)), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### ALL CLUSTERS CORRELATION\n",
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "fig, axs = plt.subplots(2, 2, figsize=(4, 4))\n",
    "sns.set_style('white')\n",
    "\n",
    "# Create a scatter plot with a regression line for clustered neurons\n",
    "for c in range(1,numclusters):\n",
    "    temp_clustered = within_stability_all[np.where(clusterids_all == c)]\n",
    "    \n",
    "    ax = axs[0,0]\n",
    "    ax.scatter(x=temp_clustered[:, 0], y=temp_clustered[:, 1], s = 1,\n",
    "        color = cluster_colors[c], alpha = .2)\n",
    "\n",
    "ax.set_ylabel('Late Trials')  # Set the y-axis label\n",
    "\n",
    "ax.set_xlabel('Early Trials')  # Set the x-axis label\n",
    "ax.set_title('clustered cells', fontsize=10)  # Set the title of the plot\n",
    "\n",
    "temp_clustered = within_stability_all[np.where(clusterids_all > 0)]\n",
    "sns.regplot(x=temp_clustered[:, 0], y=temp_clustered[:, 1], ax = ax, scatter = False,\n",
    "           line_kws={'color': 'red', 'linewidth': 1})\n",
    "\n",
    "# Create a scatter plot with a regression line for not clustered neurons\n",
    "ax = axs[0,1]\n",
    "temp_notclustered = within_stability_all[np.where(clusterids_all == 0)]\n",
    "ax.scatter(x=temp_notclustered[:, 0], y=temp_notclustered[:, 1], s = 1,\n",
    "    color = 'k', alpha = .2)\n",
    "sns.regplot(x=temp_notclustered[:, 0], y=temp_notclustered[:, 1], ax = ax, scatter = False,\n",
    "           line_kws={'color': 'red', 'linewidth': 1})\n",
    "\n",
    "ax.set_ylabel('Late Trials')  # Set the y-axis label\n",
    "ax.set_xlabel('Early Trials')  # Set the x-axis label\n",
    "ax.set_title('unclustered cells', fontsize=10)  # Set the title of the plot\n",
    "\n",
    "# Calculate means and SEM\n",
    "mean_early = np.nanmean(within_stability_all[:, 0][np.where(clusterids_all > 0)])\n",
    "mean_late = np.nanmean(within_stability_all[:, 1][np.where(clusterids_all > 0)])\n",
    "sem_early = stats.sem(within_stability_all[:, 0][np.where(clusterids_all > 0)],  nan_policy='omit')\n",
    "sem_late = stats.sem(within_stability_all[:, 1][np.where(clusterids_all > 0)], nan_policy='omit')\n",
    "\n",
    "# Subplot 2: Bar graph with error bars representing SEM\n",
    "ax = axs[1, 0]\n",
    "sns.barplot(x=['EarlyTrials', 'LateTrials'], y=[mean_early, mean_late], ax=ax, color = 'purple')\n",
    "ax.errorbar(x=['EarlyTrials', 'LateTrials'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "\n",
    "# Calculate means and SEM\n",
    "mean_early = np.nanmean(within_stability_all[:, 0][np.where(clusterids_all == 0)])\n",
    "mean_late = np.nanmean(within_stability_all[:, 1][np.where(clusterids_all == 0)])\n",
    "sem_early = stats.sem(within_stability_all[:, 0][np.where(clusterids_all == 0)],  nan_policy='omit')\n",
    "sem_late = stats.sem(within_stability_all[:, 1][np.where(clusterids_all == 0)], nan_policy='omit')\n",
    "\n",
    "# Subplot 2: Bar graph with error bars representing SEM\n",
    "ax = axs[1, 1]\n",
    "sns.barplot(x=['EarlyTrials', 'LateTrials'], y=[mean_early, mean_late], ax=ax, color = 'gray')\n",
    "ax.errorbar(x=['EarlyTrials', 'LateTrials'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "\n",
    "# Calculate Pearson correlations and T-tests\n",
    "valid_indices = ~np.isnan(temp_clustered[:, 0]) & ~np.isnan(temp_clustered[:, 1])\n",
    "rval, rpval = stats.pearsonr(temp_clustered[:, 0][valid_indices], temp_clustered[:, 1][valid_indices])\n",
    "tval, tpval = stats.ttest_rel(temp_clustered[:, 0][valid_indices],temp_clustered[:, 1][valid_indices])\n",
    "print(f'clustered r = {rval:.5f}, p = {rpval: .5f}')  # Display correlation information\n",
    "print(f'clustered t = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "valid_indices = ~np.isnan(temp_notclustered[:, 0]) & ~np.isnan(temp_notclustered[:, 1])\n",
    "rval, rpval = stats.pearsonr(temp_notclustered[:, 0][valid_indices],temp_notclustered[:, 1][valid_indices])\n",
    "tval, tpval = stats.ttest_rel(temp_notclustered[:, 0][valid_indices],temp_notclustered[:, 1][valid_indices])\n",
    "print(f'not clustered r = {rval:.5f}, p = {rpval: .5f}')  # Display correlation information\n",
    "print(f'not clustered t = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "for column in range(2):\n",
    "    ax = axs[0, column]\n",
    "    ax.set_xlim(xlim[0], xlim[1])  \n",
    "    ax.set_ylim(ylim[0], ylim[1]) \n",
    "    if measurement == 'mean':\n",
    "        ax.set_xscale('symlog')\n",
    "        ax.set_yscale('symlog')\n",
    "    ax = axs[1, column]\n",
    "    ax.set_ylim(0, 0.25)  \n",
    "    \n",
    "plt.tight_layout()  \n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability, Clusters %s %s.PDF'%(event, measurement)), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability, Clusters %s %s.png'%(event, measurement)), format='PNG')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### ALL CLUSTERS BY DAY CORRELATION\n",
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "fig, axs = plt.subplots(2, numdays, figsize=(1.8*numdays, 3.5))\n",
    "sns.set_style('white')\n",
    "\n",
    "heatmap_array = np.nan*np.ones((2, numdays))\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    # Create a scatter plot with a regression line for clustered neurons on each day\n",
    "    for c in range(1,numclusters):\n",
    "        temp_clustered = within_stability_all[idxs_day[day]][np.where(clusterids_all[idxs_day[day]] == c )]\n",
    "        ax = axs[0, d]\n",
    "        ax.scatter(x=temp_clustered[:, 0], y=temp_clustered[:, 1], s = 3,\n",
    "            color = cluster_colors[c], alpha = .2)\n",
    "    if d != 0:\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "\n",
    "    ax.set_title(day, fontsize=10)  # Set the title of the plot\n",
    "\n",
    "    temp_clustered = within_stability_all[idxs_day[day]][np.where(clusterids_all[idxs_day[day]] > 0 )]\n",
    "    sns.regplot(x=temp_clustered[:, 0], y=temp_clustered[:, 1], ax = ax, scatter = False,\n",
    "               line_kws={'color': 'red', 'linewidth': 1})\n",
    "    \n",
    "    # Create a scatter plot with a regression line for not clustered neurons\n",
    "    ax = axs[1, d]\n",
    "    temp_notclustered = within_stability_all[idxs_day[day]][np.where(clusterids_all[idxs_day[day]] == 0)]\n",
    "    ax.scatter(x=temp_notclustered[:, 0], y=temp_notclustered[:, 1], s = 3,\n",
    "        color = 'k', alpha = .2)\n",
    "    sns.regplot(x=temp_notclustered[:, 0], y=temp_notclustered[:, 1], ax = ax, scatter = False,\n",
    "               line_kws={'color': 'red', 'linewidth': 1})\n",
    "    \n",
    "    if d != 0:\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    \n",
    "    for row in axs:\n",
    "        for ax in row:\n",
    "            ax.set_xlim(xlim[0], xlim[1])  \n",
    "            ax.set_ylim(ylim[0], ylim[1]) \n",
    "            if measurement == 'mean':\n",
    "                ax.set_xscale('symlog')\n",
    "                ax.set_yscale('symlog')\n",
    "    \n",
    "    # Calculate Pearson correlations\n",
    "    if np.count_nonzero(~np.isnan(temp_clustered[:,0])) > 2:\n",
    "        valid_indices = ~np.isnan(temp_clustered[:, 0]) & ~np.isnan(temp_clustered[:, 1])\n",
    "        rval, pval = stats.pearsonr(temp_clustered[:, 0][valid_indices], temp_clustered[:, 1][valid_indices])\n",
    "        print(day, 'clustered, r = %s, p = %s'%(rval, pval))\n",
    "    heatmap_array[0, d] = rval\n",
    "\n",
    "    if np.count_nonzero(~np.isnan(temp_notclustered[:,0])) > 2:\n",
    "        valid_indices = ~np.isnan(temp_notclustered[:, 0]) & ~np.isnan(temp_notclustered[:, 1])\n",
    "        rval, pval = stats.pearsonr(temp_notclustered[:, 0][valid_indices],temp_notclustered[:, 1][valid_indices])\n",
    "        print(day, 'notclustered, r = %s, p = %s'%(rval, pval))\n",
    "    heatmap_array[1, d] = rval\n",
    "\n",
    "plt.tight_layout()  \n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability Correlation, Day %s %s.PDF'%(event, measurement)), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability Correlation, Day %s %s.png'%(event, measurement)), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Extract the purple and grey segments from the respective colormaps\n",
    "prgn_r = plt.get_cmap('PRGn_r')\n",
    "rdgy_r = plt.get_cmap('RdGy_r')\n",
    "\n",
    "# Define the number of colors you want from each colormap segment\n",
    "n_colors = 256\n",
    "n_half_colors = n_colors // 2\n",
    "\n",
    "# Create a new colormap by combining the two segments\n",
    "colors = np.vstack((\n",
    "    rdgy_r(np.linspace(0, 0.5, n_half_colors)),\n",
    "    prgn_r(np.linspace(0.5, 1, n_half_colors))\n",
    "))\n",
    "\n",
    "# Create the custom colormap\n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('custom_cmap', colors)\n",
    "\n",
    "# Now plot using the custom colormap\n",
    "fig, axs = plt.subplots(1, figsize=(2, 3), sharex=True)\n",
    "\n",
    "# First heatmap\n",
    "ax = axs\n",
    "sns.heatmap(heatmap_array.T, cmap=custom_cmap, vmin=-0.8, vmax=0.8, ax=ax, cbar=True, square=False)\n",
    "ax.set_title('r-vals trial stability')\n",
    "ax.set_xlabel('Clustered or NC')\n",
    "ax.set_ylabel('Day')\n",
    "\n",
    "\n",
    "# Adjust layout and save figures\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability Heatmap Rvals, Day %s %s.PDF' % (event, measurement)), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability Heatmap Rvals, Day %s %s.png' % (event, measurement)), format='PNG')\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## ALL CLUSTERS BY DAY BAR GRAPHS\n",
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "warnings.filterwarnings('ignore') ### temporarily ignore warnings, will turn back on at end. This is due to SEM being calculated when there is no data in some cases (e.g., cue response)\n",
    "\n",
    "fig, axs = plt.subplots(2, numdays, figsize=(1*numdays, 3), sharey = True)\n",
    "sns.set_style('white')\n",
    "\n",
    "for d, day in enumerate(sorted(days)):    \n",
    "    temp_clustered = within_stability_all[idxs_day[day]][np.where(clusterids_all[idxs_day[day]] > 0)]\n",
    "    temp_notclustered = within_stability_all[idxs_day[day]][np.where(clusterids_all[idxs_day[day]] == 0 )]\n",
    "\n",
    "    # Calculate means and SEM of clustered\n",
    "    mean_early = np.nanmean(temp_clustered[:, 0])\n",
    "    mean_late = np.nanmean(temp_clustered[:, 1])\n",
    "    if math.isnan(mean_early) == False:\n",
    "        sem_early = stats.sem(temp_clustered[:, 0],  nan_policy='omit')\n",
    "        sem_late = stats.sem(temp_clustered[:, 1], nan_policy='omit')\n",
    "    \n",
    "    # Subplot row 1: Bar graph with error bars representing SEM for clustered data\n",
    "    ax = axs[0, d]\n",
    "    if math.isnan(mean_early) == False:\n",
    "        sns.barplot(x=['Early', 'Late'], y=[mean_early, mean_late], ax=ax, color = 'purple')\n",
    "        ax.errorbar(x=['Early', 'Late'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "    \n",
    "    ax.set_title(day, fontsize=10)  # Set the title of the plot\n",
    "    if d != 0:\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Calculate means and SEM of not clustered\n",
    "    mean_early = np.nanmean(temp_notclustered[:, 0])\n",
    "    mean_late = np.nanmean(temp_notclustered[:, 1])\n",
    "    if math.isnan(mean_early) == False:\n",
    "        sem_early = stats.sem(temp_notclustered[:, 0],  nan_policy='omit')\n",
    "        sem_late = stats.sem(temp_notclustered[:, 1], nan_policy='omit')\n",
    "\n",
    "    # Subplot row 2: Bar graph with error bars representing SEM for clustered data\n",
    "    ax = axs[1, d]\n",
    "    if math.isnan(mean_early) == False:\n",
    "        sem_early = stats.sem(temp_notclustered[:, 0],  nan_policy='omit')\n",
    "        sem_late = stats.sem(temp_notclustered[:, 1], nan_policy='omit')\n",
    "        \n",
    "        sns.barplot(x=['Early', 'Late'], y=[mean_early, mean_late], ax=ax, color = 'gray')\n",
    "        ax.errorbar(x=['Early', 'Late'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "        \n",
    "    if d != 0:\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Calculate T-test values \n",
    "    if np.count_nonzero(~np.isnan(temp_clustered[:,0])) > 2:\n",
    "        valid_indices = ~np.isnan(temp_clustered[:, 0]) & ~np.isnan(temp_clustered[:, 1])\n",
    "        tval, tpval = stats.ttest_rel(temp_clustered[:, 0][valid_indices], temp_clustered[:, 1][valid_indices])\n",
    "        print(day, f'clustered t = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "    if np.count_nonzero(~np.isnan(temp_notclustered[:,0])) > 2:\n",
    "        valid_indices = ~np.isnan(temp_notclustered[:, 0]) & ~np.isnan(temp_notclustered[:, 1])\n",
    "        tval, tpval = stats.pearsonr(temp_notclustered[:, 0][valid_indices],temp_notclustered[:, 1][valid_indices])\n",
    "        print(f'notclustered t = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability BarChart, Day %s %s.PDF'%(event, measurement)), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability BarChart, Day %s %s.png'%(event, measurement)), format='PNG')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "warnings.filterwarnings('default') ### restores warnings\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### ALL CLUSTERS BY DAY BAR GRAPHS\n",
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "warnings.filterwarnings('ignore') ### temporarily ignore warnings, will turn back on at end. This is due to SEM being calculated when there is no data in some cases (e.g., cue response)\n",
    "fig, axs = plt.subplots(numclusters, numdays, figsize=(1*numdays, 2*numclusters), sharey = True)\n",
    "sns.set_style('white')\n",
    "for c in range(numclusters):\n",
    "    for d, day in enumerate(sorted(days)):    \n",
    "        temp_clustered = within_stability_all[idxs_day[day]][np.where(clusterids_all[idxs_day[day]] == c)]\n",
    "\n",
    "        # Calculate means and SEM of clustered\n",
    "        mean_early = np.nanmean(temp_clustered[:, 0])\n",
    "        mean_late = np.nanmean(temp_clustered[:, 1])\n",
    "        if math.isnan(mean_early) == False:\n",
    "            sem_early = stats.sem(temp_clustered[:, 0],  nan_policy='omit')\n",
    "            sem_late = stats.sem(temp_clustered[:, 1], nan_policy='omit')\n",
    "\n",
    "        # Subplot row 1: Bar graph with error bars representing SEM for clustered data\n",
    "        ax = axs[c, d]\n",
    "        if math.isnan(mean_early) == False:\n",
    "            sns.barplot(x=['Early', 'Late'], y=[mean_early, mean_late], ax=ax, color = cluster_colors[c])\n",
    "            ax.errorbar(x=['Early', 'Late'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_title(day, fontsize=10)  # Set the title of the plot\n",
    "        \n",
    "        if c != numclusters-1:\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "\n",
    "        if d != 0:\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # Calculate T-test values \n",
    "        if np.count_nonzero(~np.isnan(temp_clustered[:,0])) > 2:\n",
    "            valid_indices = ~np.isnan(temp_clustered[:, 0]) & ~np.isnan(temp_clustered[:, 1])\n",
    "            tval, tpval = stats.ttest_rel(temp_clustered[:, 0][valid_indices], temp_clustered[:, 1][valid_indices])\n",
    "            print(day, 'cluster = ', c, f'clustered t = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability BarChart, Cluster %s %s.PDF'%(event, measurement)), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Trial Stability BarChart, Cluster %s %s.png'%(event, measurement)), format='PNG')\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "warnings.filterwarnings('default') ### restores warnings\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### EACH CLUSTER CORRELATION\n",
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "fig, axs = plt.subplots(1, numclusters, figsize=(numdays*2, 2))\n",
    "sns.set_style('white')\n",
    "\n",
    "for c in range(numclusters):\n",
    "    temp = within_stability_all[np.where(clusterids_all == c)]\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "#     rval, pval = stats.pearsonr(temp[:,0], temp[:,1])\n",
    "    \n",
    "    ax = axs[c]\n",
    "    \n",
    "    # Create a scatter plot with a regression line\n",
    "    sns.regplot(x=temp[:, 0], y=temp[:, 1], ax=ax, \n",
    "           line_kws={'color': 'red', 'linewidth': 1}, scatter_kws={'color':cluster_colors[c], 'alpha': 0.1, 's': 10})\n",
    "\n",
    "    if c == 0:\n",
    "        ax.set_ylabel('Late Trials')  # Set the y-axis label\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(xlim[0], xlim[1])  # Set equal aspect ratio\n",
    "    ax.set_ylim(ylim[0], ylim[1])  # Set equal aspect ratio\n",
    "    \n",
    "    ax.set_xlabel('Early Trials')  # Set the x-axis label\n",
    "    ax.set_title(f'cluster = {c}', fontsize=10)  # Set the title of the plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETWEEN SESSION RESPONSE STABILITY"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LOAD TRACKING IDS FOR EACH DAY AND ANIMAL\n",
    "tracking_ids = {}\n",
    "tempfiles = next(os.walk(os.path.join(basedir, 'CellTracking', 'AllDays')))[2]\n",
    "for csvfile in sorted(tempfiles):\n",
    "    if os.path.splitext(csvfile)[1] == '.csv':\n",
    "        animal = csvfile.split(\"_\")[0]\n",
    "        with open(os.path.join(basedir, 'CellTracking', 'AllDays', csvfile)) as temp_packaged:\n",
    "            temp = np.loadtxt(temp_packaged, delimiter=\",\")\n",
    "            tracking_ids[animal] = (temp[:, :numdays] - 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CREATE DICTIONARIES FOR EACH VARIABLE OF INTEREST WITH DATA ALIGNED BY TRACKING ID\n",
    "\n",
    "max_numneurons = 3000 # Set to arbitrarily high number that exceeds the maximum number of neurons in any fov on any day, but is lower than the number that you put in for cells that weren't tracked\n",
    "\n",
    "# Create empty dictionaries for each variable\n",
    "tracked_animals = []\n",
    "tracked_popevents_fov = {}\n",
    "tracked_aucpvals_fov = {}\n",
    "tracked_clusterids_fov = {}\n",
    "\n",
    "# Loop through each folder to load/create data for each fov\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):     \n",
    "    # Check to ensure popevents has been created for each fov, otherwise skip that fov\n",
    "    try: \n",
    "        popevents_fov[day][animal][fov]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "    # Create nested dictionaries for each variable (popevents, aucpvals, etc) for each fov, and fill with empty array (np.nans)\n",
    "    # Skips fovs that do not have tracking ids loaded\n",
    "    if animal not in tracked_popevents_fov:\n",
    "        tracked_animals = np.append(tracked_animals, animal)\n",
    "        temp_numneurons = np.shape(tracking_ids[animal])[0]\n",
    "        tracked_popevents_fov[animal] = np.nan*np.ones((numdays, temp_numneurons, window_size))\n",
    "        tracked_aucpvals_fov[animal] = np.nan*np.ones((numdays, temp_numneurons, 2))\n",
    "        tracked_clusterids_fov[animal] = np.nan*np.ones((numdays, temp_numneurons))\n",
    "\n",
    "for animal in tracked_popevents_fov.keys():\n",
    "    temp_numneurons = len(tracking_ids[animal])\n",
    "    for d, day in enumerate(sorted(days)):\n",
    "        try:\n",
    "            tempkeys = popevents_fov[day][animal].keys()\n",
    "            fovkey = list(tempkeys)[0]\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "        for tracked_neuron in range(temp_numneurons):\n",
    "            temp_cellid = tracking_ids[animal][tracked_neuron, d].astype(int)\n",
    "            if temp_cellid < max_numneurons:\n",
    "                tracked_popevents_fov[animal][d, tracked_neuron, :] = popevents_fov[day][animal][fovkey][temp_cellid,:]\n",
    "                tracked_aucpvals_fov[animal][d, tracked_neuron, :] = aucpvals_fov[day][animal][fovkey][temp_cellid,:]\n",
    "                tracked_clusterids_fov[animal][d, tracked_neuron] = clusterids_fov[day][animal][fovkey][temp_cellid]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CREATE POPEVENT, AUCPVAL, AND CLUSTER ID ARRAYS WHERE RESPONSES ARE TRACKED ACROSS ANY TWO SESSIONS (day1, day2)\n",
    "\n",
    "max_trackedneurons_all = 39000 #set to higher number than the number of tracked neurons for all day-to-day comparisons combined\n",
    "max_trackedneurons_day = 3000\n",
    "trackedcellcounter_all = 0\n",
    "\n",
    "###Arrays for any day-to-day comparisons\n",
    "tracked_popevents_all = np.nan*np.ones((max_trackedneurons_all, window_size*2))\n",
    "tracked_aucpvals_all = np.nan*np.ones((max_trackedneurons_all, 4))\n",
    "tracked_clusterids_all = np.nan*np.ones((max_trackedneurons_all, 2))\n",
    "\n",
    "###Arrays for specific day-to-day comparisons\n",
    "tracked_popevents_days = {}\n",
    "tracked_aucpvals_days = {}\n",
    "tracked_clusterids_days = {}\n",
    "\n",
    "\n",
    "###Loop through day-to-day comparisons and animals and fill the empty arrays above\n",
    "for d1, day1 in enumerate(sorted(days)):\n",
    "    print(day1)\n",
    "    if day1 not in tracked_popevents_days:\n",
    "        tracked_popevents_days[day1] = {}\n",
    "        tracked_aucpvals_days[day1] = {}\n",
    "        tracked_clusterids_days[day1] = {}\n",
    "\n",
    "    for d2, day2 in enumerate(sorted(days)):\n",
    "        tracked_popevents_days[day1][day2] = np.nan*np.ones((max_trackedneurons_day, window_size * 2))\n",
    "        tracked_aucpvals_days[day1][day2] = np.nan*np.ones((max_trackedneurons_day, 4))\n",
    "        tracked_clusterids_days[day1][day2] = np.nan*np.ones((max_trackedneurons_day, 2))\n",
    "        trackedcellcounter_day = 0\n",
    "            \n",
    "        for tracked_animal in sorted(tracked_animals):\n",
    "            temp_numneurons = np.shape(tracking_ids[tracked_animal])[0]\n",
    "\n",
    "            for tracked_neuron in range(temp_numneurons):\n",
    "                response1 = tracked_popevents_fov[tracked_animal][d1, tracked_neuron, :]\n",
    "                response2 = tracked_popevents_fov[tracked_animal][d2, tracked_neuron, :]\n",
    "                neuron_responses = np.hstack((response1, response2))\n",
    "\n",
    "                aucpvals1 = tracked_aucpvals_fov[tracked_animal][d1, tracked_neuron, :]\n",
    "                aucpvals2 = tracked_aucpvals_fov[tracked_animal][d2, tracked_neuron, :]\n",
    "                neuron_aucpvals = np.hstack((aucpvals1, aucpvals2))\n",
    "\n",
    "                cluster1 = tracked_clusterids_fov[tracked_animal][d1, tracked_neuron]\n",
    "                cluster2 = tracked_clusterids_fov[tracked_animal][d2, tracked_neuron]\n",
    "                neuron_clusters = np.hstack((cluster1, cluster2))\n",
    "\n",
    "                    \n",
    "                if not np.any(np.isnan(neuron_responses)):\n",
    "                    tracked_popevents_days[day1][day2][trackedcellcounter_day, :] = neuron_responses\n",
    "                    tracked_aucpvals_days[day1][day2][trackedcellcounter_day, :] = neuron_aucpvals\n",
    "                    tracked_clusterids_days[day1][day2][trackedcellcounter_day, :] = neuron_clusters\n",
    "                    trackedcellcounter_day += 1\n",
    "\n",
    "                    if d1 != d2:\n",
    "                        tracked_popevents_all[trackedcellcounter_all, :] = neuron_responses\n",
    "                        tracked_aucpvals_all[trackedcellcounter_all, :] = neuron_aucpvals\n",
    "                        tracked_clusterids_all[trackedcellcounter_all, :] = neuron_clusters\n",
    "                        trackedcellcounter_all += 1\n",
    "                        \n",
    "            if tracked_animal == sorted(tracked_animals)[-1]:\n",
    "                tracked_popevents_days[day1][day2] = tracked_popevents_days[day1][day2][:trackedcellcounter_day, :]\n",
    "                tracked_aucpvals_days[day1][day2] = tracked_aucpvals_days[day1][day2][:trackedcellcounter_day, :]\n",
    "                tracked_clusterids_days[day1][day2] = tracked_clusterids_days[day1][day2][:trackedcellcounter_day, :]\n",
    "\n",
    "tracked_popevents_all = tracked_popevents_all[:trackedcellcounter_all, :]\n",
    "tracked_aucpvals_all = tracked_aucpvals_all[:trackedcellcounter_all, :]\n",
    "tracked_clusterids_all = tracked_clusterids_all[:trackedcellcounter_all, :]\n",
    "\n",
    "# Create shuffled control array for tracked popevent arrays \n",
    "shuffled_popevents_days = {}\n",
    "\n",
    "for d1, day1 in enumerate(sorted(days)):\n",
    "    shuffled_popevents_days[day1] = {}\n",
    "    \n",
    "    for d2, day2 in enumerate(sorted(days)):\n",
    "        responses1 = tracked_popevents_days[day1][day2][:, :window_size]  # Create a copy to avoid modifying the original array\n",
    "        responses2 = tracked_popevents_days[day1][day2][:, window_size:]\n",
    "        \n",
    "        rangeofneurons = range(np.shape(tracked_popevents_days[day1][day2])[0])\n",
    "        \n",
    "        shuffled_rangeofneurons = np.random.permutation(rangeofneurons)\n",
    "        shuffled_responses2 = responses2[shuffled_rangeofneurons, :]\n",
    "        shuffled_popevents_days[day1][day2] = np.hstack((responses1, shuffled_responses2))\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BETWEEN SESSION TRACKING: PLOT RESPONSES FOR DAY TO DAY COMPARISONS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE POPEVENT HEATMAPS FOR ALL DAYS, SHOWING TRACKED RESPONSES ON ONE DAY VS ANY OTHER DAY. \n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 3)) ### NOTE!!! MAKE LARGE IF USING SEABORN! IF THE GRAPH IS SMALL, THE HEATMAP WILL NOT BE PRESENTED IN A MANNER THAT CORRECTLY REFLECTS THE DATA\n",
    "\n",
    "cmax = 0.3\n",
    "cmin = -0.3\n",
    "\n",
    "tempresponse = np.nanmean(tracked_popevents_all[:, :window_size], axis=1)\n",
    "sortresponse = np.argsort(tempresponse)[::-1]\n",
    "temp_numneurons = len(sortresponse)\n",
    "for subplot in range(2):\n",
    "    ax = axs[subplot]\n",
    "    \n",
    "    if subplot == 0:\n",
    "        temp = tracked_popevents_all[:, :window_size]\n",
    "    else:\n",
    "        temp = tracked_popevents_all[:, window_size:]\n",
    "        \n",
    "    # Plot the heatmap using matplotlib or seaborn (sns)\n",
    "    im = ax.imshow(temp[sortresponse], cmap=plt.get_cmap('PRGn_r'), vmin=cmin, vmax=cmax, aspect='auto') #matplotlib\n",
    "    fig.colorbar(im, ax=ax, shrink=0.2) #matplotlib\n",
    "    \n",
    "#     sns.heatmap(tracked_popevents_all[sortresponse], cmap='PRGn_r', vmin=-cmax, vmax=cmax, ax = ax) #seaborn\n",
    "\n",
    "    ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth = 1)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlabel('Seconds', fontsize=10)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('%s TrackedResponses'%temp_numneurons, fontsize=10)\n",
    "    ax.set_title('All Days', fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Tracking_HeatMap_All.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Tracking_HeatMap_All.PNG'), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE LINEPLOTS SHOWING RESPONSE TYPE (NS, +, vs -) ADAPTATIONS FOR ANY DAY TO DAY COMPARISON\n",
    "\n",
    "# First, plot response types (+, -, or ns) on day clusters were defined (day 1) versus the next day (day 2)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(4, 2.5))\n",
    "scalebar_size = 0.1\n",
    "\n",
    "response_types = ['notsignificant', 'excited', 'inhibited']\n",
    "response_colors = [cluster_colors[0], cluster_colors[1], 'green']\n",
    "for rt, response_type in enumerate(response_types):\n",
    "    if rt == 0:\n",
    "        temp = tracked_popevents_all[np.where(tracked_clusterids_all[:,0] == 0)]\n",
    "    elif rt == 1:\n",
    "        temp = tracked_popevents_all[np.where(np.logical_and(tracked_clusterids_all[:,0] >= 1, tracked_clusterids_all[:,0] <= 5))]\n",
    "    elif rt == 2:\n",
    "        temp = tracked_popevents_all[np.where(tracked_clusterids_all[:,0] >= 5)]\n",
    "        \n",
    "    cluster_ci = 1.96 * np.std(temp, axis=0) / np.sqrt(len(temp))  ### cluster confidence interval\n",
    "    cluster_mean = np.mean(temp, axis=0)\n",
    "    label = f'Cluster {c}' if c > 0 else 'Unclustered'       \n",
    "\n",
    "    ax = axs[0]\n",
    "    ax.plot(cluster_mean[:window_size], label=label, color=response_colors[rt], linewidth = 1)\n",
    "    ax.fill_between(range(len(cluster_mean[:window_size])), cluster_mean[:window_size] - cluster_ci[:window_size], \\\n",
    "                    cluster_mean[:window_size] + cluster_ci[:window_size], alpha=0.3, color=response_colors[rt]) \n",
    "    ax.set_title('(Any) Day 1')\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.plot(cluster_mean[window_size:], label=label, color=response_colors[rt], linewidth = 1)\n",
    "    ax.fill_between(range(len(cluster_mean[window_size:])), cluster_mean[window_size:] - cluster_ci[window_size:], \\\n",
    "                    cluster_mean[window_size:] + cluster_ci[window_size:], alpha=0.3, color=response_colors[rt]) \n",
    "    ax.set_title('(Any) Day 2')\n",
    "\n",
    "for subplot in range(len(axs)):\n",
    "    ax = axs[subplot]\n",
    "    # Add vertical dashed line at pre_window_size\n",
    "    ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth = 0.5)\n",
    "    # Add horizontal dashed line at y=0\n",
    "    ax.axhline(y=0, color='k', linestyle='--', linewidth = 0.5)\n",
    "    \n",
    "    \n",
    "    if subplot == 0:\n",
    "        ax.set_ylabel(r'%s $\\Delta F/F$'%scalebar_size, fontsize=10)  # Use LaTeX for delta symbol\n",
    "    \n",
    "    # Set x-axis and y-axis labels\n",
    "    ax.set_xlabel(f\"{window_size / averagedframerate:.1f} Seconds\")\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])    \n",
    "    \n",
    "    ax.plot([-6, -6], [0.05, 0.05+scalebar_size], 'k-', lw = 1) ### y-scale bar with a value that is the difference between the second two digits as df/f\n",
    "\n",
    "    # Remove the border surrounding each graph\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, ResponseType LinePlots.PDF'),format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, ResponseType LinePlots.png'), format='PNG')\n",
    "\n",
    "plt.show()\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE LINEPLOTS SHOWING RESPONSE TYPE (NS, +, vs -) ADAPTATIONS FOR SPECIFIC DAY-TO-DAY COMPARISONS\n",
    "\n",
    "fig, axs = plt.subplots(numdays, numdays*2, figsize=(25, 12.5))\n",
    "scalebar_size = 0.1\n",
    "\n",
    "response_types = ['notsignificant', 'excited', 'inhibited']\n",
    "response_colors = [cluster_colors[0], cluster_colors[1], 'green']\n",
    "\n",
    "for d1, day1 in enumerate(sorted(days)):\n",
    "    for d2, day2 in enumerate(sorted(days)):\n",
    "        for rt, response_type in enumerate(response_types):\n",
    "            if rt == 0:\n",
    "                temp = tracked_popevents_days[day1][day2][np.where(tracked_clusterids_days[day1][day2][:,0] == 0)]\n",
    "            elif rt == 1:\n",
    "                temp = tracked_popevents_days[day1][day2][np.where(np.logical_and(tracked_clusterids_days[day1][day2][:,0] >= 1,\\\n",
    "                                                                                  tracked_clusterids_days[day1][day2][:,0] <= 4))]\n",
    "            elif rt == 2:\n",
    "                temp = tracked_popevents_days[day1][day2][np.where(tracked_clusterids_days[day1][day2][:,0] >= 5)]\n",
    "\n",
    "            cluster_ci = 1.96 * np.std(temp, axis=0) / np.sqrt(len(temp))  ### cluster confidence interval\n",
    "            cluster_mean = np.mean(temp, axis=0)\n",
    "            label = f'Cluster {c}' if c > 0 else 'Unclustered'       \n",
    "\n",
    "            ax = axs[d1, d2*2]\n",
    "            ax.plot(cluster_mean[:window_size], label=label, color=response_colors[rt], linewidth = 1)\n",
    "#             ax.fill_between(range(len(cluster_mean[:window_size])), cluster_mean[:window_size] - cluster_ci[:window_size], \\\n",
    "#                             cluster_mean[:window_size] + cluster_ci[:window_size], alpha=0.3, color=response_colors[rt]) \n",
    "            ax.set_title(f'{day1} \\u2192') # set title and create arrow (\\u2192)\n",
    "            ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth = 0.5)\n",
    "            ax.axhline(y=0, color='k', linestyle='--', linewidth = 0.5)\n",
    "            if d2 == 0:\n",
    "                ax.set_ylabel(r'%s $\\Delta F/F$'%scalebar_size, fontsize=10)  # Use LaTeX for delta symbol\n",
    "            \n",
    "            # Set x-axis and y-axis labels\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])  \n",
    "           \n",
    "            # Remove the border surrounding each graph\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)            \n",
    "            \n",
    "            \n",
    "            ax = axs[d1, (d2*2)+1]\n",
    "            ax.plot(cluster_mean[window_size:], label=label, color=response_colors[rt], linewidth = 1)\n",
    "#             ax.fill_between(range(len(cluster_mean[window_size:])), cluster_mean[window_size:] - cluster_ci[window_size:], \\\n",
    "#                             cluster_mean[window_size:] + cluster_ci[window_size:], alpha=0.3, color=response_colors[rt]) \n",
    "            ax.set_title(f' \\u2192 {day2}') # set title and create arrow (\\u2192)\n",
    "            \n",
    "            ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth = 0.5)\n",
    "            ax.axhline(y=0, color='k', linestyle='--', linewidth = 0.5)\n",
    "            \n",
    "            # Set x-axis and y-axis labels\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])  \n",
    "\n",
    "            # Remove the border surrounding each graph\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, ResponseType LinePlots EachDay.PDF'),format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, ResponseType LinePlots EachDay.png'), format='PNG')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE LINEPLOTS SHOWING CLUSTER ADAPTATIONS FOR ANY DAY TO DAY COMPARISON\n",
    "\n",
    "# First, plot cluster responses on day cluster was defined (day 1) versus the next day (day 2)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(4, 2.5))\n",
    "scalebar_size = 0.1\n",
    "\n",
    "for c in range(numclusters):\n",
    "    temp = tracked_popevents_all[np.where(tracked_clusterids_all[:,0] == c)]\n",
    "    cluster_ci = 1.96 * np.std(temp, axis=0) / np.sqrt(len(temp))  ### cluster confidence interval\n",
    "    cluster_mean = np.mean(temp, axis=0)\n",
    "    label = f'Cluster {c}' if c > 0 else 'Unclustered'       \n",
    "\n",
    "    ax = axs[0]\n",
    "    ax.plot(cluster_mean[:window_size], label=label, color=cluster_colors[c], linewidth = 1)\n",
    "    ax.fill_between(range(len(cluster_mean[:window_size])), cluster_mean[:window_size] - cluster_ci[:window_size], \\\n",
    "                    cluster_mean[:window_size] + cluster_ci[:window_size], alpha=0.3, color=cluster_colors[c]) \n",
    "    ax.set_title('(Any) Day 1')\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.plot(cluster_mean[window_size:], label=label, color=cluster_colors[c], linewidth = 1)\n",
    "    ax.fill_between(range(len(cluster_mean[window_size:])), cluster_mean[window_size:] - cluster_ci[window_size:], \\\n",
    "                    cluster_mean[window_size:] + cluster_ci[window_size:], alpha=0.3, color=cluster_colors[c]) \n",
    "    ax.set_title('(Any) Day 2')\n",
    "\n",
    "for subplot in range(len(axs)):\n",
    "    ax = axs[subplot]\n",
    "    # Add vertical dashed line at pre_window_size\n",
    "    ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth = 0.5)\n",
    "    # Add horizontal dashed line at y=0\n",
    "    ax.axhline(y=0, color='k', linestyle='--', linewidth = 0.5)\n",
    "    \n",
    "    \n",
    "    if subplot == 0:\n",
    "        ax.set_ylabel(r'%s $\\Delta F/F$'%scalebar_size, fontsize=10)  # Use LaTeX for delta symbol\n",
    "    \n",
    "    # Set x-axis and y-axis labels\n",
    "    ax.set_xlabel(f\"{window_size / averagedframerate:.1f} Seconds\")\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])    \n",
    "    \n",
    "    ax.plot([-6, -6], [0.15, 0.15+scalebar_size], 'k-', lw = 1) ### y-scale bar with a value that is the difference between the second two digits as df/f\n",
    "\n",
    "    # Remove the border surrounding each graph\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, Cluster LinePlots.PDF'),format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, Cluster LinePlots.png'), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CREATE LINEPLOTS SHOWING RESPONSE TYPE (NS, +, vs -) ADAPTATIONS FOR SPECIFIC DAY-TO-DAY COMPARISONS\n",
    "\n",
    "fig, axs = plt.subplots(numdays, numdays*2, figsize=(25, 12.5))\n",
    "scalebar_size = 0.1\n",
    "\n",
    "response_types = ['notsignificant', 'excited', 'inhibited']\n",
    "response_colors = [cluster_colors[0], cluster_colors[1], 'green']\n",
    "\n",
    "for d1, day1 in enumerate(sorted(days)):\n",
    "    for d2, day2 in enumerate(sorted(days)):\n",
    "        for c in range(numclusters):\n",
    "            temp = tracked_popevents_days[day1][day2][np.where(tracked_clusterids_days[day1][day2][:,0] == c)]\n",
    "\n",
    "            cluster_ci = 1.96 * np.std(temp, axis=0) / np.sqrt(len(temp))  ### cluster confidence interval\n",
    "            cluster_mean = np.mean(temp, axis=0)\n",
    "            label = f'Cluster {c}' if c > 0 else 'Unclustered'       \n",
    "\n",
    "            ax = axs[d1, d2*2]\n",
    "            ax.plot(cluster_mean[:window_size], label=label, color=cluster_colors[c], linewidth = 1)\n",
    "#             ax.fill_between(range(len(cluster_mean[:window_size])), cluster_mean[:window_size] - cluster_ci[:window_size], \\\n",
    "#                             cluster_mean[:window_size] + cluster_ci[:window_size], alpha=0.3, color=response_colors[rt]) \n",
    "            ax.set_title(f'{day1} \\u2192') # set title and create arrow (\\u2192)\n",
    "            ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth = 0.5)\n",
    "            ax.axhline(y=0, color='k', linestyle='--', linewidth = 0.5)\n",
    "            if d2 == 0:\n",
    "                ax.set_ylabel(r'%s $\\Delta F/F$'%scalebar_size, fontsize=10)  # Use LaTeX for delta symbol\n",
    "            \n",
    "            # Set x-axis and y-axis labels\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])  \n",
    "           \n",
    "            # Remove the border surrounding each graph\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)            \n",
    "            \n",
    "            \n",
    "            ax = axs[d1, (d2*2)+1]\n",
    "            ax.plot(cluster_mean[window_size:], label=label, color=cluster_colors[c], linewidth = 1)\n",
    "#             ax.fill_between(range(len(cluster_mean[window_size:])), cluster_mean[window_size:] - cluster_ci[window_size:], \\\n",
    "#                             cluster_mean[window_size:] + cluster_ci[window_size:], alpha=0.3, color=response_colors[rt]) \n",
    "            ax.set_title(f' \\u2192 {day2}') # set title and create arrow (\\u2192)\n",
    "            \n",
    "            ax.axvline(x=pre_window_size, color='k', linestyle='--', linewidth = 0.5)\n",
    "            ax.axhline(y=0, color='k', linestyle='--', linewidth = 0.5)\n",
    "            \n",
    "            # Set x-axis and y-axis labels\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])  \n",
    "\n",
    "            # Remove the border surrounding each graph\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, Cluster LinePlots EachDay.PDF'),format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, Cluster LinePlots EachDay.png'), format='PNG')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BETWEEN SESSION TRACKING: PLOT RESPONSE CORRELATIONS (PEARSONS CALCULATED BY COMPARING RESPONSE SHAPE BETWEEN TWO DAYS) FOR DAY TO DAY COMPARISONS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create Pearson-R arrays for tracked popevents and shuffled popevents\n",
    "# Shuffling array is created by shuffling popevent order in each fov, rather than all fovs, to ensure correlations aren't due to within fov response similarities\n",
    "tracked_rval_days = {}\n",
    "shuffled_rval_days = {}\n",
    "\n",
    "for d1, day1 in enumerate(sorted(days)):\n",
    "    if day1 not in tracked_rval_days:\n",
    "        tracked_rval_days[day1] = {}\n",
    "        shuffled_rval_days[day1] = {}\n",
    "    for d2, day2 in enumerate(sorted(days)):\n",
    "        if day2 not in tracked_rval_days[day1]:\n",
    "            tracked_rval_days[day1][day2] = []\n",
    "            shuffled_rval_days[day1][day2] = []\n",
    "        temp_numneurons = tracked_popevents_days[day1][day2].shape[0]\n",
    "        for tracked_neuron in range(temp_numneurons):\n",
    "            temp_tracked_rval = stats.pearsonr(tracked_popevents_days[day1][day2][tracked_neuron, 0:window_size], \\\n",
    "                                       tracked_popevents_days[day1][day2][tracked_neuron, window_size:])[0]\n",
    "            temp_shuffled_rval = stats.pearsonr(shuffled_popevents_days[day1][day2][tracked_neuron, 0:window_size], \\\n",
    "                           shuffled_popevents_days[day1][day2][tracked_neuron, window_size:])[0]\n",
    "            tracked_rval_days[day1][day2] = np.append(tracked_rval_days[day1][day2], temp_tracked_rval)\n",
    "            shuffled_rval_days[day1][day2] = np.append(shuffled_rval_days[day1][day2], temp_shuffled_rval)\n",
    "        \n",
    "#         # t-test comparing tracked and shuffled r-values for each day to day comparison\n",
    "#         if d1 != d2:\n",
    "#             tval, pval = stats.ttest_ind(tracked_rval_days[day1][day2], shuffled_rval_days[day1][day2])\n",
    "#             print('day 1 = %s, day 2 = %s, vs shuffled ttest = %s, pval = %s'%(day1, day2, tval, pval))\n",
    "\n",
    "\n",
    "tracked_rvals_all = []\n",
    "shuffled_rvals_all = []\n",
    "\n",
    "tracked_heatmap_array = np.nan*np.ones((numdays, numdays))\n",
    "shuffled_heatmap_array = np.nan*np.ones((numdays, numdays))\n",
    "for d1, day1 in enumerate(sorted(days)):\n",
    "    for d2, day2 in enumerate(sorted(days)):\n",
    "        tracked_heatmap_array[d1, d2] = np.nanmedian(tracked_rval_days[day1][day2])\n",
    "        shuffled_heatmap_array[d1, d2] = np.nanmedian(shuffled_rval_days[day1][day2])\n",
    "        if d1 != d2:\n",
    "            tracked_rvals_all = np.append(tracked_rvals_all, tracked_rval_days[day1][day2])\n",
    "            shuffled_rvals_all = np.append(shuffled_rvals_all, shuffled_rval_days[day1][day2])\n",
    "            "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#CREATE A HEATMAP USING ABOVE ARRAYS COMPARING PEARSON Rs FOR ALL DAY TO DAY COMPARISONS (TRACKED AND SHUFFLED)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 5))\n",
    "\n",
    "cmax = 0.2\n",
    "cmin = -0.2\n",
    "\n",
    "for subplot in range(3):\n",
    "    ax = axs[subplot]\n",
    "    if subplot == 0:\n",
    "        temp = tracked_heatmap_array\n",
    "        axs[0].set_title('Pearson Rs\\nTracked Heatmap')\n",
    "\n",
    "    elif subplot == 1:\n",
    "        temp = shuffled_heatmap_array\n",
    "        axs[1].set_title('Pearson Rs\\nShuffled Heatmap')\n",
    "        \n",
    "    else:\n",
    "        temp = tracked_heatmap_array - shuffled_heatmap_array\n",
    "        axs[2].set_title('Pearson Rs\\nTracked-Shuffled Heatmap')\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(temp, vmax=cmax, vmin=cmin, cmap='coolwarm')\n",
    "    fig.colorbar(im, ax=ax, shrink=0.2)\n",
    "    \n",
    "    # Set x-axis and y-axis tick labels\n",
    "    ax.set_xticks(np.arange(len(days)))\n",
    "    ax.set_yticks(np.arange(len(days)))\n",
    "    ax.set_xticklabels(sorted(days), rotation = 85)\n",
    "    ax.set_yticklabels(sorted(days))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Tracking_PearsonHeatMap_All.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Tracking_PearsonHeatMap_All.PNG'), format='PNG')\n",
    "            \n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CREATE R-VALUE HEATMAPS FOR EACH CLUSTER, WITH DAY-TO-DAY COMPARISONS\n",
    "tracked_cluster_rval_days = {}\n",
    "shuffled_cluster_rval_days = {}\n",
    "tracked_cluster_rvals_all = {}\n",
    "shuffled_cluster_rvals_all = {}\n",
    "\n",
    "for c in range(numclusters):\n",
    "    tracked_cluster_rval_days[c] = {}\n",
    "    shuffled_cluster_rval_days[c] = {}\n",
    "    tracked_cluster_rvals_all[c] = []\n",
    "    shuffled_cluster_rvals_all[c] = []\n",
    "    \n",
    "    for d1, day1 in enumerate(sorted(days)):\n",
    "        tracked_cluster_rval_days[c][day1] = {}\n",
    "        shuffled_cluster_rval_days[c][day1] = {}\n",
    "        \n",
    "        for d2, day2 in enumerate(sorted(days)):\n",
    "            tracked_cluster_rval_days[c][day1][day2] = tracked_rval_days[day1][day2][np.where(tracked_clusterids_days[day1][day2][:, 0] == c)]\n",
    "            shuffled_cluster_rval_days[c][day1][day2] = shuffled_rval_days[day1][day2][np.where(tracked_clusterids_days[day1][day2][:, 0] == c)]\n",
    "\n",
    "            if d1 != d2:\n",
    "                tracked_cluster_rvals_all[c] = np.append(tracked_cluster_rvals_all[c], tracked_cluster_rval_days[c][day1][day2])\n",
    "                shuffled_cluster_rvals_all[c] = np.append(shuffled_cluster_rvals_all[c], shuffled_cluster_rval_days[c][day1][day2])\n",
    "\n",
    "            \n",
    "tracked_cluster_heatmap_array = {}\n",
    "shuffled_cluster_heatmap_array = {}\n",
    "for c in range(numclusters):\n",
    "    tracked_cluster_heatmap_array[c] = np.nan*np.ones((numdays, numdays))\n",
    "    shuffled_cluster_heatmap_array[c] = np.nan*np.ones((numdays, numdays))\n",
    "    for d1, day1 in enumerate(sorted(days)):\n",
    "        for d2, day2 in enumerate(sorted(days)):\n",
    "            tracked_cluster_heatmap_array[c][d1, d2] = np.nanmedian(tracked_cluster_rval_days[c][day1][day2])\n",
    "            shuffled_cluster_heatmap_array[c][d1, d2] = np.nanmedian(shuffled_cluster_rval_days[c][day1][day2])\n",
    "\n",
    "# Create subplots\n",
    "rows = 2 ### 2 if you want to plot tracked and shuffled, 3 if you want to plot difference as well. 1 won't work unless you change ax = axs to 1 index for axs (c)\n",
    "fig, axs = plt.subplots(rows, 9, figsize=(12, rows*2.5))\n",
    "\n",
    "cmax = 0.8\n",
    "cmin = -0.8\n",
    "\n",
    "for row in range(rows):\n",
    "    for c in range(numclusters):\n",
    "        ax = axs[row, c]\n",
    "        if row == 0:\n",
    "            ax.set_title('Cluster %s'%c)\n",
    "            temp = tracked_cluster_heatmap_array[c]\n",
    "        elif row == 1:\n",
    "            temp = shuffled_cluster_heatmap_array[c]\n",
    "        elif row == 2:\n",
    "            temp = tracked_cluster_heatmap_array[c]-shuffled_cluster_heatmap_array[c]\n",
    "\n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(temp, vmax=cmax, vmin=cmin, cmap='coolwarm')\n",
    "\n",
    "        # Set x-axis and y-axis tick labels\n",
    "        ax.set_xticks(np.arange(len(days)))\n",
    "        ax.set_xticklabels(sorted(days), rotation = 85)\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_yticks(np.arange(len(days)))\n",
    "            ax.set_yticklabels(sorted(days))\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Tracking_PearsonHeatMap_Clusters.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Tracking_PearsonHeatMap_Clusters.PNG'), format='PNG')\n",
    "            \n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# CDF OF PEARSON R-VALUES FOR ALL DAY TO DAY COMPARISONS\n",
    "fig, axs = plt.subplots(1, numclusters + 1, figsize=(15, 2))\n",
    "\n",
    "bins = np.arange(-1,1,.001)\n",
    "\n",
    "ax = axs[0]\n",
    "ax.hist((tracked_rvals_all, shuffled_rvals_all),  density=True, cumulative=True,\\\n",
    "       label = ['tracked', 'shuffled'], histtype='step',\\\n",
    "        linestyle = ('-'), color = ['black', 'grey'], linewidth=1, bins = bins)\n",
    "ax.axvline(x=np.median(shuffled_rvals_all), color='r', linestyle='--', linewidth=1)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Pearson-R Values')\n",
    "ax.set_ylabel('CDF')\n",
    "ax.set_title('All Neurons')\n",
    "\n",
    "print('All Tracked Cells', stats.ttest_ind(tracked_rvals_all, shuffled_rvals_all))\n",
    "\n",
    "for c in range(numclusters):\n",
    "    ax = axs[c+1]\n",
    "    ax.hist((shuffled_rvals_all, tracked_cluster_rvals_all[c]),  density=True, cumulative=True, histtype='step',\\\n",
    "        linestyle = ('-'), color = ['grey', cluster_colors[c]], linewidth=1, bins = bins)\n",
    "    ax.axvline(x=np.median(shuffled_rvals_all), color='r', linestyle='-', linewidth=1)\n",
    "    ax.set_xlabel('Pearson-R Values')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Cluster %s'%c)\n",
    "\n",
    "    print('Cluster %s'%c, stats.ttest_ind(tracked_cluster_rvals_all[c], shuffled_rvals_all))\n",
    "\n",
    "\n",
    "# ax = axs[-1]\n",
    "# ax.hist(shuffled_rvals_all,  density=True, cumulative=True, histtype='step',\\\n",
    "#         linestyle = ('--'), color = ['grey'], linewidth=1, bins = bins)\n",
    "# for c in range(numclusters):\n",
    "#     plt.hist(tracked_cluster_rvals_all[c], density=True, cumulative=True,histtype='step',\\\n",
    "#         linestyle = ('-'), color = [cluster_colors[c]], linewidth=1, bins = bins)\n",
    "# ax.axvline(x=0, color='r', linestyle='--', linewidth=1)\n",
    "# ax.set_xlabel('Pearson-R Values')\n",
    "# ax.set_yticks([])\n",
    "# ax.set_title('Each Cluster')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Tracking_PearsonCDFs_Clusters.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Tracking_PearsonCDFs_Clusters.PNG'), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BETWEEN SESSION TRACKING: PLOT MEAN OR AUROC RESPONSES FOR DAY TO DAY COMPARISONS AND ANALYZE USING PEARSON"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Create array for all day to day comparisons that indicates average response on one day vs another day\n",
    "\n",
    "# Day to day correlations based on mean response or auROC value?\n",
    "measurement = 'auROC' #mean or auROC\n",
    "\n",
    "if measurement == 'mean':\n",
    "    session_stability_all = np.nan*np.ones((np.shape(tracked_popevents_all)[0], 2))\n",
    "    session_stability_all[:, 0] = np.mean(tracked_popevents_all[:, aucfirstframe:auclastframe], axis = 1)\n",
    "    session_stability_all[:, 1] = np.mean(tracked_popevents_all[:, window_size+aucfirstframe:window_size+auclastframe], axis = 1)\n",
    "    xlim=([-5,5])\n",
    "    ylim=([-5,5])\n",
    "else:\n",
    "    session_stability_all = np.nan*np.ones((np.shape(tracked_aucpvals_all)[0], 2))\n",
    "    session_stability_all[:, 0] = (tracked_aucpvals_all[:, 0]) ### column 0 of tracked_aucpvals_all is auROC for day 1, column 1 is associated pvalue\n",
    "    session_stability_all[:, 1] = (tracked_aucpvals_all[:, 2]) ### column 2 of tracked_aucpvals_all is auROC for day 2, column 3 is associated pvalue\n",
    "    xlim=([-1.1,1.1])\n",
    "    ylim=([-1.1,1.1])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ALL DAYS AND ALL CLUSTERS, CORRELATIONS\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "rval, rpval = stats.pearsonr(session_stability_all[:, 0], session_stability_all[:, 1])\n",
    "tval, tpval = stats.ttest_rel(session_stability_all[:, 0], session_stability_all[:, 1])\n",
    "\n",
    "# Create a DataFrame from within_stability_all\n",
    "df = pd.DataFrame(session_stability_all, columns=['Day 1', 'Day 2'])\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(5, 2.5))\n",
    "\n",
    "# Subplot 1: Correlation plot\n",
    "axs[0].scatter(x='Day 1', y='Day 2', data=df, \n",
    "                color='black', alpha=1, s=.1)\n",
    "sns.regplot(x='Day 1', y='Day 2', data=df, ax=axs[0],\n",
    "            line_kws={'color': 'red', 'linewidth': 1}, scatter=False)\n",
    "\n",
    "axs[0].set_xlabel('Day 1')  # Set the x-axis label\n",
    "axs[0].set_ylabel('Day 2')  # Set the y-axis label\n",
    "axs[0].set_title('Session Stability', fontsize=10)  # Set the title of the plot\n",
    "print(f'r = {rval:.5f}, p = {rpval: .5f}')  # Display correlation information\n",
    "print(f't = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "# axs[0].set_xlim(xlim[0], xlim[1])  # Set equal aspect ratio\n",
    "# axs[0].set_ylim(ylim[0], ylim[1])  # Set equal aspect ratio\n",
    "\n",
    "if measurement == 'mean':\n",
    "    axs[0].set_xscale('symlog')\n",
    "    axs[0].set_yscale('symlog')\n",
    "\n",
    "# Calculate means and SEM\n",
    "mean_early = np.nanmean(session_stability_all[:, 0])\n",
    "mean_late = np.nanmean(session_stability_all[:, 1])\n",
    "sem_early = stats.sem(session_stability_all[:, 0],  nan_policy='omit')\n",
    "sem_late = stats.sem(session_stability_all[:, 1], nan_policy='omit')\n",
    "\n",
    "# Subplot 2: Bar graph with error bars representing SEM\n",
    "sns.barplot(x=['Day 1', 'Day 2'], y=[mean_early, mean_late], ax=axs[1], color = 'gray')\n",
    "axs[1].errorbar(x=['Day 1', 'Day 2'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "\n",
    "axs[1].set_ylabel('%s'%(measurement))\n",
    "axs[1].set_ylim(0, .1)\n",
    "\n",
    "axs[1].set_title('Average Response by Group')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, All %s.PDF' % (measurement)),format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, All %s.png' % (measurement)), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### ALL CLUSTERS CORRELATION, BY MEAN RESPONSE\n",
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "fig, axs = plt.subplots(2, 2, figsize=(5, 5))\n",
    "sns.set_style('white')\n",
    "\n",
    "# Create a scatter plot with a regression line for clustered neurons\n",
    "for c in range(1, numclusters):\n",
    "    temp_clustered = session_stability_all[np.where(tracked_clusterids_all[:,0] == c)]\n",
    "    ax = axs[0,0]\n",
    "    ax.scatter(x=temp_clustered[:, 0], y=temp_clustered[:, 1], s = 1,\n",
    "        color = cluster_colors[c], alpha = .2)\n",
    "\n",
    "ax.set_ylabel('Day 2')  # Set the y-axis label\n",
    "\n",
    "ax.set_xlabel('Day 1')  # Set the x-axis label\n",
    "ax.set_title('clustered cells', fontsize=10)  # Set the title of the plot\n",
    "\n",
    "temp_clustered = session_stability_all[np.where(tracked_clusterids_all[:,0] > 0)]\n",
    "sns.regplot(x=temp_clustered[:, 0], y=temp_clustered[:, 1], ax = ax, scatter = False, line_kws={'color': 'red', 'linewidth': 1})\n",
    "\n",
    "# Create a scatter plot with a regression line for not clustered neurons\n",
    "ax = axs[0,1]\n",
    "temp_notclustered = session_stability_all[np.where(tracked_clusterids_all[:,0] == 0)]\n",
    "ax.scatter(x=temp_notclustered[:, 0], y=temp_notclustered[:, 1], s = 1, color = 'k', alpha = .2)\n",
    "sns.regplot(x=temp_notclustered[:, 0], y=temp_notclustered[:, 1], ax = ax, scatter = False, line_kws={'color': 'red', 'linewidth': 1})\n",
    "\n",
    "ax.set_ylabel('Day 2')  # Set the y-axis label\n",
    "ax.set_xlabel('Day 1')  # Set the x-axis label\n",
    "ax.set_title('unclustered cells', fontsize=10)  # Set the title of the plot\n",
    "\n",
    "# Calculate means and SEM\n",
    "mean_early = np.nanmean(session_stability_all[:, 0][np.where(tracked_clusterids_all[:,0] > 0)])\n",
    "mean_late = np.nanmean(session_stability_all[:, 1][np.where(tracked_clusterids_all[:,0] > 0)])\n",
    "sem_early = stats.sem(session_stability_all[:, 0][np.where(tracked_clusterids_all[:,0] > 0)],  nan_policy='omit')\n",
    "sem_late = stats.sem(session_stability_all[:, 1][np.where(tracked_clusterids_all[:,0] > 0)], nan_policy='omit')\n",
    "\n",
    "# Subplot 2: Bar graph with error bars representing SEM\n",
    "ax = axs[1, 0]\n",
    "sns.barplot(x=['Day1', 'Day2'], y=[mean_early, mean_late], ax=ax, color = 'purple')\n",
    "ax.errorbar(x=['Day1', 'Day2'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "\n",
    "# Calculate means and SEM\n",
    "mean_early = np.nanmean(session_stability_all[:, 0][np.where(tracked_clusterids_all[:,0] == 0)])\n",
    "mean_late = np.nanmean(session_stability_all[:, 1][np.where(tracked_clusterids_all[:,0] == 0)])\n",
    "sem_early = stats.sem(session_stability_all[:, 0][np.where(tracked_clusterids_all[:,0] == 0)],  nan_policy='omit')\n",
    "sem_late = stats.sem(session_stability_all[:, 1][np.where(tracked_clusterids_all[:,0] == 0)], nan_policy='omit')\n",
    "\n",
    "# Subplot 2: Bar graph with error bars representing SEM\n",
    "ax = axs[1, 1]\n",
    "sns.barplot(x=['Day1', 'Day2'], y=[mean_early, mean_late], ax=ax, color = 'gray')\n",
    "ax.errorbar(x=['Day1', 'Day2'], y=[mean_early, mean_late], yerr=[sem_early, sem_late], fmt='none', color='black', capsize=4)\n",
    "\n",
    "# Calculate Pearson correlations and T-tests\n",
    "rval, rpval = stats.pearsonr(temp_clustered[:, 0], temp_clustered[:, 1])\n",
    "tval, tpval = stats.ttest_rel(temp_clustered[:, 0],temp_clustered[:, 1])\n",
    "print(f'clustered r = {rval:.5f}, p = {rpval: .5f}')  # Display correlation information\n",
    "print(f'clustered t = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "rval, rpval = stats.pearsonr(temp_notclustered[:, 0],temp_notclustered[:, 1])\n",
    "tval, tpval = stats.ttest_rel(temp_notclustered[:, 0],temp_notclustered[:, 1])\n",
    "print(f'not clustered r = {rval:.5f}, p = {rpval: .5f}')  # Display correlation information\n",
    "print(f'not clustered t = {tval:.5f}, p = {tpval: .5f}')  # Display ttest information\n",
    "\n",
    "for column in range(2):\n",
    "    ax = axs[0, column]\n",
    "    ax.set_xlim(xlim[0], xlim[1])  \n",
    "    ax.set_ylim(ylim[0], ylim[1]) \n",
    "    if measurement == 'mean':\n",
    "        ax.set_xscale('symlog')\n",
    "        ax.set_yscale('symlog')\n",
    "    ax = axs[1, column]\n",
    "    ax.set_ylim(0, 0.25)  \n",
    "    \n",
    "plt.tight_layout()  \n",
    "\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, Clusters Mean Correlation.PDF'), format='PDF')\n",
    "# plt.savefig(os.path.join(basedir, 'Results', 'Session Stability, Clusters Mean Correlation.png'), format='PNG')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODING, START HERE IF YOU WANT A NEW DECODING ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def binaryclassifier(y, X):\n",
    "    hyperparameters = {'kernel': ['rbf'], 'gamma': [1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "                     'C': [1e-2, 1e-1, 1e0, 1e1, 1e2]}\n",
    "    clf = GridSearchCV(SVC(), hyperparameters, cv=10)\n",
    "    if np.any(np.isnan(X)):\n",
    "        accuracy=np.nan\n",
    "        print ('nan detected in single neuron! within ', fov)\n",
    "    if y.shape[0] < 20:\n",
    "        accuracy=np.nan\n",
    "    else:\n",
    "        clf.fit(X, y)\n",
    "        accuracy = clf.best_score_\n",
    "    return accuracy\n",
    "\n",
    "def svmregression(y, X):\n",
    "    hyperparameters = {'kernel': ['rbf'], 'C': np.logspace(-3, 3, 5),\n",
    "                      'epsilon': np.logspace(-3, 3, 5),\n",
    "                      'gamma': np.logspace(-5, 5, 10)}\n",
    "    clf = GridSearchCV(SVR(), hyperparameters, cv=10)\n",
    "    if np.all(np.isnan(X)):\n",
    "        R2=np.nan\n",
    "    else:\n",
    "        clf.fit(X, y)\n",
    "        \"\"\"y_pred = clf.predict(X)\n",
    "        plt.plot(y,y_pred,'ko')\"\"\"\n",
    "        R2 = clf.best_score_\n",
    "    #reference for 10-fold cross-validation http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf\n",
    "    return R2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODING - NO NEED TO RUN IF YOU'VE ALREADY SAVED DECODING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### SAVE NEW DECODING OR LOAD PREVIOUS? \n",
    "save_decoding_array = 'yes' ###'yes' or 'no'\n",
    "\n",
    "### SAVE/LOAD CUE OR INFUSION DECODING?\n",
    "decoding_stimulus = 'Lever' ### 'Cue' or 'Infusion'\n",
    "\n",
    "### INCREASE OR DECREASE NUMBER OF TRIALS TO EVEN TRIAL NUMBERS? 'NONE' AND 'UPSAMPLE' MAY GIVE INCORRECT RESULTS DUE TO TRIAL BIAS,\n",
    "trial_sampling = 'downsample' ###'upsample', 'downsample', or 'none'\n",
    "\n",
    "### HOW MANY TIMES DO YOU WANT TO REPEAT SHUFFLING ANALYSIS FOR EACH NEURON? THIS WILL TAKE TIME FOR EACH SHUFFLE BUT MAY PREVENT ARBITRARY DECODING IN SHUFFLED DATASETS\n",
    "numshuffles = 1\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ### THIS DECODING ALGORITHM HAS BEEN VALIDATED (CORRELATIONS WITH T-TESTS, AUROC, MEAN ADAPTATIONS. IT COMPARES THE FLUORESCENCE AVERAGE OF A BASELINE EPOCH TO CUE EPOCH###\n",
    "# ###DECODING. Note: this takes time (e.g., ~45 minutes for 3500 neurons).\n",
    "# days = ['0 EarlyAcq', '1 MidAcq', '2 LateAcq', '5 CueRein']\n",
    "\n",
    "# if decoding_stimulus == 'Cue':\n",
    "#     epoch_baseline = [pre_window_size-int(1*averagedframerate), pre_window_size] ### For Cue\n",
    "#     epoch_event = [pre_window_size-int(1*averagedframerate), pre_window_size+int(3*averagedframerate)] ### For Cue\n",
    "\n",
    "# elif decoding_stimulus == 'Infusion':\n",
    "#     epoch_baseline = [infusionframe-int(3*averagedframerate), infusionframe] ### For Infusion\n",
    "#     epoch_event = [infusionframe, infusionframe+int(3*averagedframerate)]### For Infusion\n",
    "\n",
    "# ### Save data for all decoded neurons and save to an array that is organized using cell_mapping function\n",
    "# if save_decoding_array == 'yes':\n",
    "#     decoding_all = np.nan*np.ones((numneurons_all))\n",
    "#     decoding_shuffled_all = np.nan*np.ones((numneurons_all))\n",
    "#     for basedir, day, animal, fov in iterate_dirs(basedir, days):\n",
    "#         tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "#         if 'alignedevents_%s_%s.npy'%('activelever', '1000') in tempfiles\\\n",
    "#                        and 'alignedevents_%s_%s.npy'%('activelevertimeout', '1000') in tempfiles: \n",
    "        \n",
    "#             temp1 = np.load(os.path.join(basedir, day, animal, fov, 'alignedevents_%s_%s.npy'%('activelever', '1000')))  \n",
    "#             temp2 = np.load(os.path.join(basedir, day, animal, fov, 'alignedevents_%s_%s.npy'%('activelevertimeout', '1000')))  \n",
    "            \n",
    "#             temp1_numtrials = np.shape(temp1)[2]\n",
    "#             temp2_numtrials = np.shape(temp2)[2]\n",
    "                        \n",
    "#             numneurons = np.shape(temp1)[0]\n",
    "\n",
    "#             temp_unshuffled = []\n",
    "#             temp_shuffled = []\n",
    "\n",
    "#             ####Downsample number of trials to ensure equal number of trial types between comparitors (e.g., activerewarded vs activetimeout presses)\n",
    "#             if trial_sampling == 'downsample':\n",
    "#                 if temp1_numtrials != temp2_numtrials: \n",
    "#                     mintrials = min(temp1_numtrials, temp2_numtrials)\n",
    "#                     temp1 = temp1[:, :, :mintrials]\n",
    "#                     temp2 = temp2[:, :, :mintrials]\n",
    "\n",
    "#             ###Upsample number of trials to ensure equal number of trial types between comparitors (e.g., activerewarded vs activetimeout presses)\n",
    "#             elif trial_sampling == 'upsample':\n",
    "#                 if temp1_numtrials != temp2_numtrials: \n",
    "#                     maxtrials = max(temp1_numtrials, temp2_numtrials)            \n",
    "#                     if temp1_numtrials < temp2_numtrials:\n",
    "\n",
    "#                         # Repeat trials in temp1\n",
    "#                         repeat_indices = np.random.choice(temp1_numtrials, maxtrials - temp1_numtrials, replace=True)\n",
    "#                         temp1 = np.concatenate((temp1, temp1[:, :, repeat_indices]), axis=2)\n",
    "\n",
    "#                     else:\n",
    "#                         # Repeat trials in temp2\n",
    "#                         repeat_indices = np.random.choice(temp2_numtrials, maxtrials - temp2_numtrials, replace=True)\n",
    "#                         temp2 = np.concatenate((temp2, temp2[:, :, repeat_indices]), axis=2)\n",
    "                \n",
    "#             reinforced_numtrials = np.shape(temp1)[2]\n",
    "#             timeout_numtrials = np.shape(temp2)[2]\n",
    "            \n",
    "#             if reinforced_numtrials > 9 and timeout_numtrials > 9:\n",
    "\n",
    "#                 reinforced_responses = temp1[:, epoch_event[0]:epoch_event[1], :]\n",
    "#                 timeout_responses = temp2[:, epoch_event[0]:epoch_event[1], :]\n",
    "\n",
    "#                 reinforced_baselines = np.nanmean(temp1[:, epoch_baseline[0]:epoch_baseline[1], :], axis = 1)\n",
    "#                 timeout_baselines = np.nanmean(temp2[:, epoch_baseline[0]:epoch_baseline[1], :], axis = 1)\n",
    "\n",
    "#                 reinforced_responses = reinforced_responses - reinforced_baselines[:,None,:]\n",
    "#                 timeout_responses = timeout_responses - timeout_baselines[:,None,:]\n",
    "\n",
    "#                 labels = np.hstack((np.zeros(reinforced_numtrials), np.ones(timeout_numtrials)))\n",
    "\n",
    "#                 ###Extract time series data for the current neuron from alignedevents1 and alignedevents2\n",
    "#                 for neuron in tqdm(range(numneurons), desc=f\"Processing {day} - {animal} - {fov}\"):\n",
    "\n",
    "#                     ###Stack time series data for the current neuron from both samples\n",
    "#                     features_neuron = np.hstack((reinforced_responses[neuron, :, :], timeout_responses[neuron, :, :])).T\n",
    "\n",
    "#                     ###Apply binary classifier function\n",
    "#                     decoding_neuron = binaryclassifier(labels, features_neuron)                    \n",
    "                    \n",
    "#                     ###Shuffle labels and perform shuffled binary classifier function as a control\n",
    "#                     temp_shuffled_neuron = np.nan*np.ones((numshuffles)) ### Shuffled analysis will be repeated based on \"numshuffles\" defined above\n",
    "                    \n",
    "#                     for i in range(numshuffles):\n",
    "#                         labels_shuffled = np.random.permutation(labels)\n",
    "#                         temp_shuffled_neuron[i] = binaryclassifier(labels_shuffled, features_neuron)\n",
    "                    \n",
    "#                     decoding_shuffled_neuron = np.mean(temp_shuffled_neuron)\n",
    "                    \n",
    "#                     temp_unshuffled = np.append(decoding_neuron, temp_unshuffled)\n",
    "#                     temp_shuffled = np.append(decoding_shuffled_neuron, temp_shuffled)\n",
    "\n",
    "#                     ###Use cell_mapping dictionary to infer the position of the neuron\n",
    "#                     position = cell_mapping[(day, animal, fov, neuron)]\n",
    "#                     decoding_all[position] = decoding_neuron\n",
    "#                     decoding_shuffled_all[position] = decoding_shuffled_neuron\n",
    "\n",
    "#                 print(day, animal, stats.ttest_rel(temp_unshuffled, temp_shuffled), reinforced_numtrials, timeout_numtrials)\n",
    "   \n",
    "#     if decoding_stimulus == 'Cue':                   \n",
    "#         np.save(os.path.join(basedir, 'Results', 'decoding_cue.npy'), decoding_all)  \n",
    "#         np.save(os.path.join(basedir, 'Results', 'decoding_cue_shuffled.npy'), decoding_shuffled_all) \n",
    "#     elif decoding_stimulus == 'Infusion':\n",
    "#         np.save(os.path.join(basedir, 'Results', 'decoding_infusion.npy'), decoding_all)  \n",
    "#         np.save(os.path.join(basedir, 'Results', 'decoding_infusion_shuffled.npy'), decoding_shuffled_all) \n",
    "    \n",
    "# ###Load Data and Create Arrays for Each Day\n",
    "# if decoding_stimulus == 'Cue':\n",
    "#     decoding_all = np.load(os.path.join(basedir, 'Results', 'decoding_cue.npy'))\n",
    "#     decoding_shuffled_all = np.load(os.path.join(basedir, 'Results', 'decoding_cue_shuffled.npy'))\n",
    "# elif decoding_stimulus == 'Infusion':\n",
    "#     decoding_all = np.load(os.path.join(basedir, 'Results', 'decoding_infusion.npy'))\n",
    "#     decoding_shuffled_all = np.load(os.path.join(basedir, 'Results', 'decoding_infusion_shuffled.npy'))\n",
    "# decoding_all -= np.nanmean(decoding_shuffled_all)\n",
    "# decoding_shuffled_all -= np.nanmean(decoding_shuffled_all)\n",
    "\n",
    "# decoding_day = {}\n",
    "# decoding_shuffled_day = {}\n",
    "# keys_day = {}\n",
    "\n",
    "# for d, day in enumerate(sorted(days)):\n",
    "#     decoding_day[day] = []\n",
    "#     decoding_shuffled_day[day] = []\n",
    "#     keys_day[day] = [key for key in cell_mapping.keys() if key[0] == day]\n",
    "#     for neuron_key in keys_day[day]:\n",
    "#         cell_number = cell_mapping[(neuron_key)]\n",
    "#         decoding_day[day] = np.append(decoding_day[day], decoding_all[cell_number])\n",
    "#         decoding_shuffled_day[day] = np.append(decoding_shuffled_day[day], decoding_shuffled_all[cell_number])\n",
    "#     print(day, stats.ttest_rel(decoding_day[day], decoding_shuffled_day[day], nan_policy = 'omit'))\n",
    "\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ### THIS DECODING ALGORITHM HAS BEEN VALIDATED (CORRELATIONS WITH T-TESTS, AUROC, MEAN ADAPTATIONS. IT COMPARES THE FLUORESCENCE AVERAGE OF A BASELINE EPOCH TO CUE EPOCH###\n",
    "# ###DECODING. Note: this takes time (e.g., ~45 minutes for 3500 neurons).\n",
    "\n",
    "# if decoding_stimulus == 'Lever':\n",
    "#     epoch_baseline = [0, int(1*averagedframerate)] ### For Lever baseline\n",
    "#     epoch_event = [pre_window_size-int(1*averagedframerate), pre_window_size] ### For Cue\n",
    "\n",
    "\n",
    "# ### Save data for all decoded neurons and save to an array that is organized using cell_mapping function\n",
    "# if save_decoding_array == 'yes':\n",
    "#     decoding_all = np.nan*np.ones((numneurons_all))\n",
    "#     decoding_shuffled_all = np.nan*np.ones((numneurons_all))\n",
    "#     for basedir, day, animal, fov in iterate_dirs(basedir, days):\n",
    "#         tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "#         if 'alignedevents_%s_%s.npy'%('activeleverall', separation_requirement) in tempfiles: \n",
    "        \n",
    "#             temp1 = np.load(os.path.join(basedir, day, animal, fov, 'alignedevents_%s_%s.npy'%('activeleverall', separation_requirement)))  \n",
    "            \n",
    "#             numtrials = np.shape(temp1)[2]           \n",
    "#             numneurons = np.shape(temp1)[0]\n",
    "            \n",
    "#             temp_unshuffled = []\n",
    "#             temp_shuffled = []\n",
    "            \n",
    "            \n",
    "#             if numtrials > 9:\n",
    "#                 responses = temp1[:, epoch_event[0]:epoch_event[1], :]\n",
    "#                 baselines = temp1[:, epoch_baseline[0]:epoch_baseline[1], :]\n",
    "\n",
    "#                 labels = np.hstack((np.zeros(numtrials), np.ones(numtrials)))\n",
    "\n",
    "#                 ###Extract time series data for the current neuron from alignedevents1 and alignedevents2\n",
    "#                 for neuron in tqdm(range(numneurons), desc=f\"Processing {day} - {animal} - {fov}\"):\n",
    "\n",
    "#                     ###Stack time series data for the current neuron from both samples\n",
    "#                     features_neuron = np.hstack((responses[neuron, :, :], baselines[neuron, :, :])).T\n",
    "\n",
    "#                     ###Apply binary classifier function\n",
    "#                     decoding_neuron = binaryclassifier(labels, features_neuron)                    \n",
    "                    \n",
    "#                     ###Shuffle labels and perform shuffled binary classifier function as a control\n",
    "#                     temp_shuffled_neuron = np.nan*np.ones((numshuffles)) ### Shuffled analysis will be repeated based on \"numshuffles\" defined above\n",
    "                    \n",
    "#                     for i in range(numshuffles):\n",
    "#                         labels_shuffled = np.random.permutation(labels)\n",
    "#                         temp_shuffled_neuron[i] = binaryclassifier(labels_shuffled, features_neuron)\n",
    "                    \n",
    "#                     decoding_shuffled_neuron = np.mean(temp_shuffled_neuron)\n",
    "                    \n",
    "#                     temp_unshuffled = np.append(decoding_neuron, temp_unshuffled)\n",
    "#                     temp_shuffled = np.append(decoding_shuffled_neuron, temp_shuffled)\n",
    "\n",
    "#                     ###Use cell_mapping dictionary to infer the position of the neuron\n",
    "#                     position = cell_mapping[(day, animal, fov, neuron)]\n",
    "#                     decoding_all[position] = decoding_neuron\n",
    "#                     decoding_shuffled_all[position] = decoding_shuffled_neuron\n",
    "\n",
    "#                 print(day, animal, stats.ttest_rel(temp_unshuffled, temp_shuffled))\n",
    "   \n",
    "#     if decoding_stimulus == 'Lever':                   \n",
    "#         np.save(os.path.join(basedir, 'Results', 'decoding_lever.npy'), decoding_all)  \n",
    "#         np.save(os.path.join(basedir, 'Results', 'decoding_lever_shuffled.npy'), decoding_shuffled_all) \n",
    "\n",
    "    \n",
    "# ###Load Data and Create Arrays for Each Day\n",
    "# if decoding_stimulus == 'Lever':\n",
    "#     decoding_all = np.load(os.path.join(basedir, 'Results', 'decoding_lever.npy'))\n",
    "#     decoding_shuffled_all = np.load(os.path.join(basedir, 'Results', 'decoding_lever_shuffled.npy'))\n",
    "\n",
    "# decoding_all -= np.nanmean(decoding_shuffled_all)\n",
    "# decoding_shuffled_all -= np.nanmean(decoding_shuffled_all)\n",
    "\n",
    "# decoding_day = {}\n",
    "# decoding_shuffled_day = {}\n",
    "# keys_day = {}\n",
    "\n",
    "# for d, day in enumerate(sorted(days)):\n",
    "#     decoding_day[day] = []\n",
    "#     decoding_shuffled_day[day] = []\n",
    "#     keys_day[day] = [key for key in cell_mapping.keys() if key[0] == day]\n",
    "#     for neuron_key in keys_day[day]:\n",
    "#         cell_number = cell_mapping[(neuron_key)]\n",
    "#         decoding_day[day] = np.append(decoding_day[day], decoding_all[cell_number])\n",
    "#         decoding_shuffled_day[day] = np.append(decoding_shuffled_day[day], decoding_shuffled_all[cell_number])\n",
    "#     print(day, stats.ttest_rel(decoding_day[day], decoding_shuffled_day[day], nan_policy = 'omit'))\n",
    "\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### THIS DECODING ALGORITHM HAS BEEN VALIDATED (CORRELATIONS WITH T-TESTS, AUROC, MEAN ADAPTATIONS. IT COMPARES THE FLUORESCENCE AVERAGE OF A BASELINE EPOCH TO CUE EPOCH###\n",
    "###DECODING. Note: this takes time (e.g., ~45 minutes for 3500 neurons).\n",
    "\n",
    "if decoding_stimulus == 'Lever':\n",
    "    epoch_baseline = [0, int(3*averagedframerate)] ### For Lever baseline\n",
    "    epoch_event = [pre_window_size-int(3*averagedframerate), pre_window_size] ### For Cue\n",
    "    numframes = epoch_baseline[1]-epoch_baseline[0]\n",
    "\n",
    "\n",
    "### Save data for all decoded neurons and save to an array that is organized using cell_mapping function\n",
    "if save_decoding_array == 'yes':\n",
    "    decoding_all = np.nan*np.ones((numneurons_all))\n",
    "    decoding_shuffled_all = np.nan*np.ones((numneurons_all))\n",
    "    for basedir, day, animal, fov in iterate_dirs(basedir, days):\n",
    "        tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "        if 'alignedevents_%s_%s.npy'%('activeleverall', separation_requirement) in tempfiles: \n",
    "        \n",
    "            temp1 = np.load(os.path.join(basedir, day, animal, fov, 'alignedevents_%s_%s.npy'%('activeleverall', separation_requirement)))  \n",
    "            \n",
    "            numtrials = np.shape(temp1)[2]           \n",
    "            numneurons = np.shape(temp1)[0]\n",
    "            \n",
    "            temp_unshuffled = []\n",
    "            temp_shuffled = []\n",
    "            \n",
    "            responses = temp1[:, epoch_event[0]:epoch_event[1], :]\n",
    "            baselines = temp1[:, epoch_baseline[0]:epoch_baseline[1], :]\n",
    "\n",
    "            labels = np.hstack((np.zeros(numframes), np.ones(numframes)))\n",
    "\n",
    "            ###Extract time series data for the current neuron from alignedevents1 and alignedevents2\n",
    "            for neuron in tqdm(range(numneurons), desc=f\"Processing {day} - {animal} - {fov}\"):\n",
    "\n",
    "                ###Stack time series data for the current neuron from both samples\n",
    "                features_neuron = np.vstack((responses[neuron, :, :], baselines[neuron, :, :]))\n",
    "                features_neuron = np.nanmean(features_neuron, axis = 1)\n",
    "                features_neuron = features_neuron.reshape(-1, 1)\n",
    "\n",
    "                ###Apply binary classifier function\n",
    "                decoding_neuron = binaryclassifier(labels, features_neuron)                    \n",
    "\n",
    "                ###Shuffle labels and perform shuffled binary classifier function as a control\n",
    "                temp_shuffled_neuron = np.nan*np.ones((numshuffles)) ### Shuffled analysis will be repeated based on \"numshuffles\" defined above\n",
    "\n",
    "                for i in range(numshuffles):\n",
    "                    labels_shuffled = np.random.permutation(labels)\n",
    "                    temp_shuffled_neuron[i] = binaryclassifier(labels_shuffled, features_neuron)\n",
    "\n",
    "                decoding_shuffled_neuron = np.mean(temp_shuffled_neuron)\n",
    "\n",
    "                temp_unshuffled = np.append(decoding_neuron, temp_unshuffled)\n",
    "                temp_shuffled = np.append(decoding_shuffled_neuron, temp_shuffled)\n",
    "\n",
    "                ###Use cell_mapping dictionary to infer the position of the neuron\n",
    "                position = cell_mapping[(day, animal, fov, neuron)]\n",
    "                decoding_all[position] = decoding_neuron\n",
    "                decoding_shuffled_all[position] = decoding_shuffled_neuron\n",
    "\n",
    "            print(day, animal, stats.ttest_rel(temp_unshuffled, temp_shuffled))\n",
    "   \n",
    "    if decoding_stimulus == 'Lever':                   \n",
    "        np.save(os.path.join(basedir, 'Results', 'decoding_lever.npy'), decoding_all)  \n",
    "        np.save(os.path.join(basedir, 'Results', 'decoding_lever_shuffled.npy'), decoding_shuffled_all) \n",
    "\n",
    "    \n",
    "###Load Data and Create Arrays for Each Day\n",
    "if decoding_stimulus == 'Lever':\n",
    "    decoding_all = np.load(os.path.join(basedir, 'Results', 'decoding_lever.npy'))\n",
    "    decoding_shuffled_all = np.load(os.path.join(basedir, 'Results', 'decoding_lever_shuffled.npy'))\n",
    "\n",
    "decoding_all -= np.nanmean(decoding_shuffled_all)\n",
    "decoding_shuffled_all -= np.nanmean(decoding_shuffled_all)\n",
    "\n",
    "decoding_day = {}\n",
    "decoding_shuffled_day = {}\n",
    "keys_day = {}\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    decoding_day[day] = []\n",
    "    decoding_shuffled_day[day] = []\n",
    "    keys_day[day] = [key for key in cell_mapping.keys() if key[0] == day]\n",
    "    for neuron_key in keys_day[day]:\n",
    "        cell_number = cell_mapping[(neuron_key)]\n",
    "        decoding_day[day] = np.append(decoding_day[day], decoding_all[cell_number])\n",
    "        decoding_shuffled_day[day] = np.append(decoding_shuffled_day[day], decoding_shuffled_all[cell_number])\n",
    "    print(day, stats.ttest_rel(decoding_day[day], decoding_shuffled_day[day], nan_policy = 'omit'))\n",
    "\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CDF PLOTS FOR ALL NEURONS ###\n",
    "### FIGURE SETTINGS ###\n",
    "fig, axs = plt.subplots(1, len(days), figsize=(2.5*len(days), 2), sharex=True, sharey=False)\n",
    "\n",
    "sns.set_style('white')\n",
    "colors = ['black', 'gray']\n",
    "bins = np.arange(-0.3,0.6,0.001) ###insert <min and >max of decoding values as left two nummbers, right number is binsize\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    ax=axs[d]\n",
    "    ax.hist(([decoding_day[day]-np.nanmean(decoding_shuffled_day[day]), decoding_shuffled_day[day]-np.nanmean(decoding_shuffled_day[day])]),  density=True, cumulative=True,\\\n",
    "       label = ['Actual','Shuffled'], histtype='step',\\\n",
    "        linestyle = ('-'), color = colors, linewidth=1, bins=bins)\n",
    "    ax.axvline(0, linestyle='--', color='r', linewidth=1.5)\n",
    "    ax.set_xlim(-0.3,0.6)\n",
    "    ax.set_ylim(-0.05,1.05)\n",
    "    \n",
    "    ax.set_title(day)\n",
    "    ax.set_xlabel('Decoding Value')\n",
    "    ax.set_ylabel('CDF')\n",
    "    if d != 0:\n",
    "        ax.set_yticks([])\n",
    "    if d == len(days)-1:\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# fig.savefig(os.path.join(basedir, 'Results', '%s Decoding All.PDF'%decoding_stimulus), format='PDF')\n",
    "# fig.savefig(os.path.join(basedir, 'Results', '%s Decoding All.png'%decoding_stimulus), format='PNG')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### BOX AND STRIP PLOTS FOR ALL NEURONS ###\n",
    "### FIGURE SETTINGS ###\n",
    "fig, ax = plt.subplots(figsize=(5, 4))  # Adjust the figure size as needed\n",
    "\n",
    "sns.set_style('white')\n",
    "colors = [cluster_colors[2], 'gray']\n",
    "\n",
    "# Prepare data for the box plot\n",
    "data = []\n",
    "labels = []\n",
    "group_labels = []\n",
    "\n",
    "for day in sorted(days):\n",
    "    actual_values = decoding_day[day] - np.nanmean(decoding_shuffled_day[day])\n",
    "    shuffled_values = decoding_shuffled_day[day] - np.nanmean(decoding_shuffled_day[day])\n",
    "    data.extend(actual_values)\n",
    "    labels.extend(['Actual'] * len(actual_values))\n",
    "    group_labels.extend([day] * len(actual_values))\n",
    "    data.extend(shuffled_values)\n",
    "    labels.extend(['Shuffled'] * len(shuffled_values))\n",
    "    group_labels.extend([day] * len(shuffled_values))\n",
    "\n",
    "# Create a DataFrame for the box plot\n",
    "df = pd.DataFrame({'Decoding Value': data, 'Type': labels, 'Day': group_labels})\n",
    "\n",
    "# Plot the box plot\n",
    "sns.boxplot(x='Day', y='Decoding Value', hue='Type', data=df, palette=colors, dodge=1, ax=ax, linewidth=1, fliersize=0, width=0.8, boxprops=dict(alpha=0.3), whiskerprops=dict(alpha=0), capprops=dict(alpha=0), medianprops=dict(alpha=1))\n",
    "\n",
    "# Plot the strip plot\n",
    "sns.stripplot(x='Day', y='Decoding Value', hue='Type', size=1, alpha=0.5, data=df, palette=colors, dodge=0.1, jitter=0.1, ax=ax)\n",
    "\n",
    "# Reduce legend duplication\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "unique_labels = list(set(labels))\n",
    "unique_handles = [handles[labels.index(label)] for label in unique_labels]\n",
    "ax.legend(unique_handles, unique_labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax.set_xlim(-0.5, len(days) - 0.5)\n",
    "ax.set_ylim(-0.25, 0.5)\n",
    "ax.set_title('Decoding Values Across Days')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Decoding Value')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(os.path.join(basedir, 'Results', '%s Decoding StripPlot.PDF'%decoding_stimulus), format='PDF')\n",
    "fig.savefig(os.path.join(basedir, 'Results', '%s Decoding StripPlot.png'%decoding_stimulus), format='PNG')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "fig, axs = plt.subplots(1, numclusters, figsize=(10, 2), sharex=True, sharey=False)\n",
    "sns.set_style('white')\n",
    "\n",
    "for c in range(numclusters):\n",
    "    decoding_cluster = []\n",
    "    decoding_shuffled_cluster = []\n",
    "    for d, day in enumerate(sorted(days)):   \n",
    "        decoding_temp = decoding_day[day][np.where(clusterids_day[day] == c)] \n",
    "        decoding_shuffled_temp = decoding_shuffled_day[day][np.where(clusterids_day[day] == c)] \n",
    "        \n",
    "        decoding_temp = decoding_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        decoding_shuffled_temp = decoding_shuffled_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        \n",
    "        decoding_cluster = np.hstack((decoding_cluster, decoding_temp))\n",
    "        decoding_shuffled_cluster = np.hstack((decoding_shuffled_cluster, decoding_shuffled_temp))\n",
    "        \n",
    "    ax = axs[c]\n",
    "    ax.hist(([decoding_cluster, decoding_shuffled_cluster]),  density=True, cumulative=True,\\\n",
    "        label = ['Actual','Shuffled'], histtype='step',\\\n",
    "        linestyle = ('-'), color = [cluster_colors[c], 'grey'], linewidth=1, bins=bins)\n",
    "    ax.axvline(np.nanmean(decoding_shuffled_day[day]), linestyle='--', color='r', linewidth=1)\n",
    "    ax.set_xlim(-0.3,0.6)\n",
    "    ax.set_ylim(-0.05,1.05)\n",
    "    ax.set_title(f'Cluster {c}' if c > 0 else 'Unclustered')\n",
    "    \n",
    "    if c == 0:\n",
    "        ax.set_ylabel('CDF')\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "fig.tight_layout()\n",
    "    \n",
    "\n",
    "# fig.savefig(os.path.join(basedir, 'Results', 'Decoding_CDF_Clusters.PDF'), format='PDF')\n",
    "# fig.savefig(os.path.join(basedir, 'Results', 'Decoding_CDF_Clusters.png'), format='PNG')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### BOX AND STRIP PLOTS FOR ALL NEURONS ###\n",
    "### FIGURE SETTINGS ###\n",
    "fig, ax = plt.subplots(figsize=(5, 4))  # Adjust the figure size as needed\n",
    "\n",
    "sns.set_style('white')\n",
    "colors = [cluster_colors[2], 'gray']\n",
    "\n",
    "# Prepare data for the box plot\n",
    "data = []\n",
    "labels = []\n",
    "group_labels = []\n",
    "\n",
    "for c in range(numclusters):\n",
    "    decoding_cluster = []\n",
    "    decoding_shuffled_cluster = []\n",
    "    for day in sorted(days):\n",
    "        decoding_temp = decoding_day[day][np.where(clusterids_day[day] == c)]\n",
    "        decoding_shuffled_temp = decoding_shuffled_day[day][np.where(clusterids_day[day] == c)]\n",
    "\n",
    "        decoding_temp = decoding_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        decoding_shuffled_temp = decoding_shuffled_temp - np.nanmean(decoding_shuffled_temp)\n",
    "\n",
    "        decoding_cluster.extend(decoding_temp)\n",
    "        decoding_shuffled_cluster.extend(decoding_shuffled_temp)\n",
    "\n",
    "    # Add data for the current cluster\n",
    "    data.extend(decoding_cluster)\n",
    "    labels.extend(['Actual'] * len(decoding_cluster))\n",
    "    group_labels.extend([f'Cluster {c}' if c > 0 else 'Unclustered'] * len(decoding_cluster))\n",
    "    data.extend(decoding_shuffled_cluster)\n",
    "    labels.extend(['Shuffled'] * len(decoding_shuffled_cluster))\n",
    "    group_labels.extend([f'Cluster {c}' if c > 0 else 'Unclustered'] * len(decoding_shuffled_cluster))\n",
    "\n",
    "# Create a DataFrame for the box plot\n",
    "df = pd.DataFrame({'Decoding Value': data, 'Type': labels, 'Cluster': group_labels})\n",
    "\n",
    "# Plot the box plot\n",
    "sns.boxplot(x='Cluster', y='Decoding Value', hue='Type', data=df, palette=colors, dodge=1, ax=ax, linewidth=1, fliersize=0, width=0.8, boxprops=dict(alpha=0.3), whiskerprops=dict(alpha=0), capprops=dict(alpha=0), medianprops=dict(alpha=1))\n",
    "\n",
    "# Plot the strip plot\n",
    "sns.stripplot(x='Cluster', y='Decoding Value', hue='Type', size=1, alpha=0.5, data=df, palette=colors, dodge=0.1, jitter=0.1, ax=ax)\n",
    "\n",
    "# Reduce legend duplication\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "unique_labels = list(set(labels))\n",
    "unique_handles = [handles[labels.index(label)] for label in unique_labels]\n",
    "ax.legend(unique_handles, unique_labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax.set_xlim(-0.5, numclusters - 0.5)\n",
    "ax.set_ylim(-0.25, 0.5)\n",
    "ax.set_title('Decoding Values Across Days')\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Decoding Value')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "# fig.savefig(os.path.join(basedir, 'Results', 'Decoding_Box_Strip_Plots_Clusters.PDF'), format='PDF')\n",
    "# fig.savefig(os.path.join(basedir, 'Results', 'Decoding_Box_Strip_Plots_Clusters.png'), format='PNG')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ### HEATMAP FOR THE DATA PLOTTED ABOVE ###\n",
    "# ### FIGURE SETTINGS ###\n",
    "\n",
    "data_for_heatmap = 'mean' ### 'mean' or 'zscore' or 'ttest'\n",
    "\n",
    "if data_for_heatmap == 'mean':\n",
    "    heatcmin = 0.0\n",
    "    heatcmax = 0.2\n",
    "\n",
    "elif data_for_heatmap == 'zscore':\n",
    "    heatcmin = 0.6\n",
    "    heatcmax = 1.2\n",
    "    \n",
    "elif data_for_heatmap == 'ttest':\n",
    "    heatcmin = -0.01\n",
    "    heatcmax = 6\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "heatmap_array = np.nan*np.ones((len(days)))\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    decoding_temp = decoding_day[day] - np.nanmean(decoding_shuffled_day[day])\n",
    "    decoding_shuffled_temp = decoding_shuffled_day[day] - np.nanmean(decoding_shuffled_day[day])\n",
    "\n",
    "    ttest = stats.ttest_rel(decoding_temp, decoding_shuffled_temp, nan_policy = 'omit')\n",
    "    print(day, ttest)\n",
    "    \n",
    "    if data_for_heatmap == 'mean':\n",
    "        heatmap_array[d] = np.nanmean(decoding_temp)\n",
    "\n",
    "    elif data_for_heatmap == 'zscore':\n",
    "        mean_shuffled_cluster = np.nanmean(decoding_shuffled_temp)\n",
    "        std_shuffled_cluster = np.nanstd(decoding_temp)\n",
    "        heatmap_array[d] = (np.nanmean(decoding_temp) - mean_shuffled_cluster) / std_shuffled_cluster\n",
    "\n",
    "    elif data_for_heatmap == 'ttest':\n",
    "        tvalue, pvalue = stats.ttest_ind(decoding_temp, decoding_shuffled_temp, nan_policy='omit')\n",
    "        heatmap_array[d] = tvalue\n",
    "        \n",
    "heatmap_array = heatmap_array.reshape(1,len(days))\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(3, 2))\n",
    "ax = sns.heatmap(heatmap_array, vmin=heatcmin, vmax=heatcmax, cmap='Purples')\n",
    "\n",
    "xtick_labels = [day for day in sorted(days)]\n",
    "ax.set_xticklabels(xtick_labels, rotation=90)\n",
    "\n",
    "ax.set_xlabel('Day')\n",
    "plt.title('Decoding Heatmap', fontsize=10)\n",
    "\n",
    "# Adjust margins to prevent axis labels from being cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_HeatMap_Days.PDF'), format='PDF')\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_HeatMap_Days.png'), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CDF PLOTS FOR SPLIT BY RESPONSE TYPE (e.g., excited vs inhibited) ###\n",
    "# Define response direction of each cluster for future separation\n",
    "clusters_unclustered = [0]\n",
    "clusters_excited = np.where(peak_responses > 0)[0]\n",
    "clusters_inhibited = np.where(peak_responses < 0)[0]\n",
    "\n",
    "response_types = 3\n",
    "\n",
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "fig, axs = plt.subplots(response_types, len(days), figsize=(2.5*len(days), 1.5*response_types), sharex=True, sharey=False)\n",
    "sns.set_style('white')\n",
    "\n",
    "response_type_colors = ['black', 'purple', 'green']\n",
    "\n",
    "for d, day in enumerate(sorted(days)):   \n",
    "    for response_type in range(response_types):\n",
    "        if response_type == 0:\n",
    "            decoding_temp = decoding_day[day][np.where(np.isin(clusterids_day[day], clusters_unclustered))] \n",
    "            decoding_shuffled_temp = decoding_shuffled_day[day][np.where(np.isin(clusterids_day[day], clusters_unclustered))]\n",
    "        elif response_type == 1:\n",
    "            decoding_temp = decoding_day[day][np.where(np.isin(clusterids_day[day], clusters_excited))] \n",
    "            decoding_shuffled_temp = decoding_shuffled_day[day][np.where(np.isin(clusterids_day[day], clusters_excited))]       \n",
    "        elif response_type == 2:\n",
    "            decoding_temp = decoding_day[day][np.where(np.isin(clusterids_day[day], clusters_inhibited))] \n",
    "            decoding_shuffled_temp = decoding_shuffled_day[day][np.where(np.isin(clusterids_day[day], clusters_inhibited))] \n",
    "        \n",
    "        decoding_temp = decoding_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        decoding_shuffled_temp = decoding_shuffled_temp - np.nanmean(decoding_shuffled_temp)\n",
    "\n",
    "        ax = axs[response_type, d]\n",
    "        ax.hist(([decoding_temp, decoding_shuffled_temp]),  density=True, cumulative=True,\\\n",
    "            label = ['Actual','Shuffled'], histtype='step',\\\n",
    "            linestyle = ('-'), color = [response_type_colors[response_type], 'grey'], linewidth=1, bins=bins)\n",
    "        ax.axvline(np.nanmean(decoding_shuffled_day[day]), linestyle='--', color='r', linewidth=1)\n",
    "        ax.set_xlim(-0.3,0.6)\n",
    "        ax.set_ylim(-0.05,1.05)\n",
    "        if response_type == 0:\n",
    "            ax.set_title(day)\n",
    "    \n",
    "        if response_type == 0 and d == 0:\n",
    "            ax.set_ylabel('unclustered')\n",
    "        elif response_type == 1 and d == 0:\n",
    "            ax.set_ylabel('excited')\n",
    "        elif response_type == 2 and d == 0:\n",
    "            ax.set_ylabel('inhibited')\n",
    "\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_CDF_ResponseTypes.PDF'), format='PDF')\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_CDF_ResponseTypes.png'), format='PNG')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ### HEATMAP FOR THE DATA PLOTTED ABOVE ###\n",
    "# ### FIGURE SETTINGS ###\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "if data_for_heatmap == 'mean':\n",
    "    heatcmin = 0\n",
    "    heatcmax = 0.3\n",
    "\n",
    "elif data_for_heatmap == 'zscore':\n",
    "    heatcmin = 0.6\n",
    "    heatcmax = 3\n",
    "    \n",
    "elif data_for_heatmap == 'ttest':\n",
    "    heatcmin = -0.01\n",
    "    heatcmax = 6\n",
    "    \n",
    "heatmap_array = np.nan*np.ones((response_types, len(days)))\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    for rt in range(response_types):\n",
    "        if rt == 0:\n",
    "            decoding_temp = decoding_day[day][np.where(np.isin(clusterids_day[day], clusters_unclustered))] \n",
    "            decoding_shuffled_temp = decoding_shuffled_day[day][np.where(np.isin(clusterids_day[day], clusters_unclustered))]\n",
    "        elif rt == 1:\n",
    "            decoding_temp = decoding_day[day][np.where(np.isin(clusterids_day[day], clusters_excited))] \n",
    "            decoding_shuffled_temp = decoding_shuffled_day[day][np.where(np.isin(clusterids_day[day], clusters_excited))]       \n",
    "        elif rt == 2:\n",
    "            decoding_temp = decoding_day[day][np.where(np.isin(clusterids_day[day], clusters_inhibited))] \n",
    "            decoding_shuffled_temp = decoding_shuffled_day[day][np.where(np.isin(clusterids_day[day], clusters_inhibited))] \n",
    "        \n",
    "        decoding_temp = decoding_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        decoding_shuffled_temp = decoding_shuffled_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        \n",
    "        ttest = stats.ttest_rel(decoding_temp, decoding_shuffled_temp, nan_policy = 'omit')\n",
    "        print(day, rt, ttest)\n",
    "\n",
    "        if data_for_heatmap == 'mean':\n",
    "            heatmap_array[rt, d] = np.nanmean(decoding_temp)\n",
    "\n",
    "        elif data_for_heatmap == 'zscore':\n",
    "            mean_shuffled_cluster = np.nanmean(decoding_shuffled_temp)\n",
    "            std_shuffled_cluster = np.nanstd(decoding_temp)\n",
    "            heatmap_array[rt, d] = (np.nanmean(decoding_temp) - mean_shuffled_cluster) / std_shuffled_cluster\n",
    "\n",
    "        elif data_for_heatmap == 'ttest':\n",
    "            tvalue, pvalue = stats.ttest_ind(decoding_temp, decoding_shuffled_temp, nan_policy='omit')\n",
    "            heatmap_array[rt, d] = tvalue\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(3, 2.5))\n",
    "ax = sns.heatmap(heatmap_array, vmin=heatcmin, vmax=heatcmax, cmap='Purples')\n",
    "xtick_labels = [day for day in sorted(days)]\n",
    "ax.set_xticklabels(xtick_labels, rotation=60)\n",
    "ytick_labels = ['Response Type {}'.format(rt) if rt > 0 else 'Unclustered' for rt in range(response_types)]\n",
    "ax.set_yticklabels(ytick_labels, rotation=0)\n",
    "\n",
    "ax.set_xlabel('Day')\n",
    "plt.title('Decoding Heatmap', fontsize=10)\n",
    "\n",
    "# Adjust margins to prevent axis labels from being cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_HeatMap_ResponseTypes.PDF'), format='PDF')\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_HeatMap_ResponseTypes.png'), format='PNG')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### CDF PLOTS FOR SPLIT BY CLUSTER ###\n",
    "fig, axs = plt.subplots(numclusters, len(days), figsize=(1.5*len(days), 1*numclusters), sharex=True, sharey=False)\n",
    "sns.set_style('white')\n",
    "\n",
    "for d, day in enumerate(sorted(days)):   \n",
    "    for c in range(numclusters):\n",
    "        decoding_temp = decoding_day[day][np.where(clusterids_day[day] == c)] \n",
    "        decoding_shuffled_temp = decoding_shuffled_day[day][np.where(clusterids_day[day] == c)] \n",
    "        \n",
    "        decoding_temp = decoding_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        decoding_shuffled_temp = decoding_shuffled_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        \n",
    "        ax = axs[c, d]\n",
    "        ax.hist(([decoding_temp, decoding_shuffled_temp]),  density=True, cumulative=True,\\\n",
    "            label = ['Actual','Shuffled'], histtype='step',\\\n",
    "            linestyle = ('-'), color = [cluster_colors[c], 'grey'], linewidth=1, bins=bins)\n",
    "        ax.axvline(np.nanmean(decoding_shuffled_day[day]), linestyle='--', color='r', linewidth=1)\n",
    "        ax.set_xlim(-0.3,0.6)\n",
    "        ax.set_ylim(-0.05,1.05)\n",
    "        if c == 0:\n",
    "            ax.set_title(day)\n",
    "        if d == 0:\n",
    "            ax.set_ylabel(f'Cluster {c}' if c > 0 else 'Unclustered')\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_CDF_Clusters.PDF'), format='PDF')\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_CDF_Clusters.png'), format='PNG')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ### BOX AND STRIP PLOTS FOR EACH CLUSTER ALL DAYS ###\n",
    "# ### FIGURE SETTINGS ###\n",
    "# fig, ax = plt.subplots(figsize=(5, 4))  # Adjust the figure size as needed\n",
    "\n",
    "# sns.set_style('white')\n",
    "# # colors = [cluster_colors[2], 'gray']\n",
    "\n",
    "# # Prepare data for the box plot\n",
    "# data = []\n",
    "# labels = []\n",
    "# group_labels = []\n",
    "\n",
    "# for cluster in range(numclusters):\n",
    "#     actual_values = []\n",
    "#     for day in sorted(days):\n",
    "#         actual_values_day = decoding_day[day] - np.nanmean(decoding_shuffled_day[day])\n",
    "#         shuffled_values_day = decoding_shuffled_day[day] - np.nanmean(decoding_shuffled_day[day])\n",
    "#         data.extend(actual_values)\n",
    "#         labels.extend(['Actual'] * len(actual_values))\n",
    "#         group_labels.extend([day] * len(actual_values))\n",
    "#         data.extend(shuffled_values)\n",
    "#         labels.extend(['Shuffled'] * len(shuffled_values))\n",
    "#         group_labels.extend([day] * len(shuffled_values))\n",
    "\n",
    "# # Create a DataFrame for the box plot\n",
    "# df = pd.DataFrame({'Decoding Value': data, 'Type': labels, 'Day': group_labels})\n",
    "\n",
    "# # Plot the box plot\n",
    "# sns.boxplot(x='Day', y='Decoding Value', hue='Type', data=df, palette=colors, dodge=1, ax=ax, linewidth=1, fliersize=0, width=0.8, boxprops=dict(alpha=0.3), whiskerprops=dict(alpha=0), capprops=dict(alpha=0), medianprops=dict(alpha=1))\n",
    "\n",
    "# # Plot the strip plot\n",
    "# sns.stripplot(x='Day', y='Decoding Value', hue='Type', size=1, alpha=0.5, data=df, palette=colors, dodge=0.1, jitter=0.1, ax=ax)\n",
    "\n",
    "# # Reduce legend duplication\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# unique_labels = list(set(labels))\n",
    "# unique_handles = [handles[labels.index(label)] for label in unique_labels]\n",
    "# ax.legend(unique_handles, unique_labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# ax.set_xlim(-0.5, len(days) - 0.5)\n",
    "# ax.set_ylim(-0.25, 0.5)\n",
    "# ax.set_title('Decoding Values Across Days')\n",
    "# ax.set_xlabel('Day')\n",
    "# ax.set_ylabel('Decoding Value')\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "# # Save the figure\n",
    "# fig.savefig(os.path.join(basedir, 'Results', '%s Decoding StripPlot.PDF'%decoding_stimulus), format='PDF')\n",
    "# fig.savefig(os.path.join(basedir, 'Results', '%s Decoding StripPlot.png'%decoding_stimulus), format='PNG')\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# ### HEATMAP SUMMARY, ANOVA, AND TUKEYS POSTHOCS FOR THE DATA PLOTTED ABOVE ###\n",
    "# ### FIGURE SETTINGS ###\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "heatmap_array = np.nan*np.ones((numclusters, len(days)))\n",
    "\n",
    "for d, day in enumerate(sorted(days)):\n",
    "    for c in range(numclusters):\n",
    "        decoding_temp = decoding_day[day][np.where(clusterids_day[day] == c)]\n",
    "        decoding_shuffled_temp = decoding_shuffled_day[day][np.where(clusterids_day[day] == c)]\n",
    "        \n",
    "        decoding_temp = decoding_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        decoding_shuffled_temp = decoding_shuffled_temp - np.nanmean(decoding_shuffled_temp)\n",
    "        \n",
    "        print(day, 'cluster = %s'%c, stats.ttest_rel(decoding_temp, decoding_shuffled_temp, nan_policy = 'omit'))\n",
    "\n",
    "        if data_for_heatmap == 'mean':\n",
    "            if np.count_nonzero(decoding_temp > -1) >= 2:\n",
    "                heatmap_array[c, d] = np.nanmean(decoding_temp)\n",
    "\n",
    "        elif data_for_heatmap == 'zscore':\n",
    "            mean_shuffled_cluster = np.nanmean(decoding_shuffled_temp)\n",
    "            std_shuffled_cluster = np.nanstd(decoding_temp)\n",
    "            heatmap_array[c, d] = (np.nanmean(decoding_temp) - mean_shuffled_cluster) / std_shuffled_cluster\n",
    "\n",
    "        elif data_for_heatmap == 'ttest':\n",
    "            tvalue, pvalue = stats.ttest_ind(decoding_temp, decoding_shuffled_temp, nan_policy='omit')\n",
    "            heatmap_array[c, d] = tvalue\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(3, 4))\n",
    "ax = sns.heatmap(heatmap_array, vmin=0, vmax=0.4, cmap='Purples')\n",
    "xtick_labels = [day for day in sorted(days)]\n",
    "ax.set_xticklabels(xtick_labels, rotation=60)\n",
    "ytick_labels = ['Cluster {}'.format(c) if c > 0 else 'Unclustered' for c in range(numclusters)]\n",
    "ax.set_yticklabels(ytick_labels, rotation=0)\n",
    "\n",
    "ax.set_xlabel('Day')\n",
    "plt.title('Decoding Heatmap', fontsize=10)\n",
    "\n",
    "# Adjust margins to prevent axis labels from being cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_HeatMap_Clusters.PDF'), format='PDF')\n",
    "fig.savefig(os.path.join(basedir, 'Results', 'Decoding_HeatMap_Clusters.png'), format='PNG')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPIKE INFERENCE (FIRST USE CASCADE CO-LAB CODE TO CREATE SPIKE PREDICTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### DEFINE THESE VARIABLES FOR CREATING SPIKE PREDICTION MEASUREMENTS\n",
    "\n",
    "save_cascade_measures = 'yes' #'yes' to save new cascade measures. 'no' to load saved data for each FOV\n",
    "\n",
    "dataset = 'baseline' #'baseline' or 'behavior' sessions?\n",
    "\n",
    "# Define the start and end times to analyze (in minutes)\n",
    "if dataset == 'baseline':\n",
    "    startmin = 0 #start time in minutes\n",
    "    endmin = 5 #end time in minutes\n",
    "elif dataset == 'behavior':\n",
    "    startmin = 0 #start time in minutes\n",
    "    endmin = 120 #end time in minutes\n",
    "\n",
    "window_predictions = [int(startmin*averagedframerate*60), int(endmin*averagedframerate*60)]\n",
    "window_frames = window_predictions[1]-window_predictions[0]\n",
    "\n",
    "### SELECTS ALL DAYS WITHIN A DIRECTORY. HAVE TO REDO THIS BECAUSE OF LIMITED DAY SAMPLING IN DECODING ABOVE###\n",
    "tempdays = next(os.walk(os.path.join(basedir)))[1]\n",
    "days = []\n",
    "for t in tempdays:\n",
    "    if t!= 'Cascade' and t!= 'CellTracking' and t!= 'Codes' and t != '.ipynb_checkpoints' and t != 'Other' and t!= 'Results':\n",
    "        days = np.append(days, t)\n",
    "numdays = len(days)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### IF SAVING CASCADE MEASURES, FIRST LOAD YOUR SPIKE PREDICTIONS DATA SO THAT YOU CAN QUANTIFY EACH MEASURE \n",
    "if save_cascade_measures == 'yes':\n",
    "    predictions_fov = {}\n",
    "    for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "        tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "        if day not in predictions_fov:\n",
    "            predictions_fov[day] = {}\n",
    "        if animal not in predictions_fov[day]:\n",
    "            predictions_fov[day][animal] = {}\n",
    "        if dataset == 'baseline':\n",
    "            try:\n",
    "                predictions_fov[day][animal][fov] = \\\n",
    "                                np.load(os.path.join(basedir, day, animal, fov, 'baseline_predictions.npy'))[:, window_predictions[0]:window_predictions[1]]\n",
    "            except FileNotFoundError:\n",
    "                print('No baseline data for: ', day, animal, fov)\n",
    "                continue\n",
    "        if dataset == 'behavior':\n",
    "            try:\n",
    "                predictions_fov[day][animal][fov] = \\\n",
    "                                np.load(os.path.join(basedir, day, animal, fov, 'behavior_predictions.npy'))[:, window_predictions[0]:window_predictions[1]]\n",
    "            except FileNotFoundError:\n",
    "                print('No behavior data for: ', day, animal, fov)\n",
    "                continue\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### IF SAVING CASCADE MEASURES, DEFINE YOUR FUNCTION FOR CALCULATING AND EXTRACTING EACH MEASURE FROM EACH DATASET (E.G., EACH FOV)\n",
    "def calculate_cascade_measures(data):\n",
    "    numneurons = data.shape[0]\n",
    "    cascade_measures = np.nan*np.ones((numneurons, 9))\n",
    "    for neuron in range(numneurons):\n",
    "        predictions_neuron = data[neuron, :] \n",
    "        \n",
    "        if np.all(np.isnan(predictions_neuron)) == False:\n",
    "            \n",
    "            # Calculate basal firing properties for each neuron (basal firing rate, mean firing rate, maximum spike rate)\n",
    "            basalfiring_neuron = np.nanquantile(predictions_neuron, 0.25)\n",
    "            meanfiring_neuron = np.nanmean(predictions_neuron)\n",
    "            maxfiring_neuron = np.nanmax(predictions_neuron)\n",
    "            \n",
    "            # Find spike bursts for each neuron\n",
    "            predictions_subtracted = predictions_neuron-basalfiring_neuron\n",
    "            burst_frames, burst_dict = find_peaks(predictions_subtracted,\\\n",
    "                    distance = averagedframerate, width = (3, 3*averagedframerate), height = (1.5))  # Find frame number for bursts\n",
    "            \n",
    "            # Calculate burst firing properties for each neuron (basal firing rate, mean firing rate, maximum spike rate)\n",
    "            burstfreq_neuron = np.nanmean(len(burst_frames)/np.count_nonzero(predictions_neuron >= 0)*averagedframerate) # Get the frequency of bursts\n",
    "            burstisi_neuron = np.nanmean(np.diff(burst_frames)) # Get the average inter-burst-intervals between bursts\n",
    "            burstamp_neuron = np.nanmean(predictions_neuron[burst_frames])  # Get the average peak amplitude of bursts\n",
    "            burstampchange_neuron = np.nanmean(burst_dict['width_heights'])\n",
    "            burstwidth_neuron = np.nanmean(burst_dict['widths']/averagedframerate)\n",
    "            signalnoise_neuron = ((burstampchange_neuron*burstfreq_neuron)/(basalfiring_neuron))\n",
    "            \n",
    "            # Insert each calculation into \"cascade_measures\" array\n",
    "            cascade_measures_neuron = [basalfiring_neuron, meanfiring_neuron, maxfiring_neuron, \\\n",
    "                                       burstfreq_neuron, burstisi_neuron, burstamp_neuron, burstampchange_neuron, \\\n",
    "                                       burstwidth_neuron, signalnoise_neuron]\n",
    "            cascade_measures[neuron, :] = cascade_measures_neuron\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return cascade_measures\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### SAVE OR LOAD CASCADE MEASURES FOR ALL CELLS. CREATE DICTIONARY ARRAYS FOR EACH FOV AND EACH DAY.\n",
    "cascade_measures_fov = {}\n",
    "cascade_measures_day = {}\n",
    "\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "    tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "    \n",
    "    if day not in cascade_measures_fov:\n",
    "        cascade_measures_day[day] = np.nan*np.ones((1, 9))\n",
    "        cascade_measures_fov[day] = {}\n",
    "\n",
    "    if animal not in cascade_measures_fov[day]:\n",
    "        cascade_measures_fov[day][animal] = {}\n",
    "        cascade_measures_fov[day][animal] = {}\n",
    "        \n",
    "    if fov not in cascade_measures_fov[day][animal]:\n",
    "        \n",
    "        ### SAVE CASCADE MEASURES FOR EACH FOV\n",
    "        if save_cascade_measures == 'yes':\n",
    "            try:\n",
    "                cascade_measures_fov[day][animal][fov] = calculate_cascade_measures(predictions_fov[day][animal][fov])\n",
    "                np.save(os.path.join(basedir, day, animal, fov, 'cascade_measures_%s.npy'%dataset), cascade_measures_fov[day][animal][fov])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        ### LOAD CASCADE MEASURES FOR EACH FOV\n",
    "        try:\n",
    "            cascade_measures_fov[day][animal][fov] = np.load(os.path.join(basedir, day, animal, fov, 'cascade_measures_%s.npy'%dataset))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "                \n",
    "        # Stack Cascade Measures From Each FOV For Each Day\n",
    "        cascade_measures_day[day] = np.vstack((cascade_measures_day[day], cascade_measures_fov[day][animal][fov]))\n",
    "            \n",
    "\n",
    "for day in sorted(days):\n",
    "    cascade_measures_day[day] = cascade_measures_day[day][1:, :]\n",
    "               \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### NOW THAT WE'VE SAVED/LOADED DATA, WE ARE READY TO PLOT\n",
    "plot_by_subtraction = 'no' #Plot data as a change from day 1 ('yes')?\n",
    "daysforbaselines = sorted(days)[:]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CASCADE MEASURES AS BAR GRAPHS FOR EACH DAY ###\n",
    "measurements = ['Basal Firing (Hz)', 'Mean Firing (Hz)', 'Max Firing (Hz)', 'Burst Freq (Hz)', \\\n",
    "                'Burst ISI (s)', 'Burst Amp (Hz)', 'Burst Amp (Hz)', 'Burst Width (s)', 'SNR']\n",
    "\n",
    "fig, axs = plt.subplots(1, len(measurements), figsize=(11, 3))\n",
    "\n",
    "for m, measurement in enumerate(measurements):\n",
    "    # Calculate means and SEMs for each day\n",
    "    means = []\n",
    "    sems = []\n",
    "    all_values = []\n",
    "    \n",
    "    for day in sorted(daysforbaselines):\n",
    "        if plot_by_subtraction == 'yes':\n",
    "            values = cascade_measures_day[day][:, m]-np.nanmean(cascade_measures_day['0 EarlyAcq'][:, m])\n",
    "        else:\n",
    "            values = cascade_measures_day[day][:, m]\n",
    "        means.append(np.nanmean(values))\n",
    "        sems.append(np.nanstd(values) / np.sqrt(np.count_nonzero(values > -1000)))\n",
    "        all_values = np.append(all_values, values)\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({'Day': sorted(daysforbaselines), 'Mean': means, 'SEM': sems})\n",
    "\n",
    "    # Plot bar plot with error bars\n",
    "    ax = axs[m]\n",
    "    sns.barplot(x='Day', y='Mean', yerr=plot_df['SEM'], data=plot_df, ax=ax, capsize=0.2)\n",
    "    ax.set_title(measurement)\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "   \n",
    "    # Calculate min and max for binning data\n",
    "    ax.set_ylim (np.nanquantile(all_values, 0.3), np.nanquantile(all_values, 0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASCADE MEASURES BY CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### TO PLOT CASCADE MEASURES BY EACH CLUSTER, WE MUST HAVE DATA LOADED ONLY FOR CELLS THAT WERE CLUSTERED. HERE WE RELOAD THAT DATA\n",
    "cascade_measures_fov = {}\n",
    "cascade_measures_day = {}\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "    tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "    \n",
    "    if day not in cascade_measures_fov:\n",
    "        cascade_measures_day[day] = np.nan*np.ones((1, 9))\n",
    "        cascade_measures_fov[day] = {}\n",
    "        cascade_measures_fov[day] = {}\n",
    "\n",
    "    if animal not in cascade_measures_fov[day]:\n",
    "        cascade_measures_fov[day][animal] = {}\n",
    "        cascade_measures_fov[day][animal] = {}\n",
    "        \n",
    "    if fov not in cascade_measures_fov[day][animal]:\n",
    "        \n",
    "        ### LOAD CASCADE MEASURES FOR EACH FOV, BUT ONLY IF CLUSTERING DATA IS AVAILABLE\n",
    "        try: \n",
    "            tempnumneurons = np.shape(np.load(os.path.join(basedir, day, animal, fov, 'aucpvals_%s_%s.npy'%(eventofinterest, separation_requirement))))[0]\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            tempnumneurons = np.shape(np.load(os.path.join(basedir, day, animal, fov, 'aucpvals_%s_%s.npy'%(eventofinterest, separation_requirement))))[0]\n",
    "            cascade_measures_fov[day][animal][fov] = np.load(os.path.join(basedir, day, animal, fov, 'cascade_measures_%s.npy'%dataset))\n",
    "            \n",
    "            numneuronstest = np.load(os.path.join(basedir, day, animal, fov, 'aucpvals_%s_%s.npy'%(eventofinterest, separation_requirement)))\n",
    "            if numneuronstest.shape[0] != cascade_measures_fov[day][animal][fov].shape[0]:\n",
    "                print('Issue! NumNeurons in Loaded Dataset != NumNeurons in Folder', day, animal, fov, x.shape, cascade_measures_fov[day][animal][fov].shape)\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            cascade_measures_fov[day][animal][fov] = np.nan*np.ones((tempnumneurons, 9))\n",
    "            \n",
    "                \n",
    "        # Stack Cascade Measures From Each FOV For Each Day\n",
    "        cascade_measures_day[day] = np.vstack((cascade_measures_day[day], cascade_measures_fov[day][animal][fov]))\n",
    "\n",
    "for day in sorted(days):\n",
    "    cascade_measures_day[day] = cascade_measures_day[day][1:, :]\n",
    "               \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CASCADE MEASURES AS BAR GRAPHS BY EACH CELLS THAT WERE IN RESPONDING CLUSTERS VS NON RESPONDING ###\n",
    "measurements = ['Basal Firing (Hz)', 'Mean Firing (Hz)', 'Max Firing (Hz)', 'Burst Freq (Hz)', \\\n",
    "                'Burst ISI (s)', 'Burst Amp (Hz)', 'Burst Amp (Hz)', 'Burst Width (s)', 'SNR']\n",
    "\n",
    "fig, axs = plt.subplots(2, len(measurements), figsize=(11, 6), sharey = 'col')\n",
    "\n",
    "for c in range(2):\n",
    "    for m, measurement in enumerate(measurements):\n",
    "        # Calculate means and SEMs for each day\n",
    "        means = []\n",
    "        sems = []\n",
    "        all_values = []\n",
    "        \n",
    "        for day in sorted(daysforbaselines):\n",
    "            if plot_by_subtraction == 'yes':\n",
    "                if c == 0:\n",
    "                    values = cascade_measures_day[day][:, m][np.where(clusterids_day[day] == 0)]\\\n",
    "                                        -np.nanmean(cascade_measures_day['0 EarlyAcq'][:, m][np.where(clusterids_day['0 EarlyAcq'] == 0)])\n",
    "                else:\n",
    "                    values = cascade_measures_day[day][:, m][np.where(clusterids_day[day] > 0)]\\\n",
    "                                        -np.nanmean(cascade_measures_day['0 EarlyAcq'][:, m][np.where(clusterids_day['0 EarlyAcq'] > 0)])            \n",
    "            else:\n",
    "                if c == 0:\n",
    "                    values = cascade_measures_day[day][:, m][np.where(clusterids_day[day] == 0)]\n",
    "                else:\n",
    "                    values = cascade_measures_day[day][:, m][np.where(clusterids_day[day] > 0)]\n",
    "\n",
    "            means.append(np.nanmean(values))\n",
    "            sems.append(np.nanstd(values) / np.sqrt(np.count_nonzero(values > -1000)))\n",
    "            all_values = np.append(all_values, values)\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        plot_df = pd.DataFrame({'Day': sorted(daysforbaselines), 'Mean': means, 'SEM': sems})\n",
    "\n",
    "        # Plot bar plot with error bars\n",
    "        ax = axs[c, m]\n",
    "        sns.barplot(x='Day', y='Mean', yerr=plot_df['SEM'], data=plot_df, ax=ax, capsize=0.2)\n",
    "        ax.set_title(measurement)\n",
    "        ax.set_xlabel('Day')\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "        # Calculate min and max for binning data\n",
    "        ax.set_ylim (np.nanquantile(all_values, 0.25), np.nanquantile(all_values, 0.75))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CASCADE MEASURES AS BAR GRAPHS BY EACH CELLS THAT WERE IN RESPONDING CLUSTERS VS NON RESPONDING ###\n",
    "measurements = ['Basal Firing (Hz)', 'Mean Firing (Hz)', 'Max Firing (Hz)', 'Burst Freq (Hz)', \\\n",
    "                'Burst ISI (s)', 'Burst Amp (Hz)', 'Burst Amp (Hz)', 'Burst Width (s)', 'SNR']\n",
    "\n",
    "fig, axs = plt.subplots(numclusters, len(measurements), figsize=(11, 3*numclusters), sharey = 'col')\n",
    "\n",
    "heatmap_array = np.nan*np.ones((numclusters, len(daysforbaselines), len(measurements)))\n",
    "\n",
    "for c in range(numclusters):\n",
    "    for m, measurement in enumerate(measurements):\n",
    "        # Calculate means and SEMs for each day\n",
    "        means = []\n",
    "        sems = []\n",
    "        all_values = []\n",
    "        control_values = cascade_measures_day['0 EarlyAcq'][:, m][np.where(clusterids_day['0 EarlyAcq'] == c)]\n",
    "\n",
    "        for d, day in enumerate(sorted(daysforbaselines)):\n",
    "            if plot_by_subtraction == 'yes':\n",
    "                values = cascade_measures_day[day][:, m][np.where(clusterids_day[day] == c)]\\\n",
    "                                    -np.nanmean(control_values)\n",
    "            else:\n",
    "                values = cascade_measures_day[day][:, m][np.where(clusterids_day[day] == c)]\n",
    "                \n",
    "            means.append(np.nanmean(values))\n",
    "            sems.append(np.nanstd(values) / np.sqrt(np.count_nonzero(values > -1000)))\n",
    "            \n",
    "#             heatmap_array[c, d, m] = stats.ttest_ind(values, control_values, nan_policy = 'omit')[0]\n",
    "#             heatmap_array[c, d, m] = calculate_auROC(values[np.where(values > -1000)], control_values[np.where(control_values > -1000)])[0]\n",
    "            heatmap_array[c, d, m] = (np.nanmean(values) - np.nanmean(control_values)) / (stats.sem(values, nan_policy = 'omit'))\n",
    "\n",
    "            if m == 4:\n",
    "                heatmap_array[c, d, m] = -heatmap_array[c, d, m]\n",
    "            all_values = np.append(all_values, values)\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        plot_df = pd.DataFrame({'Day': sorted(daysforbaselines), 'Mean': means, 'SEM': sems})\n",
    "\n",
    "        # Plot bar plot with error bars\n",
    "        ax = axs[c, m]\n",
    "        sns.barplot(x='Day', y='Mean', yerr=plot_df['SEM'], data=plot_df, ax=ax, capsize=0.2, color = cluster_colors[c])\n",
    "        ax.set_title(measurement)\n",
    "        ax.set_xlabel('Day')\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "        # Calculate min and max for binning data\n",
    "        ax.set_ylim (np.nanquantile(all_values, 0.1), np.nanquantile(all_values, 0.95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### PLOT HEATMAPS REPRESENTING AVERAGE CHANGES FOR EACH CLUSTER, ON EACH DAY\n",
    "fig, axs = plt.subplots(1,len(daysforbaselines)-1, figsize=(10, 2))\n",
    "\n",
    "for d, day in enumerate(sorted(daysforbaselines[1:])):\n",
    "    ax = axs[d]\n",
    "    sns.heatmap(heatmap_array[:, d+1, :], cmap = 'coolwarm', vmax = 5, vmin = -5, ax = axs[d])\n",
    "    ax.set_title(day)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('Cluster')\n",
    "    else:\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_xlabel(None)\n",
    "    xtick_labels = measurements[:6]\n",
    "#     ax.set_xticklabels(xtick_labels, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(2, 2))\n",
    "\n",
    "sns.heatmap(np.nanmean(heatmap_array[:, 1:, :], axis = 1), cmap = 'coolwarm', vmax = 5, vmin = -5)\n",
    "axs.set_ylabel('Cluster')\n",
    "axs.set_xlabel('Measure of Spiking')\n",
    "axs.set_xlabel(None)\n",
    "xtick_labels = measurements[:]\n",
    "# axs.set_xticklabels(xtick_labels, rotation=90)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETWEEN SESSION SPIKE PREDICTION STABILITY"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cells_tracked_alldays = 'no' ### haven't updated this yet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LOAD TRACKING IDS FOR EACH DAY AND ANIMAL\n",
    "tracking_ids = {}\n",
    "tempfiles = next(os.walk(os.path.join(basedir, 'CellTracking', 'AllDays')))[2]\n",
    "for csvfile in sorted(tempfiles):\n",
    "    if os.path.splitext(csvfile)[1] == '.csv':\n",
    "        animal = csvfile.split(\"_\")[0]\n",
    "        with open(os.path.join(basedir, 'CellTracking', 'AllDays', csvfile)) as temp_packaged:\n",
    "            temp = np.loadtxt(temp_packaged, delimiter=\",\")\n",
    "            tracking_ids[animal] = (temp[:, :numdays] - 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CREATE DICTIONARIES FOR EACH VARIABLE OF INTEREST WITH DATA ALIGNED BY TRACKING ID\n",
    "\n",
    "# Create empty dictionaries for each variable\n",
    "tracked_cmeasures_fov = {}\n",
    "tracked_animals = []\n",
    "\n",
    "# Loop through each folder to load/create data for each fov\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):     \n",
    "    # Check to ensure popevents has been created for each fov, otherwise skip that fov\n",
    "    try: \n",
    "        popevents_fov[day][animal][fov]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "    # Create nested dictionaries for each variable (cascade meassures baseline) for each fov, and fill with empty array (np.nans)\n",
    "    # Skips fovs that do not have tracking ids loaded\n",
    "    if animal not in tracked_cmeasures_fov:\n",
    "        tracked_animals = np.append(tracked_animals, animal)\n",
    "        temp_numneurons = np.shape(tracking_ids[animal])[0]\n",
    "        tracked_cmeasures_fov[animal] = np.nan*np.ones((numdays, temp_numneurons, 9))\n",
    "\n",
    "for animal in tracked_popevents_fov.keys():\n",
    "    temp_numneurons = len(tracking_ids[animal])\n",
    "    for d, day in enumerate(sorted(days)):\n",
    "        try:\n",
    "            tempkeys = popevents_fov[day][animal].keys()\n",
    "            fovkey = list(tempkeys)[0]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        for tracked_neuron in range(temp_numneurons):\n",
    "            if cells_tracked_alldays == 'yes':\n",
    "                ### HAVEN\"T UPDATED THIS YET\n",
    "                temp_cellid = tracking_ids[animal][tracked_neuron, d].astype(int)\n",
    "#                 temp_cellid_alldays = tracking_ids[animal][tracked_neuron, :].astype(int)\n",
    "#                 if temp_cellid < max_numneurons:\n",
    "#                     x = cascade_measures_fov[day][animal][fovkey][temp_cellid,:]\n",
    "        \n",
    "            else:\n",
    "                temp_cellid = tracking_ids[animal][tracked_neuron, d].astype(int)\n",
    "                if temp_cellid < max_numneurons:\n",
    "                    tracked_cmeasures_fov[animal][d, tracked_neuron, :] = cascade_measures_fov[day][animal][fovkey][temp_cellid,:]\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CREATE POPEVENT, AUCPVAL, AND CLUSTER ID ARRAYS WHERE RESPONSES ARE TRACKED ACROSS ANY TWO SESSIONS (day1, day2)\n",
    "\n",
    "trackedcellcounter_all = 0\n",
    "\n",
    "###Arrays for any day-to-day comparisons\n",
    "tracked_cmeasures_all = np.nan*np.ones((max_trackedneurons_all, 18))\n",
    "\n",
    "###Arrays for specific day-to-day comparisons\n",
    "tracked_cmeasures_days = {}\n",
    "\n",
    "###Loop through day-to-day comparisons and animals and fill the empty arrays above\n",
    "for d1, day1 in enumerate(sorted(days)):\n",
    "    if day1 not in tracked_cmeasures_days:\n",
    "        tracked_cmeasures_days[day1] = {}\n",
    "\n",
    "    for d2, day2 in enumerate(sorted(days)):\n",
    "        tracked_cmeasures_days[day1][day2] = np.nan*np.ones((max_trackedneurons_day, 18))\n",
    "        \n",
    "        trackedcellcounter_day = 0\n",
    "        for tracked_animal in sorted(tracked_animals):\n",
    "            temp_numneurons = np.shape(tracking_ids[tracked_animal])[0]\n",
    "\n",
    "            for tracked_neuron in range(temp_numneurons):\n",
    "                response1 = tracked_popevents_fov[tracked_animal][d1, tracked_neuron, :]\n",
    "                response2 = tracked_popevents_fov[tracked_animal][d2, tracked_neuron, :]\n",
    "                neuron_responses = np.hstack((response1, response2))\n",
    "                \n",
    "                cmeasures1 = tracked_cmeasures_fov[tracked_animal][d1, tracked_neuron, :]\n",
    "                cmeasures2 = tracked_cmeasures_fov[tracked_animal][d2, tracked_neuron, :]\n",
    "                neuron_cmeasures = np.hstack((cmeasures1, cmeasures2))\n",
    "\n",
    "                if not np.any(np.isnan(neuron_responses)):\n",
    "                    tracked_cmeasures_days[day1][day2][trackedcellcounter_day, :] = neuron_cmeasures\n",
    "                    trackedcellcounter_day += 1\n",
    "\n",
    "                    if d1 != d2:\n",
    "                        tracked_cmeasures_all[trackedcellcounter_all, :] = neuron_cmeasures\n",
    "                        trackedcellcounter_all += 1\n",
    "                        \n",
    "            if tracked_animal == sorted(tracked_animals)[-1]:\n",
    "                tracked_cmeasures_days[day1][day2] = tracked_cmeasures_days[day1][day2][:trackedcellcounter_day, :]\n",
    "                \n",
    "tracked_cmeasures_all = tracked_cmeasures_all[:trackedcellcounter_all, :]\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BETWEEN SESSION TRACKING: PLOT RESPONSES FOR DAY TO DAY COMPARISONS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reference_day = '5 CueRein' #Day at which data is tracked from, and clusters are identified through\n",
    "baseline_day = '0 EarlyAcq' #Day at which data is normalized to (starting point = 0)\n",
    "daysforbaselines = sorted(days)[:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT THE CHANGE IN CASCADE MEASURES IN TRACKED NEURONS\n",
    "\n",
    "fig, axs = plt.subplots(1, len(measurements), figsize=(11, 3))\n",
    "\n",
    "for m, measurement in enumerate(measurements):\n",
    "    \n",
    "    # Calculate means and SEMs for each day\n",
    "    means = []\n",
    "    sems = []\n",
    "    all_values = []\n",
    "\n",
    "    for d, day in enumerate(sorted(daysforbaselines)):\n",
    "        values_cue = tracked_cmeasures_days[reference_day][day][:, m]\n",
    "        values_comparison = tracked_cmeasures_days[reference_day][day][:, m+(len(measurements))]\n",
    "        values = values_comparison-np.nanmean(values_cue)\n",
    "        if day == baseline_day:\n",
    "            print(measurement, day, stats.ttest_rel(values_cue, values_comparison, nan_policy = 'omit'))\n",
    "        \n",
    "\n",
    "        means.append(np.nanmean(values))\n",
    "        sems.append(np.nanstd(values) / np.sqrt(np.count_nonzero(values > -1000)))\n",
    "\n",
    "    means = means - means[0]\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({'Day': sorted(daysforbaselines), 'Mean': means, 'SEM': sems})\n",
    "\n",
    "    # Plot bar plot with error bars\n",
    "    ax = axs[m]\n",
    "    sns.barplot(x='Day', y='Mean', yerr=plot_df['SEM'], data=plot_df, ax=ax, capsize=0.2)\n",
    "    ax.set_title(measurement)\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "   \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "  \n",
    "      \n",
    "      "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### PLOT THE CHANGE IN CASCADE MEASURES IN TRACKED NEURONS, SPLIT BY CLUSTERED AND NON CLUSTERED CELLS\n",
    "\n",
    "fig, axs = plt.subplots(2, len(measurements), figsize=(11, 6), sharey = 'col')\n",
    "\n",
    "for c in range(2):\n",
    "    for m, measurement in enumerate(measurements):\n",
    "\n",
    "        # Calculate means and SEMs for each day\n",
    "        means = []\n",
    "        sems = []\n",
    "        all_values = []\n",
    "\n",
    "        for d, day in enumerate(sorted(daysforbaselines)):\n",
    "            if c == 0:\n",
    "                values_cue = tracked_cmeasures_days[reference_day][day][:, m][np.where(tracked_clusterids_days[reference_day][day][:,0] == 0)]\n",
    "                values_comparison = tracked_cmeasures_days[reference_day][day][:, m+(len(measurements))][np.where(tracked_clusterids_days[reference_day][day][:,0] == 0)]\n",
    "                values = values_comparison-np.nanmean(values_cue)\n",
    "\n",
    "            else:\n",
    "                values_cue = tracked_cmeasures_days[reference_day][day][:, m][np.where(tracked_clusterids_days[reference_day][day][:,0] > 0)]\n",
    "                values_comparison = tracked_cmeasures_days[reference_day][day][:, m+(len(measurements))][np.where(tracked_clusterids_days[reference_day][day][:,0] > 0)]\n",
    "                values = values_comparison-np.nanmean(values_cue)\n",
    "                \n",
    "            if d == 0:\n",
    "                print('cluster=',c, measurement, day, stats.ttest_rel(values_cue, values_comparison, nan_policy = 'omit'))\n",
    "\n",
    "\n",
    "            means.append(np.nanmean(values))\n",
    "            sems.append(np.nanstd(values) / np.sqrt(np.count_nonzero(values > -1000)))\n",
    "\n",
    "        means = means - means[0]\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        plot_df = pd.DataFrame({'Day': sorted(daysforbaselines), 'Mean': means, 'SEM': sems})\n",
    "\n",
    "        # Plot bar plot with error bars\n",
    "        ax = axs[c, m]\n",
    "        sns.barplot(x='Day', y='Mean', yerr=plot_df['SEM'], data=plot_df, ax=ax, capsize=0.2)\n",
    "        ax.set_title(measurement)\n",
    "        ax.set_xlabel('Day')\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "   \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "  \n",
    "      \n",
    "      "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT THE CHANGE IN CASCADE MEASURES IN TRACKED NEURONS, SPLIT BY CLUSTERED AND NON CLUSTERED CELLS\n",
    "\n",
    "fig, axs = plt.subplots(3, len(measurements), figsize=(11, 7.5), sharey = 'col')\n",
    "\n",
    "for rt in range(len(response_types)):\n",
    "    for m, measurement in enumerate(measurements):\n",
    "\n",
    "        # Calculate means and SEMs for each day\n",
    "        means = []\n",
    "        sems = []\n",
    "        all_values = []\n",
    "\n",
    "        for d, day in enumerate(sorted(daysforbaselines)):\n",
    "            if rt == 0:\n",
    "                values_cue = tracked_cmeasures_days[reference_day][day][:, m][np.where(tracked_clusterids_days[reference_day][day][:,0] == 0)]\n",
    "                values_comparison = tracked_cmeasures_days[reference_day][day][:, m+(len(measurements))][np.where(tracked_clusterids_days[reference_day][day][:,0] == 0)]\n",
    "                values = values_comparison-np.nanmean(values_cue)\n",
    "\n",
    "            elif rt == 1:\n",
    "                values_cue = tracked_cmeasures_days[reference_day][day][:, m][np.logical_and(tracked_clusterids_days[reference_day][day][:,0] > 0, tracked_clusterids_days[reference_day][day][:,0] < 5)]\n",
    "                values_comparison = tracked_cmeasures_days[reference_day][day][:, m+(len(measurements))][np.logical_and(tracked_clusterids_days[reference_day][day][:,0] > 0, tracked_clusterids_days[reference_day][day][:,0] < 5)]\n",
    "                values = values_comparison-np.nanmean(values_cue)\n",
    "            \n",
    "            elif rt == 2:\n",
    "                values_cue = tracked_cmeasures_days[reference_day][day][:, m][np.where(tracked_clusterids_days[reference_day][day][:,0] >= 5)]\n",
    "                values_comparison = tracked_cmeasures_days[reference_day][day][:, m+(len(measurements))][np.where(tracked_clusterids_days[reference_day][day][:,0] >= 5)]\n",
    "                values = values_comparison-np.nanmean(values_cue)\n",
    "                \n",
    "            if d == 0:\n",
    "                print(response_types[rt], measurement, day, stats.ttest_rel(values_cue, values_comparison, nan_policy = 'omit'))\n",
    "\n",
    "\n",
    "            means.append(np.nanmean(values))\n",
    "            sems.append(np.nanstd(values) / np.sqrt(np.count_nonzero(values > -1000)))\n",
    "\n",
    "        means = means - means[0]\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        plot_df = pd.DataFrame({'Day': sorted(daysforbaselines), 'Mean': means, 'SEM': sems})\n",
    "\n",
    "        # Plot bar plot with error bars\n",
    "        ax = axs[rt, m]\n",
    "        sns.barplot(x='Day', y='Mean', yerr=plot_df['SEM'], data=plot_df, ax=ax, capsize=0.2, color = response_colors[rt])\n",
    "        ax.set_title(measurement)\n",
    "        ax.set_xlabel('Day')\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "   \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT THE CHANGE IN CASCADE MEASURES IN TRACKED NEURONS, SPLIT BY EACH CLUSTER\n",
    "\n",
    "fig, axs = plt.subplots(numclusters, len(measurements), figsize=(11, 3*numclusters), sharey = 'col')\n",
    "\n",
    "heatmap_array = np.nan*np.ones((numclusters, len(daysforbaselines), len(measurements)))\n",
    "\n",
    "for c in range(numclusters):\n",
    "    for m, measurement in enumerate(measurements):\n",
    "\n",
    "        # Calculate means and SEMs for each day\n",
    "        means = []\n",
    "        sems = []\n",
    "        all_values = []\n",
    "\n",
    "        for d, day in enumerate(sorted(daysforbaselines)):\n",
    "            values_cue = tracked_cmeasures_days[reference_day][day][:, m][np.where(tracked_clusterids_days[reference_day][day][:,0] == c)]\n",
    "            values_comparison = tracked_cmeasures_days[reference_day][day][:, m+(len(measurements))][np.where(tracked_clusterids_days[reference_day][day][:,0] ==c)]\n",
    "            values = values_comparison-np.mean(values_cue)\n",
    "                \n",
    "#             if d == 0:\n",
    "#                 print('cluster=',c, measurement, day, stats.ttest_rel(values_cue, values_comparison, nan_policy = 'omit'))\n",
    "\n",
    "            means.append(np.nanmean(values))\n",
    "            sems.append(np.nanstd(values) / np.sqrt(np.count_nonzero(values > -1000)))\n",
    "            heatmap_array[c, d, m] = np.nanmean(values)/stats.sem(values, nan_policy = 'omit')\n",
    "\n",
    "            if m == 4:\n",
    "                heatmap_array[c, d, m] = -heatmap_array[c, d, m]\n",
    "        \n",
    "        means = means - means[0]\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        plot_df = pd.DataFrame({'Day': sorted(daysforbaselines), 'Mean': means, 'SEM': sems})\n",
    "\n",
    "        # Plot bar plot with error bars\n",
    "        ax = axs[c, m]\n",
    "        sns.barplot(x='Day', y='Mean', yerr=plot_df['SEM'], data=plot_df, ax=ax, capsize=0.2, color = cluster_colors[c])\n",
    "        ax.set_title(measurement)\n",
    "        ax.set_xlabel('Day')\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "   \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT HEATMAPS REPRESENTING AVERAGE CHANGES FOR EACH CLUSTER, ON EACH DAY\n",
    "fig, axs = plt.subplots(1,len(daysforbaselines), figsize=(7.5, 1.5))\n",
    "\n",
    "heatmap_array = heatmap_array - heatmap_array[:, 0, :][:, np.newaxis, :]\n",
    "\n",
    "for d, day in enumerate(sorted(daysforbaselines)):\n",
    "    ax = axs[d]\n",
    "    sns.heatmap(heatmap_array[:, d, :], cmap = 'coolwarm',  ax = axs[d], vmax = 8, vmin = -8)\n",
    "    ax.set_title(day)\n",
    "    if d == 0:\n",
    "        ax.set_ylabel('Cluster')\n",
    "    else:\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_xlabel(None)\n",
    "#     xtick_labels = measurements[:3]\n",
    "#     ax.set_xticklabels(xtick_labels, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(2, 2))\n",
    "\n",
    "sns.heatmap(np.nanmean(heatmap_array[:, :, [0,1,3,4,5,6,8]], axis = 1), cmap = 'coolwarm', vmax = 5, vmin = -5)\n",
    "axs.set_ylabel('Cluster')\n",
    "axs.set_xlabel('Measure of Spiking')\n",
    "axs.set_xlabel(None)\n",
    "# xtick_labels = measurements[:]\n",
    "# axs.set_xticklabels(xtick_labels, rotation=90)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reference_days = ['5 CueRein', '6 DrugRein', '7 TMTRein'] #Day at which data is tracked from, and clusters are identified through\n",
    "# reference_days = ['3 EarlyExt', '5 CueRein', '6 DrugRein', '7 TMTRein'] #Day at which data is tracked from, and clusters are identified through\n",
    "# reference_days = ['0 EarlyAcq', '1 MidAcq', '2 LateAcq'] #Day at which data is tracked from, and clusters are identified through\n",
    "\n",
    "baseline_day = '4 LastExt' #Day at which data is normalized to (starting point = 0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(len(reference_days)+1, len(measurements), figsize=(15, 2*(len(reference_days)+1)))\n",
    "for m, measurement in enumerate(measurements):\n",
    "    filtered_cmeasures_all = []\n",
    "    filtered_aucpvals_all = []\n",
    "    for rd, reference_day in enumerate(reference_days):\n",
    "        valid_indices = np.where(\n",
    "        np.isfinite(tracked_aucpvals_days[reference_day][baseline_day][:,0]) & \n",
    "        np.isfinite(tracked_cmeasures_days[reference_day][baseline_day][:,m]) &\n",
    "        np.isfinite(tracked_cmeasures_days[reference_day][baseline_day][:,m+len(measurements)]))[0]\n",
    "\n",
    "        # Filter the arrays to only include valid values\n",
    "        filtered_aucpvals = tracked_aucpvals_days[reference_day][baseline_day][:,1][valid_indices]\n",
    "        filtered_cmeasures_cue = tracked_cmeasures_days[reference_day][baseline_day][:, m][valid_indices]\n",
    "        filtered_cmeasures_comparison = tracked_cmeasures_days[reference_day][baseline_day][:, m+(len(measurements))][valid_indices]\n",
    "        filtered_cmeasures = filtered_cmeasures_cue - filtered_cmeasures_comparison\n",
    "        print(measurement, reference_day, stats.spearmanr(filtered_cmeasures, filtered_aucpvals))\n",
    "\n",
    "        ax = axs[rd, m]\n",
    "        sns.regplot(x= filtered_cmeasures, y=filtered_aucpvals, ax = ax, scatter = False, line_kws={'color': 'red', 'linewidth': 1})\n",
    "        ax.scatter(x = filtered_cmeasures, y = filtered_aucpvals, s = 1, color = 'k', alpha = 0.2)\n",
    "        ax.set_title(measurement)\n",
    "        if m == 0:\n",
    "            ax.set_ylabel('auROC P-Value')\n",
    "        ax.set_xlabel('delta %s'%measurement)\n",
    "        if m != 0:\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "        filtered_cmeasures_all = np.concatenate((filtered_cmeasures_all, filtered_cmeasures))\n",
    "        filtered_aucpvals_all = np.concatenate((filtered_aucpvals_all, filtered_aucpvals))\n",
    "        \n",
    "    ax = axs[rd+1, m]\n",
    "    sns.regplot(x=filtered_cmeasures_all, y=filtered_aucpvals_all, ax = ax, scatter = False, line_kws={'color': 'red', 'linewidth': 1})\n",
    "    ax.scatter(x = filtered_cmeasures_all, y = filtered_aucpvals_all, s = 1, color = 'k', alpha = 0.2)\n",
    "    ax.set_title(measurement)\n",
    "    if m == 0:\n",
    "        ax.set_ylabel('auROC')\n",
    "    ax.set_xlabel('delta %s'%measurement)\n",
    "    if m != 0:\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    print(measurement, 'AllDays', stats.spearmanr(filtered_cmeasures_all, filtered_aucpvals_all))\n",
    "\n",
    "\n",
    "        \n",
    "                "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(len(reference_days)+1, len(measurements), figsize=(15, 2*(len(reference_days)+1)))\n",
    "for m, measurement in enumerate(measurements):\n",
    "    filtered_cmeasures_all = []\n",
    "    filtered_responses_all = []\n",
    "    for rd, reference_day in enumerate(reference_days):\n",
    "        valid_indices = np.where(\n",
    "        np.isfinite(np.mean(tracked_popevents_days[reference_day][baseline_day], axis = 1)) & \n",
    "        np.isfinite(tracked_cmeasures_days[reference_day][baseline_day][:,m]) &\n",
    "        np.isfinite(tracked_cmeasures_days[reference_day][baseline_day][:,m+len(measurements)]))[0]\n",
    "\n",
    "        # Filter the arrays to only include valid values\n",
    "        filtered_responses = tracked_popevents_days[reference_day][baseline_day][valid_indices]\n",
    "        filtered_responses = np.abs(np.nanmean(filtered_responses[:, aucfirstframe:auclastframe], axis = 1))\n",
    "        filtered_cmeasures_cue = tracked_cmeasures_days[reference_day][baseline_day][:, m][valid_indices]\n",
    "        filtered_cmeasures_comparison = tracked_cmeasures_days[reference_day][baseline_day][:, m+(len(measurements))][valid_indices]\n",
    "        filtered_cmeasures = filtered_cmeasures_cue - filtered_cmeasures_comparison\n",
    "        print(measurement, reference_day, stats.spearmanr(filtered_cmeasures, filtered_responses))\n",
    "\n",
    "        ax = axs[rd, m]\n",
    "        sns.regplot(x=filtered_cmeasures, y=filtered_responses, ax = ax, scatter = False, line_kws={'color': 'red', 'linewidth': 1})\n",
    "        ax.scatter(x = filtered_cmeasures, y = filtered_responses, s = 1, color = 'k', alpha = 0.2)\n",
    "        ax.set_title(measurement)\n",
    "        if m == 0:\n",
    "            ax.set_ylabel('Response Amplitude')\n",
    "        ax.set_xlabel('delta %s'%measurement)\n",
    "        if m != 0:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        filtered_cmeasures_all = np.concatenate((filtered_cmeasures_all, filtered_cmeasures))\n",
    "        filtered_responses_all = np.concatenate((filtered_responses_all, filtered_responses))\n",
    "        \n",
    "    ax = axs[rd+1, m]\n",
    "    sns.regplot(x=filtered_cmeasures_all, y=filtered_responses_all, ax = ax, scatter = False, line_kws={'color': 'red', 'linewidth': 1})\n",
    "    ax.scatter(x = filtered_cmeasures_all, y = filtered_responses_all, s = 1, color = 'k', alpha = 0.2)\n",
    "    ax.set_title(measurement)\n",
    "    if m == 0:\n",
    "        ax.set_ylabel('Response Amplitude')\n",
    "    ax.set_xlabel('delta %s'%measurement)\n",
    "    if m != 0:\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    print(measurement, 'AllDays', stats.spearmanr(filtered_cmeasures_all, filtered_responses_all))\n",
    "\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPIKE INFERENCE WITHIN SESSION ADAPTATION"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### DEFINE THESE VARIABLES FOR CREATING SPIKE PREDICTION MEASUREMENTS\n",
    "\n",
    "save_cascade_measures_binned = 'yes' #'yes' to save new cascade measures. 'no' to load saved data for each FOV\n",
    "only_clustered_cells = 'yes'\n",
    "dataset = 'behavior' #'baseline' or 'behavior' sessions?\n",
    "cascade_lengthbins = 5 # Length of each bin in minutes\n",
    "binstoplot = 24\n",
    "concatenate_with_baselines = 'yes'\n",
    "\n",
    "# Define the start and end times to analyze (in minutes)\n",
    "if dataset == 'baseline':\n",
    "    startmin = 0 #start time in minutes\n",
    "    endmin = 5 #end time in minutes\n",
    "elif dataset == 'behavior':\n",
    "    startmin = 0 #start time in minutes\n",
    "    endmin = 120 #end time in minutes\n",
    "    \n",
    "cascade_numbins = int((endmin-startmin)/cascade_lengthbins)\n",
    "cascade_lengthbins = int(cascade_lengthbins*60*averagedframerate)\n",
    "\n",
    "window_predictions = [int(startmin*averagedframerate*60), int(endmin*averagedframerate*60)]\n",
    "window_frames = window_predictions[1]-window_predictions[0]\n",
    "\n",
    "### SELECTS ALL DAYS WITHIN A DIRECTORY. HAVE TO REDO THIS BECAUSE OF LIMITED DAY SAMPLING IN DECODING ABOVE###\n",
    "tempdays = next(os.walk(os.path.join(basedir)))[1]\n",
    "days = []\n",
    "for t in tempdays:\n",
    "    if t!= 'Cascade' and t!= 'CellTracking' and t!= 'Codes' and t != '.ipynb_checkpoints' and t != 'Other' and t!= 'Results':\n",
    "        days = np.append(days, t)\n",
    "days = sorted(days)[:-2]\n",
    "# days = ['5 CueRein']\n",
    "numdays = len(days)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### IF SAVING CASCADE MEASURES, FIRST LOAD YOUR SPIKE PREDICTIONS DATA SO THAT YOU CAN QUANTIFY EACH MEASURE \n",
    "if save_cascade_measures_binned == 'yes':\n",
    "    predictions_fov = {}\n",
    "    for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "        tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "        if day not in predictions_fov:\n",
    "            predictions_fov[day] = {}\n",
    "        if animal not in predictions_fov[day]:\n",
    "            predictions_fov[day][animal] = {}\n",
    "        if dataset == 'baseline':\n",
    "            try:\n",
    "                predictions_fov[day][animal][fov] = \\\n",
    "                                np.load(os.path.join(basedir, day, animal, fov, 'baseline_predictions.npy'))[:, window_predictions[0]:window_predictions[1]]\n",
    "            except FileNotFoundError:\n",
    "                print('No baseline data for: ', day, animal, fov)\n",
    "                continue\n",
    "        if dataset == 'behavior':\n",
    "            try:\n",
    "                predictions_fov[day][animal][fov] = \\\n",
    "                                np.load(os.path.join(basedir, day, animal, fov, 'behavior_predictions.npy'))[:, window_predictions[0]:window_predictions[1]]\n",
    "            except FileNotFoundError:\n",
    "                print('No behavior data for: ', day, animal, fov)\n",
    "                continue\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### IF SAVING BINNED CASCADE MEASURES, DEFINE YOUR FUNCTION FOR CALCULATING AND EXTRACTING EACH MEASURE FROM EACH DATASET ACROSS TIME\n",
    "def calculate_cascade_measures_binned(data, numbins, lengthbin):\n",
    "    numneurons = data.shape[0]\n",
    "    cascade_measures_binned = np.nan*np.ones((numneurons, numbins, 9))\n",
    "    for neuron in range(numneurons):\n",
    "        predictions_neuron = data[neuron, :] \n",
    "        for b in range(numbins):\n",
    "            predictions_neuron_bin = predictions_neuron[b*lengthbin:(b*lengthbin)+lengthbin]\n",
    "\n",
    "            if np.all(np.isnan(predictions_neuron_bin)) == False:\n",
    "\n",
    "                # Calculate basal firing properties for each neuron (basal firing rate, mean firing rate, maximum spike rate)\n",
    "                basalfiring_neuron = np.nanquantile(predictions_neuron_bin, 0.33)\n",
    "                meanfiring_neuron = np.nanmean(predictions_neuron_bin)\n",
    "                maxfiring_neuron = np.nanmax(predictions_neuron_bin)\n",
    "\n",
    "                # Find spike bursts for each neuron\n",
    "                predictions_subtracted_bin = predictions_neuron_bin-basalfiring_neuron\n",
    "                burst_frames, burst_dict = find_peaks(predictions_subtracted_bin,\\\n",
    "                        distance = averagedframerate, width = (3, 3*averagedframerate), height = (1.5))  # Find frame number for bursts\n",
    "\n",
    "                # Calculate burst firing properties for each neuron (basal firing rate, mean firing rate, maximum spike rate)\n",
    "                burstfreq_neuron = np.nanmean(len(burst_frames)/np.count_nonzero(predictions_neuron_bin >= 0)*averagedframerate) # Get the frequency of bursts\n",
    "                burstisi_neuron = np.nanmean(np.diff(burst_frames)) # Get the average inter-burst-intervals between bursts\n",
    "                burstamp_neuron = np.nanmean(predictions_neuron_bin[burst_frames])  # Get the average peak amplitude of bursts\n",
    "                burstampchange_neuron = np.nanmean(burst_dict['width_heights'])\n",
    "                burstwidth_neuron = np.nanmean(burst_dict['widths']/averagedframerate)\n",
    "                signalnoise_neuron = ((burstampchange_neuron*burstwidth_neuron)/(basalfiring_neuron*burstisi_neuron))\n",
    "\n",
    "                # Insert each calculation into \"cascade_measures_binned\" array\n",
    "                cascade_measures_neuron_bin = [basalfiring_neuron, meanfiring_neuron, maxfiring_neuron, \\\n",
    "                                                         burstfreq_neuron, burstisi_neuron, burstamp_neuron, \\\n",
    "                                                           burstampchange_neuron, burstwidth_neuron, signalnoise_neuron]\n",
    "\n",
    "                cascade_measures_binned[neuron, b, :] = cascade_measures_neuron_bin\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    return cascade_measures_binned\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### SAVE OR LOAD CASCADE MEASURES FOR ALL CELLS. CREATE DICTIONARY ARRAYS FOR EACH FOV AND EACH DAY.\n",
    "cascade_measures_binned_fov = {}\n",
    "cascade_measures_binned_day = {}\n",
    "\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "    tempfiles = next(os.walk(os.path.join(basedir, day, animal, fov)))[2]\n",
    "    \n",
    "    if day not in cascade_measures_binned_fov:\n",
    "        print(day)\n",
    "        cascade_measures_binned_day[day] = np.nan*np.ones((1, cascade_numbins, 9))\n",
    "        cascade_measures_binned_fov[day] = {}\n",
    "\n",
    "    if animal not in cascade_measures_binned_fov[day]:\n",
    "        cascade_measures_binned_fov[day][animal] = {}\n",
    "        cascade_measures_binned_fov[day][animal] = {}\n",
    "        \n",
    "    if fov not in cascade_measures_binned_fov[day][animal]:\n",
    "        \n",
    "        ### SAVE CASCADE MEASURES FOR EACH FOV\n",
    "        if save_cascade_measures == 'yes':\n",
    "            try:\n",
    "                cascade_measures_binned_fov[day][animal][fov] = calculate_cascade_measures_binned(predictions_fov[day][animal][fov], cascade_numbins, cascade_lengthbins)\n",
    "                np.save(os.path.join(basedir, day, animal, fov, 'cascade_measures_binned_%s.npy'%dataset), cascade_measures_binned_fov[day][animal][fov])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        ### LOAD CASCADE MEASURES FOR EACH FOV\n",
    "        if only_clustered_cells == 'yes':\n",
    "            try: \n",
    "                tempnumneurons = np.shape(np.load(os.path.join(basedir, day, animal, fov, 'aucpvals_%s_%s.npy'%(eventofinterest, separation_requirement))))[0]\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            cascade_measures_binned_fov[day][animal][fov] = np.load(os.path.join(basedir, day, animal, fov, 'cascade_measures_binned_%s.npy'%dataset))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "                \n",
    "        # Stack Cascade Measures From Each FOV For Each Day\n",
    "        cascade_measures_binned_day[day] = np.vstack((cascade_measures_binned_day[day], cascade_measures_binned_fov[day][animal][fov]))\n",
    "\n",
    "for day in sorted(days):\n",
    "    cascade_measures_binned_day[day] = cascade_measures_binned_day[day][1:, :]\n",
    "               \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if concatenate_with_baselines == 'yes':\n",
    "    if np.shape(cascade_measures_binned_day[day])[1] == cascade_numbins:\n",
    "        for d, day in enumerate(sorted(days)):\n",
    "            cascade_measures_binned_day[day] = np.hstack((cascade_measures_day[day][:, np.newaxis, :], cascade_measures_binned_day[day]))\n",
    "        binstoplot += 1\n",
    "print(day, cascade_measures_binned_day[day].shape[1])   \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CASCADE MEASURES AS LINE GRAPHS FOR EACH DAY ###\n",
    "measurements = ['Basal Firing (Hz)', 'Mean Firing (Hz)', 'Max Firing (Hz)', 'Burst Freq (Hz)', \\\n",
    "                'Burst ISI (s)', 'Burst Amp (Hz)', 'Burst Amp (Hz)', 'Burst Width (s)', 'SNR']\n",
    "\n",
    "fig, axs = plt.subplots(1, len(measurements), figsize=(20, 3))\n",
    "\n",
    "# Store the handles and labels for the legend\n",
    "handles, labels = [], []\n",
    "\n",
    "for m, measurement in enumerate(measurements):\n",
    "    # Calculate means and SEMs for each day\n",
    "    means = np.nan * np.ones((len(days), binstoplot))\n",
    "    sems = np.nan * np.ones((len(days), binstoplot))\n",
    "\n",
    "    for d, day in enumerate(sorted(days)):\n",
    "        if d == 5:\n",
    "            values = cascade_measures_binned_day[day][:, :binstoplot, m]\n",
    "            values = values - values[:, 0, np.newaxis]\n",
    "            means[d, :] = np.nanmean(values, axis=0)\n",
    "            sems[d, :] = stats.sem(values, axis=0, nan_policy='omit')\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        'Day': np.repeat(sorted(days), binstoplot),\n",
    "        'Time': np.tile(np.arange(binstoplot), len(days)),\n",
    "        'Mean': means.flatten(),\n",
    "        'SEM': sems.flatten()\n",
    "    })\n",
    "\n",
    "    # Plot line plot with error bars\n",
    "    ax = axs[m]\n",
    "    sns_plot = sns.lineplot(x='Time', y='Mean', hue='Day', data=plot_df, ax=ax, legend='full')\n",
    "    for day in sorted(days):\n",
    "        day_df = plot_df[plot_df['Day'] == day]\n",
    "        ax.fill_between(day_df['Time'], day_df['Mean'] - day_df['SEM'], day_df['Mean'] + day_df['SEM'], alpha=0.2)\n",
    "    ax.set_title(measurement)\n",
    "    ax.set_xlabel('Time (bins)')\n",
    "    ax.set_ylabel(None)\n",
    "    \n",
    "    # Capture handles and labels only from the first plot\n",
    "    if m == 0:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    # Remove the legend from the individual plots\n",
    "    ax.legend_.remove()\n",
    "\n",
    "# Add a single legend to the right of the final plot\n",
    "fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CASCADE MEASURES AS LINE GRAPHS FOR EACH DAY ###\n",
    "\n",
    "fig, axs = plt.subplots(2, len(measurements), figsize=(20, 4*2), sharey='col')\n",
    "\n",
    "# Store the handles and labels for the legend\n",
    "handles, labels = [], []\n",
    "\n",
    "for c in range(2):\n",
    "    for m, measurement in enumerate(measurements):\n",
    "        # Calculate means and SEMs for each day\n",
    "        means = np.nan * np.ones((len(days), binstoplot))\n",
    "        sems = np.nan * np.ones((len(days), binstoplot))\n",
    "\n",
    "        for d, day in enumerate(sorted(days)):\n",
    "            if d == 5:\n",
    "                if c == 0:\n",
    "                    values = cascade_measures_binned_day[day][:, :binstoplot, m][np.where(clusterids_day[day] == 0)]\n",
    "                if c == 1:\n",
    "                    values = cascade_measures_binned_day[day][:, :binstoplot, m][np.where(clusterids_day[day] > 0)]\n",
    "                values = values - values[:, 0, np.newaxis]\n",
    "                means[d, :] = np.nanmean(values, axis=0)\n",
    "                sems[d, :] = stats.sem(values, axis=0, nan_policy='omit')\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        plot_df = pd.DataFrame({\n",
    "            'Day': np.repeat(sorted(days), binstoplot),\n",
    "            'Time': np.tile(np.arange(binstoplot), len(days)),\n",
    "            'Mean': means.flatten(),\n",
    "            'SEM': sems.flatten()\n",
    "        })\n",
    "\n",
    "        # Plot line plot with error bars\n",
    "        ax = axs[c, m]\n",
    "        sns_plot = sns.lineplot(x='Time', y='Mean', hue='Day', data=plot_df, ax=ax, legend='full')\n",
    "        for day in sorted(days):\n",
    "            day_df = plot_df[plot_df['Day'] == day]\n",
    "            ax.fill_between(day_df['Time'], day_df['Mean'] - day_df['SEM'], day_df['Mean'] + day_df['SEM'], alpha=0.2)\n",
    "        ax.set_title(measurement)\n",
    "        ax.set_xlabel('Time (bins)')\n",
    "        ax.set_ylabel(None)\n",
    "\n",
    "        # Capture handles and labels only from the first plot\n",
    "        if m == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        # Remove the legend from the individual plots\n",
    "        ax.legend_.remove()\n",
    "\n",
    "# Add a single legend to the right of the final plot\n",
    "fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CASCADE MEASURES AS LINE GRAPHS FOR EACH DAY ###\n",
    "\n",
    "fig, axs = plt.subplots(3, len(measurements), figsize=(20, 4*3), sharey='col')\n",
    "\n",
    "# Store the handles and labels for the legend\n",
    "handles, labels = [], []\n",
    "\n",
    "for rt in range(3):\n",
    "    for m, measurement in enumerate(measurements):\n",
    "        # Calculate means and SEMs for each day\n",
    "        means = np.nan * np.ones((len(days), binstoplot))\n",
    "        sems = np.nan * np.ones((len(days), binstoplot))\n",
    "\n",
    "        for d, day in enumerate(sorted(days)):\n",
    "            if d == 5:\n",
    "                if rt == 0:\n",
    "                    indices = np.where(clusterids_day[day] == 0)\n",
    "                elif rt == 1:\n",
    "                    indices = np.where(np.logical_and(clusterids_day[day] > 0, clusterids_day[day] < 5))\n",
    "                elif rt == 2:\n",
    "                    indices = np.where(clusterids_day[day] >= 5)\n",
    "\n",
    "                values = cascade_measures_binned_day[day][indices][:, :binstoplot, m]\n",
    "                values = values - values[:, 0, np.newaxis]\n",
    "                means[d, :] = np.nanmean(values, axis=0)\n",
    "                sems[d, :] = stats.sem(values, axis=0, nan_policy='omit')\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        plot_df = pd.DataFrame({\n",
    "            'Day': np.repeat(sorted(days), binstoplot),\n",
    "            'Time': np.tile(np.arange(binstoplot), len(days)),\n",
    "            'Mean': means.flatten(),\n",
    "            'SEM': sems.flatten()\n",
    "        })\n",
    "\n",
    "        # Plot line plot with error bars\n",
    "        ax = axs[rt, m]\n",
    "        sns_plot = sns.lineplot(x='Time', y='Mean', hue='Day', data=plot_df, ax=ax, legend='full')\n",
    "        for day in sorted(days):\n",
    "            day_df = plot_df[plot_df['Day'] == day]\n",
    "            ax.fill_between(day_df['Time'], day_df['Mean'] - day_df['SEM'], day_df['Mean'] + day_df['SEM'], alpha=0.2)\n",
    "        ax.set_title(measurement)\n",
    "        ax.set_xlabel('Time (bins)')\n",
    "        ax.set_ylabel(None)\n",
    "\n",
    "        # Capture handles and labels only from the first plot\n",
    "        if m == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        # Remove the legend from the individual plots\n",
    "        ax.legend_.remove()\n",
    "\n",
    "# Add a single legend to the right of the final plot\n",
    "fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### PLOT CASCADE MEASURES AS LINE GRAPHS FOR EACH DAY ###\n",
    "\n",
    "fig, axs = plt.subplots(numclusters, len(measurements), figsize=(20, 4*numclusters), sharey='col')\n",
    "\n",
    "# Store the handles and labels for the legend\n",
    "handles, labels = [], []\n",
    "\n",
    "for c in range(numclusters):\n",
    "    for m, measurement in enumerate(measurements):\n",
    "        # Calculate means and SEMs for each day\n",
    "        means = np.nan * np.ones((len(days), binstoplot))\n",
    "        sems = np.nan * np.ones((len(days), binstoplot))\n",
    "\n",
    "        for d, day in enumerate(sorted(days)):\n",
    "            if d == 5:\n",
    "                indices = np.where(clusterids_day[day] == c)\n",
    "                values = cascade_measures_binned_day[day][indices][:, :binstoplot, m]\n",
    "                values = values - values[:, 0, np.newaxis]\n",
    "                means[d, :] = np.nanmean(values, axis=0)\n",
    "                sems[d, :] = stats.sem(values, axis=0, nan_policy='omit')\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        plot_df = pd.DataFrame({\n",
    "            'Day': np.repeat(sorted(days), binstoplot),\n",
    "            'Time': np.tile(np.arange(binstoplot), len(days)),\n",
    "            'Mean': means.flatten(),\n",
    "            'SEM': sems.flatten()\n",
    "        })\n",
    "\n",
    "        # Plot line plot with error bars\n",
    "        ax = axs[c, m]\n",
    "        sns_plot = sns.lineplot(x='Time', y='Mean', hue='Day', data=plot_df, ax=ax, legend='full')\n",
    "        for day in sorted(days):\n",
    "            day_df = plot_df[plot_df['Day'] == day]\n",
    "            ax.fill_between(day_df['Time'], day_df['Mean'] - day_df['SEM'], day_df['Mean'] + day_df['SEM'], alpha=0.2)\n",
    "        ax.set_title(measurement)\n",
    "        ax.set_xlabel('Time (bins)')\n",
    "        ax.set_ylabel(None)\n",
    "\n",
    "        # Capture handles and labels only from the first plot\n",
    "        if m == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        # Remove the legend from the individual plots\n",
    "        ax.legend_.remove()\n",
    "\n",
    "# Add a single legend to the right of the final plot\n",
    "fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the Correlation Between Activity and Behavior ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Extract Press Timestamps by \"Tricking\" the Analyze Single Session Function\n",
    "\n",
    "### SELECTS ALL DAYS WITHIN A DIRECTORY ###\n",
    "tempdays = next(os.walk(os.path.join(basedir)))[1]\n",
    "days = []\n",
    "for t in tempdays:\n",
    "    if t!= 'Cascade' and t!= 'CellTracking' and t!= 'Codes' and t != '.ipynb_checkpoints' and t != 'Other' and t!= 'Results':\n",
    "        days = np.append(days, t)\n",
    "numdays = len(days)\n",
    "\n",
    "behavior_fov = {} # Dictionary for all popevents data for all fovs\n",
    "\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "    try:\n",
    "        popevents_fov[day][animal][fov]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "    if day not in behavior_fov:\n",
    "        behavior_fov[day] = {}\n",
    "    if animal not in behavior_fov[day]:\n",
    "        behavior_fov[day][animal] = {}\n",
    "    activelever, activelevertimeout = analyze_single_session(os.path.join(basedir, day, animal, fov), 'purple', 'iscool') ###insert any string as last two inputs to extract lever press information\n",
    "    \n",
    "    behavior_fov[day][animal][fov] = sorted(np.concatenate((activelever, activelevertimeout))/1000) ### Extracted events in milliseconds\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Correlate number of presses in a session by cascade measures in each fov\n",
    "pressrates_session_day = {}\n",
    "cms_session_day = {}\n",
    "\n",
    "for basedir, day, animal, fov in iterate_dirs(basedir, sorted(days)):\n",
    "    \n",
    "    try:\n",
    "        popevents_fov[day][animal][fov]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    if day not in pressrates_session_day:\n",
    "        pressrates_session_day[day] = []\n",
    "        cms_session_day[day] = np.nan*np.ones((1, len(measurements)))\n",
    "        \n",
    "    pressrate_session = len(behavior_fov[day][animal][fov])/np.amax(behavior_fov[day][animal][fov])*60 ###presses per minute\n",
    "   \n",
    "    \n",
    "    tempnumneurons = np.shape(cascade_measures_fov[day][animal][fov])[0]\n",
    "    \n",
    "    for neuron in range(tempnumneurons):\n",
    "        cms_session = cascade_measures_fov[day][animal][fov][neuron, :]\n",
    "        \n",
    "        cms_session_day[day] = np.vstack((cms_session_day[day], cms_session))\n",
    "        pressrates_session_day[day].append(pressrate_session)\n",
    "\n",
    "for day in sorted(days):\n",
    "    cms_session_day[day] = cms_session_day[day][1:, :]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cms_session_all = []\n",
    "pressrates_session_all = []\n",
    "for m, measurement in enumerate(measurements):\n",
    "    if m == 8:\n",
    "        for d, day in enumerate(sorted(days)):\n",
    "            indices = np.where(np.logical_and(np.isfinite(cms_session_day[day][:, m]), np.isfinite(pressrates_session_day[day])))[0]\n",
    "            print(measurement, day, stats.spearmanr(np.array(cms_session_day[day][:, m])[indices], np.array(pressrates_session_day[day])[indices]))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cms_session_all = []\n",
    "pressrates_session_all = []\n",
    "for m, measurement in enumerate(measurements):\n",
    "    if m == 8:\n",
    "        for d, day in enumerate(sorted(days)):\n",
    "            for c in range(numclusters):\n",
    "                indices = np.where(np.logical_and(np.isfinite(cms_session_day[day][:, m]), \\\n",
    "                                                  clusterids_day[day] == c))[0]\n",
    "\n",
    "                print(measurement, day, 'cluster = ', c, stats.spearmanr(np.array(cms_session_day[day][:, m])[indices], np.array(pressrates_session_day[day])[indices]))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
